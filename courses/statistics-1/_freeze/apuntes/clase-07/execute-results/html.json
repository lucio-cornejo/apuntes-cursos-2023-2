{
  "hash": "9170db0d6c825aac9652ae398996f154",
  "result": {
    "markdown": "# Apuntes de clase {-}\n\n## Ejemplo práctico\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Fahrmeir)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'Fahrmeir' was built under R version 4.1.3\n```\n:::\n\n```{.r .cell-code}\n# Cargamos una base de datos ya pre-procesada\ndata(credit)\n# help(credit)\n\nhead(credit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Y      Cuenta Mes             Ppag         Uso   DM   Sexo         Estc\n1 buen          no  18 pre buen pagador     privado 1049  mujer    vive solo\n2 buen          no   9 pre buen pagador profesional 2799 hombre no vive solo\n3 buen bad running  12 pre buen pagador profesional  841  mujer    vive solo\n4 buen          no  12 pre buen pagador profesional 2122 hombre no vive solo\n5 buen          no  12 pre buen pagador profesional 2171 hombre no vive solo\n6 buen          no  10 pre buen pagador profesional 2241 hombre no vive solo\n```\n:::\n:::\n\n\n### Estimación\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodelo_logistic <- glm(Y ~ ., family = binomial, data = credit)\nsummary(modelo_logistic)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = Y ~ ., family = binomial, data = credit)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.0572  -0.8050  -0.4581   0.9483   2.4655  \n\nCoefficients:\n                      Estimate Std. Error z value Pr(>|z|)    \n(Intercept)         -1.178e+00  2.693e-01  -4.374 1.22e-05 ***\nCuentagood running  -1.952e+00  2.060e-01  -9.472  < 2e-16 ***\nCuentabad running   -6.346e-01  1.764e-01  -3.598 0.000321 ***\nMes                  3.503e-02  7.849e-03   4.463 8.10e-06 ***\nPpagpre mal pagador  9.884e-01  2.529e-01   3.907 9.33e-05 ***\nUsoprofesional       4.744e-01  1.605e-01   2.956 0.003112 ** \nDM                   3.242e-05  3.335e-05   0.972 0.330893    \nSexohombre          -2.235e-01  2.208e-01  -1.012 0.311453    \nEstcvive solo        3.854e-01  2.194e-01   1.757 0.078921 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1221.7  on 999  degrees of freedom\nResidual deviance: 1017.3  on 991  degrees of freedom\nAIC: 1035.3\n\nNumber of Fisher Scoring iterations: 4\n```\n:::\n:::\n\n\n### Cálculo de métricas de evaluación del modelo\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Probabilidades predichas\nyprob <- predict(modelo_logistic, type = 'response')\n\n# Valores predichos\nypred <- as.numeric(yprob >= 0.5) |>\n  factor(labels = levels(credit$Y))\n\nhead(ypred)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] buen buen buen buen buen buen\nLevels: buen mal\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Matriz de confusión\nmc <- table(ypred, credit$Y)\nmc\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      \nypred  buen mal\n  buen  636 184\n  mal    64 116\n```\n:::\n\n```{.r .cell-code}\ntesterr <- mean(ypred != credit$Y)\ntesterr\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.248\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Matriz de confusión, usando caret\ncaret::confusionMatrix(\n  data = ypred, reference = credit$Y, positive = \"mal\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction buen mal\n      buen  636 184\n      mal    64 116\n                                         \n               Accuracy : 0.752          \n                 95% CI : (0.724, 0.7785)\n    No Information Rate : 0.7            \n    P-Value [Acc > NIR] : 0.000151       \n                                         \n                  Kappa : 0.3333         \n                                         \n Mcnemar's Test P-Value : 4.14e-14       \n                                         \n            Sensitivity : 0.3867         \n            Specificity : 0.9086         \n         Pos Pred Value : 0.6444         \n         Neg Pred Value : 0.7756         \n             Prevalence : 0.3000         \n         Detection Rate : 0.1160         \n   Detection Prevalence : 0.1800         \n      Balanced Accuracy : 0.6476         \n                                         \n       'Positive' Class : mal            \n                                         \n```\n:::\n\n```{.r .cell-code}\ncaret::confusionMatrix(\n  data = ypred, reference = credit$Y, positive = \"mal\", \n  # Precision vs recall (incluye F1)\n  mode = \"prec_recall\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction buen mal\n      buen  636 184\n      mal    64 116\n                                         \n               Accuracy : 0.752          \n                 95% CI : (0.724, 0.7785)\n    No Information Rate : 0.7            \n    P-Value [Acc > NIR] : 0.000151       \n                                         \n                  Kappa : 0.3333         \n                                         \n Mcnemar's Test P-Value : 4.14e-14       \n                                         \n              Precision : 0.6444         \n                 Recall : 0.3867         \n                     F1 : 0.4833         \n             Prevalence : 0.3000         \n         Detection Rate : 0.1160         \n   Detection Prevalence : 0.1800         \n      Balanced Accuracy : 0.6476         \n                                         \n       'Positive' Class : mal            \n                                         \n```\n:::\n\n```{.r .cell-code}\ncaret::confusionMatrix(\n  data = ypred, reference = credit$Y, positive = \"mal\", \n  mode = \"everything\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction buen mal\n      buen  636 184\n      mal    64 116\n                                         \n               Accuracy : 0.752          \n                 95% CI : (0.724, 0.7785)\n    No Information Rate : 0.7            \n    P-Value [Acc > NIR] : 0.000151       \n                                         \n                  Kappa : 0.3333         \n                                         \n Mcnemar's Test P-Value : 4.14e-14       \n                                         \n            Sensitivity : 0.3867         \n            Specificity : 0.9086         \n         Pos Pred Value : 0.6444         \n         Neg Pred Value : 0.7756         \n              Precision : 0.6444         \n                 Recall : 0.3867         \n                     F1 : 0.4833         \n             Prevalence : 0.3000         \n         Detection Rate : 0.1160         \n   Detection Prevalence : 0.1800         \n      Balanced Accuracy : 0.6476         \n                                         \n       'Positive' Class : mal            \n                                         \n```\n:::\n:::\n\n\n### LogLoss\n\n- Hasta ahora, no aprovechamos que no predecimos solo\nclases, sino también probabilidad de pertenencia a una clase. \\\nEn ese sentido, no hemos empleado que una predicción de clase\npuede estar asociada a una predicción de probabilidad muy cercana a 1.\n\n- Esta cantidad **no es interpretable**,\npero se usa para comparar modelos de clasificación.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nunique(as.numeric(credit$Y))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1 2\n```\n:::\n\n```{.r .cell-code}\nMLmetrics::LogLoss(\n  y_pred = yprob, y_true = as.numeric(credit$Y) - 1\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.5086734\n```\n:::\n:::\n\n\n### Curvas ROC\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(pROC)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'pROC' was built under R version 4.1.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nType 'citation(\"pROC\")' for a citation.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'pROC'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    cov, smooth, var\n```\n:::\n\n```{.r .cell-code}\n# Area debajo de la curva ROC\nanalysis <- roc(response = credit$Y, predictor = yprob)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSetting levels: control = buen, case = mal\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nSetting direction: controls < cases\n```\n:::\n\n```{.r .cell-code}\nanalysis\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nroc.default(response = credit$Y, predictor = yprob)\n\nData: yprob in 700 controls (credit$Y buen) < 300 cases (credit$Y mal).\nArea under the curve: 0.7753\n```\n:::\n\n```{.r .cell-code}\nMLmetrics::AUC(y_pred = yprob, y_true = as.numeric(credit$Y) -1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.775319\n```\n:::\n:::\n\n\n#### Coeficiente Gini\n\n\n::: {.cell}\n\n```{.r .cell-code}\n2*analysis$auc - 1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.5506381\n```\n:::\n\n```{.r .cell-code}\nMLmetrics::Gini(y_pred = yprob, y_true = as.numeric(credit$Y) - 1) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.5506381\n```\n:::\n:::\n\n\n#### Gráfica de la curva ROC\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(1 - analysis$specificities, analysis$sensitivities,\n  ylab = \"Sensitividad\", xlab = \"1 - Especificidad\",\n  main = \"Curva ROC para el modelo logistico\",\n  type = \"l\", col = \"blue\", lwd = 2\n)\nabline(a = 0, b = 1, col = \"red\")\n```\n\n::: {.cell-output-display}\n![](clase-07_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n#### Punto de corte\n\nPara encontrar el **mejor equilibrio posible** \nentre *sensibilidad* y *especificidad* \n(en caso nos interese balancearlos),\nusamos el criterio del **índice J de Youden**:\n$J = \\text{ sensitivity } + \\text{ specificity } - 1$ .\n\nSe puede usar otro criterio, como encontrar el punto\nmás en la curva ROC más cercano, respecto a distancia euclideana,\nal punto (1, 0), pues tal vértice representa \n100% sensibilidad y 100% especificidad.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ne <- cbind(\n  analysis$thresholds,\n  analysis$sensitivities + analysis$specificities - 1\n)\nhead(e)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           [,1]        [,2]\n[1,]       -Inf 0.000000000\n[2,] 0.04054953 0.001428571\n[3,] 0.04142278 0.002857143\n[4,] 0.04262476 0.004285714\n[5,] 0.04298069 0.005714286\n[6,] 0.04305416 0.007142857\n```\n:::\n\n```{.r .cell-code}\nopt_t <- subset(e, e[,2] == max(e[,2]))[,1]\nopt_t\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.2842726\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nypred2 <- factor(\n  as.numeric(yprob >= opt_t ), labels = levels(credit$Y)\n)\ncaret::confusionMatrix(ypred2, credit$Y, positive = \"mal\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction buen mal\n      buen  463  67\n      mal   237 233\n                                          \n               Accuracy : 0.696           \n                 95% CI : (0.6664, 0.7244)\n    No Information Rate : 0.7             \n    P-Value [Acc > NIR] : 0.6235          \n                                          \n                  Kappa : 0.377           \n                                          \n Mcnemar's Test P-Value : <2e-16          \n                                          \n            Sensitivity : 0.7767          \n            Specificity : 0.6614          \n         Pos Pred Value : 0.4957          \n         Neg Pred Value : 0.8736          \n             Prevalence : 0.3000          \n         Detection Rate : 0.2330          \n   Detection Prevalence : 0.4700          \n      Balanced Accuracy : 0.7190          \n                                          \n       'Positive' Class : mal             \n                                          \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Otra forma\ncoords(analysis , \"b\", ret = \"t\", best.method = \"youden\") \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  threshold\n1 0.2842726\n```\n:::\n:::\n\n\n### Otros enlaces\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfmla <- Y ~ Cuenta + Mes + Ppag + Uso + Estc\n```\n:::\n\n\n#### probit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodelo_probit <- glm(fmla, data = credit, \n  family = binomial(link = probit)\n)\nsummary(modelo_probit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = fmla, family = binomial(link = probit), data = credit)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.9938  -0.8084  -0.4554   0.9565   2.5057  \n\nCoefficients:\n                     Estimate Std. Error z value Pr(>|z|)    \n(Intercept)         -0.791777   0.124560  -6.357 2.06e-10 ***\nCuentagood running  -1.126848   0.115658  -9.743  < 2e-16 ***\nCuentabad running   -0.379371   0.106709  -3.555 0.000378 ***\nMes                  0.022802   0.003696   6.169 6.88e-10 ***\nPpagpre mal pagador  0.584158   0.151081   3.867 0.000110 ***\nUsoprofesional       0.276439   0.094022   2.940 0.003281 ** \nEstcvive solo        0.309828   0.093472   3.315 0.000918 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1221.7  on 999  degrees of freedom\nResidual deviance: 1019.7  on 993  degrees of freedom\nAIC: 1033.7\n\nNumber of Fisher Scoring iterations: 4\n```\n:::\n\n```{.r .cell-code}\nyprob <- predict(modelo_probit, type = \"response\")\nypred <- factor(as.numeric(yprob >= 0.5 ), labels = levels(credit$Y))\n\ncaret::confusionMatrix(ypred, credit$Y, positive = \"mal\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction buen mal\n      buen  641 182\n      mal    59 118\n                                          \n               Accuracy : 0.759           \n                 95% CI : (0.7313, 0.7852)\n    No Information Rate : 0.7             \n    P-Value [Acc > NIR] : 1.921e-05       \n                                          \n                  Kappa : 0.3501          \n                                          \n Mcnemar's Test P-Value : 3.881e-15       \n                                          \n            Sensitivity : 0.3933          \n            Specificity : 0.9157          \n         Pos Pred Value : 0.6667          \n         Neg Pred Value : 0.7789          \n             Prevalence : 0.3000          \n         Detection Rate : 0.1180          \n   Detection Prevalence : 0.1770          \n      Balanced Accuracy : 0.6545          \n                                          \n       'Positive' Class : mal             \n                                          \n```\n:::\n\n```{.r .cell-code}\nMLmetrics::LogLoss(yprob, as.numeric(credit$Y ) - 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.5098517\n```\n:::\n\n```{.r .cell-code}\nMLmetrics::AUC(yprob, as.numeric(credit$Y) - 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.7718333\n```\n:::\n\n```{.r .cell-code}\nMLmetrics::Gini(yprob, as.numeric(credit$Y) - 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.5384095\n```\n:::\n\n```{.r .cell-code}\nMLmetrics::KS_Stat(yprob, as.numeric(credit$Y) - 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 42.66667\n```\n:::\n:::\n\n\n#### cloglog\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#cloglog\nmodelo_cloglog <- glm(fmla, data = credit,  \n  family = binomial(link = cloglog)\n)\nsummary(modelo_cloglog)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = fmla, family = binomial(link = cloglog), data = credit)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.1526  -0.8058  -0.4693   0.9521   2.3490  \n\nCoefficients:\n                     Estimate Std. Error z value Pr(>|z|)    \n(Intercept)         -1.345386   0.158400  -8.494  < 2e-16 ***\nCuentagood running  -1.635383   0.172965  -9.455  < 2e-16 ***\nCuentabad running   -0.509948   0.130723  -3.901 9.58e-05 ***\nMes                  0.028268   0.004526   6.246 4.21e-10 ***\nPpagpre mal pagador  0.616605   0.166996   3.692 0.000222 ***\nUsoprofesional       0.330992   0.121958   2.714 0.006648 ** \nEstcvive solo        0.391084   0.121486   3.219 0.001286 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1221.7  on 999  degrees of freedom\nResidual deviance: 1022.3  on 993  degrees of freedom\nAIC: 1036.3\n\nNumber of Fisher Scoring iterations: 6\n```\n:::\n\n```{.r .cell-code}\nyprob <- predict(modelo_cloglog, type = \"response\")\nypred <- factor(as.numeric(yprob >= 0.5 ), labels = levels(credit$Y))\n\ncaret::confusionMatrix(ypred, credit$Y, positive = \"mal\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction buen mal\n      buen  641 185\n      mal    59 115\n                                          \n               Accuracy : 0.756           \n                 95% CI : (0.7281, 0.7823)\n    No Information Rate : 0.7             \n    P-Value [Acc > NIR] : 4.795e-05       \n                                          \n                  Kappa : 0.3398          \n                                          \n Mcnemar's Test P-Value : 1.221e-15       \n                                          \n            Sensitivity : 0.3833          \n            Specificity : 0.9157          \n         Pos Pred Value : 0.6609          \n         Neg Pred Value : 0.7760          \n             Prevalence : 0.3000          \n         Detection Rate : 0.1150          \n   Detection Prevalence : 0.1740          \n      Balanced Accuracy : 0.6495          \n                                          \n       'Positive' Class : mal             \n                                          \n```\n:::\n\n```{.r .cell-code}\nMLmetrics::LogLoss(yprob, as.numeric(credit$Y ) - 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.5111412\n```\n:::\n\n```{.r .cell-code}\nMLmetrics::AUC(yprob, as.numeric(credit$Y) - 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.7708857\n```\n:::\n\n```{.r .cell-code}\nMLmetrics::Gini(yprob, as.numeric(credit$Y) - 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.5365143\n```\n:::\n\n```{.r .cell-code}\nMLmetrics::KS_Stat(yprob, as.numeric(credit$Y) - 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 41.47619\n```\n:::\n:::\n\n\n#### cauchit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodelo_cauchit <- glm(fmla, data = credit,\n  family = binomial(link = cauchit)\n)\nsummary(modelo_cauchit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = fmla, family = binomial(link = cauchit), data = credit)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.0058  -0.7719  -0.4911   0.8748   2.1980  \n\nCoefficients:\n                     Estimate Std. Error z value Pr(>|z|)    \n(Intercept)         -1.463846   0.242897  -6.027 1.67e-09 ***\nCuentagood running  -2.389430   0.351330  -6.801 1.04e-11 ***\nCuentabad running   -0.547335   0.171681  -3.188 0.001432 ** \nMes                  0.042563   0.007369   5.776 7.64e-09 ***\nPpagpre mal pagador  1.119789   0.276162   4.055 5.02e-05 ***\nUsoprofesional       0.512370   0.171564   2.986 0.002822 ** \nEstcvive solo        0.573785   0.171457   3.347 0.000818 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1221.7  on 999  degrees of freedom\nResidual deviance: 1020.7  on 993  degrees of freedom\nAIC: 1034.7\n\nNumber of Fisher Scoring iterations: 7\n```\n:::\n\n```{.r .cell-code}\nyprob <- predict(modelo_cauchit, type=\"response\")\nypred <- factor(as.numeric(yprob >= 0.5 ), labels = levels(credit$Y))\n\ncaret::confusionMatrix(ypred, credit$Y, positive = \"mal\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction buen mal\n      buen  630 167\n      mal    70 133\n                                          \n               Accuracy : 0.763           \n                 95% CI : (0.7354, 0.7891)\n    No Information Rate : 0.7             \n    P-Value [Acc > NIR] : 5.274e-06       \n                                          \n                  Kappa : 0.3783          \n                                          \n Mcnemar's Test P-Value : 4.493e-10       \n                                          \n            Sensitivity : 0.4433          \n            Specificity : 0.9000          \n         Pos Pred Value : 0.6552          \n         Neg Pred Value : 0.7905          \n             Prevalence : 0.3000          \n         Detection Rate : 0.1330          \n   Detection Prevalence : 0.2030          \n      Balanced Accuracy : 0.6717          \n                                          \n       'Positive' Class : mal             \n                                          \n```\n:::\n\n```{.r .cell-code}\nMLmetrics::LogLoss(yprob, as.numeric(credit$Y ) - 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.5103524\n```\n:::\n\n```{.r .cell-code}\nMLmetrics::AUC(yprob, as.numeric(credit$Y) - 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.7720143\n```\n:::\n\n```{.r .cell-code}\nMLmetrics::Gini(yprob, as.numeric(credit$Y) - 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.5387714\n```\n:::\n\n```{.r .cell-code}\nMLmetrics::KS_Stat(yprob, as.numeric(credit$Y) - 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 40.85714\n```\n:::\n:::\n\n\n## Remuestreo\n\n### ¿Qué aprenderemos?\n\n- ¿Cómo se evalúa y selecciona un modelo predictivo?\n- Solución ideal en una situación de abundancia de datos.\n- **Validación cruzada (CV)**:\n    - Conjunto de validación\n    - *LOOCV*\n    - *k-fold CV*\n- **Bootstrapping**\n\n### Eficiencia de un método de aprendizaje\n\n- Modelo *bueno* cuando se puede **generalizar**.\n- Queremos un método de aprendizaje que funcione \nbien con datos nuevos (**error de prueba bajo**).\n\n- Cada modelo tiene una métrica *poblacional* \n(accuracy, especificidad, etc), que no depende de \nlos datos de entrenamiento ni test.\n\n- Esto es importante para:\n    - **Selección de modelos**: Estimar el \n    *rendimiento predictivo* de diferentes modelos\n    para elegir el *mejor*.\n    - **Evaluación de modelos**: Estimar su rendimiento\n    (error de predicción) del modelo final sobre un\n    nuevo conjunto de datos.\n\n\n### Error de entrenamiento vs Error de prueba\n\n- El error de prueba puede **subestimar drásticamente**\nel error de prueba.\n\n### Funciones de pérdida\n\nSe suele usar **MSE** (error cuadrático medio)\ny el **ratio de mala clasificación**.\n\n\n### El reto\n\nEn los ejemplos anteriores, **sabíamos la verdad**,\npor lo que pudimos evaluar el error de entrenamiento y prueba.\n\nEn la realidad, tal **no suele ser el caso**.\n\n\n### Situación con abundancia de datos\n\n- Es una situación *ideal*, usualmente no realista.\n\n- Consiste en que se cuenta con una muestra *tan representativa*\nde la población, que, al dividirla en **tres partes**,\ncada una de ellas es también *representativa de la población*.\n\n- Partición:\n    - **Conjunto de entrenamiento** (training set): Para **ajustar (estimar) el modelo** .\n    - **Conjunto de validación** (validation set): Para **seleccionar el mejor modelo** .\n    - **Conjunto de evaluación o prueba** (test set): Para **evaluar** qué tan bien\n    el **modelo** se ajusta un conjunto nuevo e **independiente** de datos.\n\n- Como esta situación no suele ocurrir, se hace *uso eficiente de la muestra*\naplicando técnicas de **remuestro de datos**.\n\n- Una **estrategia alternativa** para **seleccionar el modelo*** es usando\n**métodos de penalización** por complejidad, como Lasso.o AIC.\n\n::: {#prp-}\n\nTras realizar el ajuste de modelo, selección de un único modelo\ny evaluación de aquel modelo; el modelo se vuelve a estimar usando ahora\naquellos **tres conjuntos de datos juntos**, modelo que sería\nel que se aplica en producción.\n\n:::\n\n\n### Validación cruzada (CV)\n\n- Situación de **selección** de modelos, no evaluación.\n\n- No es necesariamente un muestro simple, puede ser\n*muestreo estratificado*, entre otros.\n\n- Algunas técnicas:\n    - **LOOOCV**: Validación cruzada dejando uno afuera.\n    - **K-fold Cross Validation**: Validación cruzada\n    con $K$ iteraciones; usualmente en 5 o 10 grupos.\n\n\n#### Enfoque usando un conjunto de validación\n\n\n- Suponga que cuenta con un conjunto de datos; y, **a parte**,\nun conjunto de evaluación.\n\n\n",
    "supporting": [
      "clase-07_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}