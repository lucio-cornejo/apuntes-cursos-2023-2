{
  "hash": "5646782bd5f978e3a546733ff4baa017",
  "result": {
    "markdown": "# Apuntes de clase {-}\n\n## Bias-variance tradeoff\n\nFijando un valor $x_0$ para los predictores, \ny considerando una familia de *training datasets*,\ncada uno de esos training sets produce una estimación\n$\\hat{f}$ de la función $f$. \n\nAhora, considere el siguiente resultado:\n\n::: {#thm-bias-variance}\n\n$$\nE \\left[\\left( Y - \\hat{f}\\left( x_o \\right) \\right)^2\\right]\n= \\text{ Var}\\left( \\epsilon \\right) +\n\\text{ Var}\\left( \\hat{f}\\left( x_0 \\right) \\right) +\n\\left[\\text{ Bias}\\left( \\hat{f}\\left( x_0 \\right) \\right)\\right]^2\n$$\n\n:::\n\nEl **sesgo** consiste en cómo se aleja el valor real de la variable,\nen comparacion con el promedio de las estimaciones.\n\nModelos que suelen tenor **menor sesgo**, suelen tener\n**mayor varianza**, **y viceversa**.\n\n::: {#prp-regla-general}\n\nPara modelos más flexibles, la varianza se \n*incrementa* y el sesgo *disminuye*. (not **aaalways** true).\n\n:::\n\n::: {#prp-}\n\nResulta que el punto donde el $\\text{ MSE }_{test}$ \nalcanza un mínimo (en el eje Y, siendo el eje X el nivel de flexibilidad), a vecese coincide con el punto donde\nse intersectan el *bias* y *varianza* de la estimación $\\hat{f}$ \n\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Bias-variance tradeoff](../media/bias-variance-tradeoff.jpg){width=100%}\n:::\n:::\n\n\n\n- Resumen:\n    - A medida que aumenta la complejidad (y, por tanto\n    la flexibilidad) de un modelo, el modelo se vuelve \n    más adaptable a estructuras subyacentes y cae el error\n    de entrenaminto (*sobreajuste*)\n    - El error de prueba es el error de predicción sobre una\n    muestra de prueba.\n    - Los modelos inflexibles (con parámetros para ajustarse)\n    son fáciles de calcular pero pueden producir subajuste (alto sesgo).\n    - Los modelos flexibles pueden producir sobreajuste.\n\n## Clasificación\n\n### ¿Qué es clasificación?\n\n- **Modelo politómico**: Cuando la variable respuesta cuenta\ncon *más de dos* categorías.\n\n- Cuando el objetivo es predecir, no suele tomarse en cuenta\nla **jerarquía** (en caso exista) entre las categorías de la\nvariable respuesta. Por ejemplo, en caso $Y$ sea una variable *ordinal*.\n\n- Usualmente construimos modelos que **predigan las probabilidades de categorías**, dadas ciertas covariables $X$.\n\n- No siempre los modelos de clasificación te danla clase y probabilidad de pertenencia a la clase. Los modelos por lo general\ndan solo la **probabilidad de pertenencia** a las clases. Por ejemplo:\n    - Regresión logística solo te da la probabilidad\n    de pertenencia a la clase. Sin mbargo, ni en el caso binario\n    basta la regla \"probabilidad mayor de 50%\" para asignar una\n    clase a una nueva observación.\n    - Árboles de decisión te da la clase,\n    pero no la probabilidad de pertenencia.\n\n\n### Configuración de la clasifiación general\n\n- **Contexto**:\n    - La variable respuesta cuenta con una cantidad **finita**\n    de valores posibles ... categorías. \n    - La variable respuesta $Y$ es **cualitativa**.\n\n- **Objetivo**:\n    - Construir un clasificador que **asigne una etiqueta**\n    de clasificación a una observación futura sin etiquetar, \n    además de evaluar la **incertidumbre** en esta clasificación.\n\n- **Medidas de rendimiento**\n    - La más popular es la tasa de error de clasificación\n    érronea (versión de entrenamiento y prueba).\n\n- **Pérdida 0/1**: \n    - Las clasificaciones erróneas reciben la pérdida\n    1; y las clasificaciones correctas, pérdida 0.\n    - No se emplea pérdida cuadrática para la clasificación.\n\n\n### Métodos Estadísticos Tradicionales para Clasificación\n\n- Tres métodos comúnmente usados para clasificación:\n    - Regresión Logística\n    - Análisis Discrimante Lineal (LDA)\n    - Análisis Discrimante Cuadrático (QDA)\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Generalized Linear Models](../media/glm-01.jpg){width=100%}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![Generalized Linear Models](../media/glm-02.jpg){width=100%}\n:::\n:::\n\n\n\n- La **exponential family** es una familia de distribuciones, \ntales como la Normal, Gamma, Binomial, etc.\n\n- La función $g$ se denomina **función de enlace** y debe\nsatisfacer tener inversa, $g^{-1}$, la cual se denomina\n**función de respuesta**.\n\n### Predicción con el modelo logit\n\n- Se denomina modelo logit al modelo de regresión logística binaria.\n\n- Es un modelo **ideal para interpretar**.\n- No es un modelo ideal para predecir, en especial si se\ntienen **clases desbalanceadas**.\n\n- Recuerde que desbalanceado no implica difícil de predecir.\nAdemás, *clases desbalanceadas* **no es un problema de los datos**.\n\n- Predicción\n    1. $\\hat{p}_i = \\dfrac{1}{1 + e^{-\\eta_i}}$ \n    = $\\dfrac{1}{1 + e^{-\\left( \\hat{\\beta}_0 + \\hat{\\beta}_ 1 x_{1i} + \\dots + \\hat{\\beta}_p x_{pi}\\right)}}$ \n    1. \n    $$\n    \\begin{equation}\n      \\hat{Y}_i =\n      \\begin{cases*}\n        1 & si $\\hat{p}_i \\geq c$ \\\\\n        0 & si $\\hat{p}_i < c$\n      \\end{cases*}\n    \\end{equation}\n    $$\n    donde $c$ se denomina punto de corte (umbral).  \n    Usualmente se utiliza $c = 0.50$ .\n\n::: {#thm-}\n\nSe puede demostrar (es complicado) que con la elección de umbral \n$c = 0.50$ se minimiza el error de clasificación;\nademás de ser el único umbral que minimiza tal error.\n\n:::\n\n### ¿Regresión lineal para una clasificación binaria?\n\n- La regresión lineal puede producir probabilidades\nmenores que cero o mayores que uno.\n\n### Regresión logística binaria\n\n- La response presenta solo dos categorías posibles.\n- Se modela entonces $Y_i \\sim Bernoulli(p_i)$ .\n\n- **Objetivo**: Estimar $p_i = P\\left( Y_i = 1 \\mid X_1, \\dots, X_p \\right)$ \n",
    "supporting": [
      "clase-03_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}