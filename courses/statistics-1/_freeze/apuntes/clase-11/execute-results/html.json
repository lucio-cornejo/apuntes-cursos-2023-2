{
  "hash": "3903d95dd20ae33038efeb731f50bd04",
  "result": {
    "markdown": "# Apuntes de clase {-}\n\n## Boosting\n\n- La idea principal es realizar un **ponderación con varios modelos**.\n- Es **más complejo** que **bagging** respecto a **cómo calibrarlo** (la ponderación).\n\n### ¿Cómo funciona?\n\n1. Asignar pesos a las unidades de entrenamiento.\n1. Entrenar iterativamente $k$ clasificadores.\n1. Tras entrenar el clasificador $M_i$, **actualizar los pesos**\nde manera que el siguiente clasificador $M_{i+1}$ ponga más atención\na las unidades entrenamiento que $M_i$ **clasificó erróneamente**.\n1. El clasificador final combina los votos de cada clasificador individual,\ndonde el peso del voto de cada clasificador es una función de su precisión.\n\n### AdaBoost (Adaptative Boosting)\n\n- Este algoritmo no surgió de Estadística ... **no hay supuestos**\n(normalidad, etc) sobre los predictores y la response.\n\n- Propiamnte dicho, el muestro de AdaBoost no es como en Bootstrap,\npues las observaciones no siempre tienen el mismo peso al ser sampled with replacement.\n\n- A **mayor peso** de unidad, **más veces fue wrongly classified**.\n\n- La idea es **centrarse más en las unidades que en el paso anterior fueron wrongly classified**.\n\n- En cada loop, el error de los modelos se calcula respecto a los \n**mismos datos con los que fue entrenado** (no validation ni testing sets).\n\n::: {#prp-}\n\nAdaBoost no es adecuado de usar cuando las clases están **muy desbalanceadas**.\nEsto debido a que AdaBoost usa el **error** (complemnto de accuracy)\npara la ponderación de los clasificadores; pese a que otras métricas,\ncomo **sensibilidad** son más adecuadas para medir la eficiencia del modelo\ncomo predictor.\n\n:::\n\n\n#### Algoritmo\n\n- **Input:**\n    - $D$: datos con $d$ unidades clasificadas\n    - $k$ (**hiperparámetro**): número de iteracions (se genera un clasificador por iteración)\n\n- **Output:** Modelo compuesto\n    1. Inicializar peso de unidad como $\\dfrac{1}{d}$ .\n    1. **Para cada iteración**, muestrear $D$ **con reemplazo** de acuerdo a los pesos, para obtener $D_i$ .\n    1. Usar $D_i$ para derivar $M_i$ .\n    1. Calcular $\\text{ error}\\left( M_i \\right) = \\displaystyle{ \\sum_{j=1}^{d}} w_j \\text{ err}\\left( X_j \\right)$ , \n    donde $\\text{ err }\\left( X_j \\right)$ indica si la unidad $X_j$ fue mal clasificada.\n    1. Si $\\text{ error }\\left( M_i \\right) > 0.5$, volver al paso 2.\n    1. Para cada unidad en $D_i$ **bien clasificada**, multiplicar su peso por \n    $\\dfrac{\\text{ error}\\left( M_i \\right)}{1- \\text{ error}\\left( M_i \\right)}$ \n    1. Actualizar los pesos y normalizarlos.\n\n#### Peso de clasificadores\n\n- A **menor tasa de error**, el clasificador es más exacto, así que su **voto vale más**.\n\n- El peso del voto del clasificador $M_i$ es \n$\\log \\left( \\dfrac{1- \\text{ error}\\left( M_i \\right)}{\\text{ error}\\left( M_i \\right)} \\right)$ .\n\n- Para cada clase $c$, se suman los pesos de cada clasificador que asignó\nla clase $c$ a la unidad de observación $X$. Luego, **gana** la clase con la\n**mayor suma de pesos**, y esa corresponde a la predicción asignada a $X$ .\n\n\n## Bagging vs Boosting\n\n- Como Boosting se focaliza en las **unidades mal clasificadas**,\ncorre el riesgo de **sobreajustar el modelo resultante a esos datos**;\nmientras que **Bagging es menos susceptible a un sobreajuste del modelo**.\n\n- Algunas veces, el modelo resultante puede ser **menos preciso**\nque un modelo único para los mismos datos.\n\n- Boosting tiende a lograr **mayor precisión** que Bagging,\ndel **modelo resultante**, en comparación a un modelo único.\n\n\n## Ejemplo práctico: AdaBag\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(adabag)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: rpart\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'rpart' was built under R version 4.1.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: caret\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'caret' was built under R version 4.1.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: ggplot2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'ggplot2' was built under R version 4.1.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: lattice\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: foreach\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'foreach' was built under R version 4.1.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: doParallel\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'doParallel' was built under R version 4.1.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: iterators\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'iterators' was built under R version 4.1.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: parallel\n```\n:::\n\n```{.r .cell-code}\nlibrary(rpart)\ndata(irir)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in data(irir): data set 'irir' not found\n```\n:::\n\n```{.r .cell-code}\nhead(iris)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Seleccion de datos de trabajo o entrenamiento,\n# se seleccionan 25 de cada especie\ntrain <- c(\n  sample(1:50, 25), sample(51:100, 25), sample(101:150, 25)\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Modelamos la especie a partir de todas las variables en los datos de entrenamiento\n# mfinal -> N de iteraciones\n# boos -> Permite submuestrear para generar los pesos\niris.adaboost <- adabag::boosting(\n  Species ~ ., data = iris[train,], \n  boos = TRUE, mfinal = 10\n)\niris.adaboost\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$formula\nSpecies ~ .\n\n$trees\n$trees[[1]]\nn= 75 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 75 48 virginica (0.29333333 0.34666667 0.36000000)  \n  2) Petal.Length< 4.85 47 22 versicolor (0.46808511 0.53191489 0.00000000)  \n    4) Petal.Length< 2.45 22  0 setosa (1.00000000 0.00000000 0.00000000) *\n    5) Petal.Length>=2.45 25  0 versicolor (0.00000000 1.00000000 0.00000000) *\n  3) Petal.Length>=4.85 28  1 virginica (0.00000000 0.03571429 0.96428571) *\n\n$trees[[2]]\nn= 75 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 75 48 setosa (0.36000000 0.30666667 0.33333333)  \n  2) Petal.Length< 2.6 27  0 setosa (1.00000000 0.00000000 0.00000000) *\n  3) Petal.Length>=2.6 48 23 virginica (0.00000000 0.47916667 0.52083333)  \n    6) Petal.Width< 1.75 23  1 versicolor (0.00000000 0.95652174 0.04347826) *\n    7) Petal.Width>=1.75 25  1 virginica (0.00000000 0.04000000 0.96000000) *\n\n$trees[[3]]\nn= 75 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 75 48 virginica (0.3333333 0.3066667 0.3600000)  \n  2) Petal.Length< 2.8 25  0 setosa (1.0000000 0.0000000 0.0000000) *\n  3) Petal.Length>=2.8 50 23 virginica (0.0000000 0.4600000 0.5400000)  \n    6) Petal.Length< 4.75 16  0 versicolor (0.0000000 1.0000000 0.0000000) *\n    7) Petal.Length>=4.75 34  7 virginica (0.0000000 0.2058824 0.7941176) *\n\n$trees[[4]]\nn= 75 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 75 34 versicolor (0.2000000 0.5466667 0.2533333)  \n  2) Petal.Length< 2.6 15  0 setosa (1.0000000 0.0000000 0.0000000) *\n  3) Petal.Length>=2.6 60 19 versicolor (0.0000000 0.6833333 0.3166667)  \n    6) Petal.Width< 1.9 48  7 versicolor (0.0000000 0.8541667 0.1458333) *\n    7) Petal.Width>=1.9 12  0 virginica (0.0000000 0.0000000 1.0000000) *\n\n$trees[[5]]\nn= 75 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 75 43 virginica (0.17333333 0.40000000 0.42666667)  \n  2) Petal.Width< 1.75 43 16 versicolor (0.30232558 0.62790698 0.06976744)  \n    4) Sepal.Length< 5.55 13  0 setosa (1.00000000 0.00000000 0.00000000) *\n    5) Sepal.Length>=5.55 30  3 versicolor (0.00000000 0.90000000 0.10000000) *\n  3) Petal.Width>=1.75 32  3 virginica (0.00000000 0.09375000 0.90625000) *\n\n$trees[[6]]\nn= 75 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 75 39 versicolor (0.1466667 0.4800000 0.3733333)  \n   2) Petal.Length< 2.45 11  0 setosa (1.0000000 0.0000000 0.0000000) *\n   3) Petal.Length>=2.45 64 28 versicolor (0.0000000 0.5625000 0.4375000)  \n     6) Petal.Length< 4.85 32  4 versicolor (0.0000000 0.8750000 0.1250000)  \n      12) Sepal.Length< 6.05 25  0 versicolor (0.0000000 1.0000000 0.0000000) *\n      13) Sepal.Length>=6.05 7  3 virginica (0.0000000 0.4285714 0.5714286) *\n     7) Petal.Length>=4.85 32  8 virginica (0.0000000 0.2500000 0.7500000) *\n\n$trees[[7]]\nn= 75 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 75 42 versicolor (0.1333333 0.4400000 0.4266667)  \n   2) Petal.Length< 2.45 10  0 setosa (1.0000000 0.0000000 0.0000000) *\n   3) Petal.Length>=2.45 65 32 versicolor (0.0000000 0.5076923 0.4923077)  \n     6) Petal.Length< 5.05 49 16 versicolor (0.0000000 0.6734694 0.3265306)  \n      12) Sepal.Width>=2.25 42  9 versicolor (0.0000000 0.7857143 0.2142857)  \n        24) Petal.Width< 1.75 26  0 versicolor (0.0000000 1.0000000 0.0000000) *\n        25) Petal.Width>=1.75 16  7 virginica (0.0000000 0.4375000 0.5625000) *\n      13) Sepal.Width< 2.25 7  0 virginica (0.0000000 0.0000000 1.0000000) *\n     7) Petal.Length>=5.05 16  0 virginica (0.0000000 0.0000000 1.0000000) *\n\n$trees[[8]]\nn= 75 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 75 26 versicolor (0.06666667 0.65333333 0.28000000)  \n   2) Petal.Length< 5.05 68 19 versicolor (0.07352941 0.72058824 0.20588235)  \n     4) Sepal.Width>=2.85 44  5 versicolor (0.11363636 0.88636364 0.00000000)  \n       8) Petal.Length< 4.5 7  2 setosa (0.71428571 0.28571429 0.00000000) *\n       9) Petal.Length>=4.5 37  0 versicolor (0.00000000 1.00000000 0.00000000) *\n     5) Sepal.Width< 2.85 24 10 virginica (0.00000000 0.41666667 0.58333333)  \n      10) Petal.Width< 1.45 10  0 versicolor (0.00000000 1.00000000 0.00000000) *\n      11) Petal.Width>=1.45 14  0 virginica (0.00000000 0.00000000 1.00000000) *\n   3) Petal.Length>=5.05 7  0 virginica (0.00000000 0.00000000 1.00000000) *\n\n$trees[[9]]\nn= 75 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 75 27 versicolor (0.06666667 0.64000000 0.29333333)  \n  2) Petal.Length< 5.05 66 18 versicolor (0.07575758 0.72727273 0.19696970)  \n    4) Sepal.Width>=2.85 52  8 versicolor (0.07692308 0.84615385 0.07692308) *\n    5) Sepal.Width< 2.85 14  5 virginica (0.07142857 0.28571429 0.64285714) *\n  3) Petal.Length>=5.05 9  0 virginica (0.00000000 0.00000000 1.00000000) *\n\n$trees[[10]]\nn= 75 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 75 37 versicolor (0.18666667 0.50666667 0.30666667)  \n   2) Sepal.Length< 5.35 14  0 setosa (1.00000000 0.00000000 0.00000000) *\n   3) Sepal.Length>=5.35 61 23 versicolor (0.00000000 0.62295082 0.37704918)  \n     6) Petal.Length< 4.85 35  3 versicolor (0.00000000 0.91428571 0.08571429) *\n     7) Petal.Length>=4.85 26  6 virginica (0.00000000 0.23076923 0.76923077)  \n      14) Petal.Width< 1.75 7  1 versicolor (0.00000000 0.85714286 0.14285714) *\n      15) Petal.Width>=1.75 19  0 virginica (0.00000000 0.00000000 1.00000000) *\n\n\n$weights\n [1] 1.7986561 1.8633467 0.9054636 0.8980642 0.5340658 0.8290422 0.9012836\n [8] 1.5820530 0.6341534 0.7857651\n\n$votes\n            [,1]       [,2]       [,3]\n [1,]  8.7779095  1.9539843  0.0000000\n [2,] 10.0977403  0.6341534  0.0000000\n [3,] 10.0977403  0.6341534  0.0000000\n [4,] 10.0977403  0.6341534  0.0000000\n [5,] 10.0977403  0.6341534  0.0000000\n [6,] 10.0977403  0.6341534  0.0000000\n [7,] 10.0977403  0.6341534  0.0000000\n [8,]  9.3119753  1.4199185  0.0000000\n [9,] 10.0977403  0.6341534  0.0000000\n[10,] 10.0977403  0.6341534  0.0000000\n[11,]  8.7779095  1.9539843  0.0000000\n[12,]  9.3119753  1.4199185  0.0000000\n[13,] 10.0977403  0.6341534  0.0000000\n[14,] 10.0977403  0.6341534  0.0000000\n[15,] 10.0977403  0.6341534  0.0000000\n[16,] 10.0977403  0.6341534  0.0000000\n[17,] 10.0977403  0.6341534  0.0000000\n[18,] 10.0977403  0.6341534  0.0000000\n[19,]  8.5156874  1.5820530  0.6341534\n[20,] 10.0977403  0.6341534  0.0000000\n[21,] 10.0977403  0.6341534  0.0000000\n[22,] 10.0977403  0.6341534  0.0000000\n[23,]  9.3119753  1.4199185  0.0000000\n[24,] 10.0977403  0.6341534  0.0000000\n[25,] 10.0977403  0.6341534  0.0000000\n[26,]  0.0000000  6.5277340  4.2041597\n[27,]  0.0000000  9.9028516  0.8290422\n[28,]  0.0000000  9.9028516  0.8290422\n[29,]  0.0000000 10.0977403  0.6341534\n[30,]  1.5820530  8.3207986  0.8290422\n[31,]  0.0000000  9.9028516  0.8290422\n[32,]  0.0000000  9.2686981  1.4631956\n[33,]  0.0000000  8.3632345  2.3686592\n[34,]  0.0000000 10.0977403  0.6341534\n[35,]  0.0000000  9.9028516  0.8290422\n[36,]  0.0000000  9.9028516  0.8290422\n[37,]  1.5820530  9.1498408  0.0000000\n[38,]  1.5820530  9.1498408  0.0000000\n[39,]  1.5820530  9.1498408  0.0000000\n[40,]  0.0000000 10.0977403  0.6341534\n[41,]  0.0000000  9.9028516  0.8290422\n[42,]  0.0000000 10.0977403  0.6341534\n[43,]  0.5340658  9.5636746  0.6341534\n[44,]  0.0000000  9.9028516  0.8290422\n[45,]  0.0000000 10.0977403  0.6341534\n[46,]  0.0000000  9.1964567  1.5354370\n[47,]  0.0000000 10.7318938  0.0000000\n[48,]  0.0000000  7.1987318  3.5331619\n[49,]  1.3198309  8.7779095  0.6341534\n[50,]  1.5820530  9.1498408  0.0000000\n[51,]  0.0000000  0.0000000 10.7318938\n[52,]  0.0000000  0.0000000 10.7318938\n[53,]  0.0000000  0.0000000 10.7318938\n[54,]  0.0000000  0.0000000 10.7318938\n[55,]  0.0000000  0.0000000 10.7318938\n[56,]  0.0000000  0.0000000 10.7318938\n[57,]  0.0000000  0.0000000 10.7318938\n[58,]  0.0000000  0.0000000 10.7318938\n[59,]  0.0000000  0.0000000 10.7318938\n[60,]  0.0000000  0.0000000 10.7318938\n[61,]  0.0000000  0.8980642  9.8338295\n[62,]  0.0000000  0.0000000 10.7318938\n[63,]  0.0000000  0.0000000 10.7318938\n[64,]  0.0000000  0.8980642  9.8338295\n[65,]  0.0000000  0.0000000 10.7318938\n[66,]  0.0000000  0.0000000 10.7318938\n[67,]  0.0000000  0.0000000 10.7318938\n[68,]  0.0000000  3.1142706  7.6176231\n[69,]  0.0000000  0.8980642  9.8338295\n[70,]  0.0000000  0.0000000 10.7318938\n[71,]  0.0000000  4.0812418  6.6506519\n[72,]  0.0000000  3.4824854  7.2494083\n[73,]  0.0000000  0.0000000 10.7318938\n[74,]  0.0000000  0.0000000 10.7318938\n[75,]  0.0000000  0.0000000 10.7318938\n\n$prob\n            [,1]       [,2]       [,3]\n [1,] 0.81792736 0.18207264 0.00000000\n [2,] 0.94090946 0.05909054 0.00000000\n [3,] 0.94090946 0.05909054 0.00000000\n [4,] 0.94090946 0.05909054 0.00000000\n [5,] 0.94090946 0.05909054 0.00000000\n [6,] 0.94090946 0.05909054 0.00000000\n [7,] 0.94090946 0.05909054 0.00000000\n [8,] 0.86769171 0.13230829 0.00000000\n [9,] 0.94090946 0.05909054 0.00000000\n[10,] 0.94090946 0.05909054 0.00000000\n[11,] 0.81792736 0.18207264 0.00000000\n[12,] 0.86769171 0.13230829 0.00000000\n[13,] 0.94090946 0.05909054 0.00000000\n[14,] 0.94090946 0.05909054 0.00000000\n[15,] 0.94090946 0.05909054 0.00000000\n[16,] 0.94090946 0.05909054 0.00000000\n[17,] 0.94090946 0.05909054 0.00000000\n[18,] 0.94090946 0.05909054 0.00000000\n[19,] 0.79349345 0.14741601 0.05909054\n[20,] 0.94090946 0.05909054 0.00000000\n[21,] 0.94090946 0.05909054 0.00000000\n[22,] 0.94090946 0.05909054 0.00000000\n[23,] 0.86769171 0.13230829 0.00000000\n[24,] 0.94090946 0.05909054 0.00000000\n[25,] 0.94090946 0.05909054 0.00000000\n[26,] 0.00000000 0.60825556 0.39174444\n[27,] 0.00000000 0.92274968 0.07725032\n[28,] 0.00000000 0.92274968 0.07725032\n[29,] 0.00000000 0.94090946 0.05909054\n[30,] 0.14741601 0.77533367 0.07725032\n[31,] 0.00000000 0.92274968 0.07725032\n[32,] 0.00000000 0.86365914 0.13634086\n[33,] 0.00000000 0.77928786 0.22071214\n[34,] 0.00000000 0.94090946 0.05909054\n[35,] 0.00000000 0.92274968 0.07725032\n[36,] 0.00000000 0.92274968 0.07725032\n[37,] 0.14741601 0.85258399 0.00000000\n[38,] 0.14741601 0.85258399 0.00000000\n[39,] 0.14741601 0.85258399 0.00000000\n[40,] 0.00000000 0.94090946 0.05909054\n[41,] 0.00000000 0.92274968 0.07725032\n[42,] 0.00000000 0.94090946 0.05909054\n[43,] 0.04976436 0.89114510 0.05909054\n[44,] 0.00000000 0.92274968 0.07725032\n[45,] 0.00000000 0.94090946 0.05909054\n[46,] 0.00000000 0.85692767 0.14307233\n[47,] 0.00000000 1.00000000 0.00000000\n[48,] 0.00000000 0.67077927 0.32922073\n[49,] 0.12298210 0.81792736 0.05909054\n[50,] 0.14741601 0.85258399 0.00000000\n[51,] 0.00000000 0.00000000 1.00000000\n[52,] 0.00000000 0.00000000 1.00000000\n[53,] 0.00000000 0.00000000 1.00000000\n[54,] 0.00000000 0.00000000 1.00000000\n[55,] 0.00000000 0.00000000 1.00000000\n[56,] 0.00000000 0.00000000 1.00000000\n[57,] 0.00000000 0.00000000 1.00000000\n[58,] 0.00000000 0.00000000 1.00000000\n[59,] 0.00000000 0.00000000 1.00000000\n[60,] 0.00000000 0.00000000 1.00000000\n[61,] 0.00000000 0.08368180 0.91631820\n[62,] 0.00000000 0.00000000 1.00000000\n[63,] 0.00000000 0.00000000 1.00000000\n[64,] 0.00000000 0.08368180 0.91631820\n[65,] 0.00000000 0.00000000 1.00000000\n[66,] 0.00000000 0.00000000 1.00000000\n[67,] 0.00000000 0.00000000 1.00000000\n[68,] 0.00000000 0.29018836 0.70981164\n[69,] 0.00000000 0.08368180 0.91631820\n[70,] 0.00000000 0.00000000 1.00000000\n[71,] 0.00000000 0.38029093 0.61970907\n[72,] 0.00000000 0.32449869 0.67550131\n[73,] 0.00000000 0.00000000 1.00000000\n[74,] 0.00000000 0.00000000 1.00000000\n[75,] 0.00000000 0.00000000 1.00000000\n\n$class\n [1] \"setosa\"     \"setosa\"     \"setosa\"     \"setosa\"     \"setosa\"    \n [6] \"setosa\"     \"setosa\"     \"setosa\"     \"setosa\"     \"setosa\"    \n[11] \"setosa\"     \"setosa\"     \"setosa\"     \"setosa\"     \"setosa\"    \n[16] \"setosa\"     \"setosa\"     \"setosa\"     \"setosa\"     \"setosa\"    \n[21] \"setosa\"     \"setosa\"     \"setosa\"     \"setosa\"     \"setosa\"    \n[26] \"versicolor\" \"versicolor\" \"versicolor\" \"versicolor\" \"versicolor\"\n[31] \"versicolor\" \"versicolor\" \"versicolor\" \"versicolor\" \"versicolor\"\n[36] \"versicolor\" \"versicolor\" \"versicolor\" \"versicolor\" \"versicolor\"\n[41] \"versicolor\" \"versicolor\" \"versicolor\" \"versicolor\" \"versicolor\"\n[46] \"versicolor\" \"versicolor\" \"versicolor\" \"versicolor\" \"versicolor\"\n[51] \"virginica\"  \"virginica\"  \"virginica\"  \"virginica\"  \"virginica\" \n[56] \"virginica\"  \"virginica\"  \"virginica\"  \"virginica\"  \"virginica\" \n[61] \"virginica\"  \"virginica\"  \"virginica\"  \"virginica\"  \"virginica\" \n[66] \"virginica\"  \"virginica\"  \"virginica\"  \"virginica\"  \"virginica\" \n[71] \"virginica\"  \"virginica\"  \"virginica\"  \"virginica\"  \"virginica\" \n\n$importance\nPetal.Length  Petal.Width Sepal.Length  Sepal.Width \n   65.483536    22.138266     6.220378     6.157821 \n\n$terms\nSpecies ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width\nattr(,\"variables\")\nlist(Species, Sepal.Length, Sepal.Width, Petal.Length, Petal.Width)\nattr(,\"factors\")\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSpecies                 0           0            0           0\nSepal.Length            1           0            0           0\nSepal.Width             0           1            0           0\nPetal.Length            0           0            1           0\nPetal.Width             0           0            0           1\nattr(,\"term.labels\")\n[1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\" \nattr(,\"order\")\n[1] 1 1 1 1\nattr(,\"intercept\")\n[1] 1\nattr(,\"response\")\n[1] 1\nattr(,\".Environment\")\n<environment: R_GlobalEnv>\nattr(,\"predvars\")\nlist(Species, Sepal.Length, Sepal.Width, Petal.Length, Petal.Width)\nattr(,\"dataClasses\")\n     Species Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n    \"factor\"    \"numeric\"    \"numeric\"    \"numeric\"    \"numeric\" \n\n$call\nadabag::boosting(formula = Species ~ ., data = iris[train, ], \n    boos = TRUE, mfinal = 10)\n\nattr(,\"vardep.summary\")\n    setosa versicolor  virginica \n        25         25         25 \nattr(,\"class\")\n[1] \"boosting\"\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Graficamos las variables según su porcentaje de relevancia  \n# El largo de pétalo es la más relevante.\nbarplot(\n  iris.adaboost$imp[order(iris.adaboost$imp, decreasing = TRUE)],\n  ylim = c(0, 100), main = \"Importancia Relativa de las Variables\",\n  col = \"lightblue\"\n)\n```\n\n::: {.cell-output-display}\n![](clase-11_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Ajustamos a los nuevos datos, y vemos el ajuste\niris.predboosting <- adabag::predict.boosting(\n  iris.adaboost, newdata = iris[-train,]\n)\niris.predboosting\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$formula\nSpecies ~ .\n\n$votes\n            [,1]       [,2]       [,3]\n [1,] 10.0977403  0.6341534  0.0000000\n [2,] 10.0977403  0.6341534  0.0000000\n [3,] 10.0977403  0.6341534  0.0000000\n [4,]  9.3119753  1.4199185  0.0000000\n [5,] 10.0977403  0.6341534  0.0000000\n [6,]  8.7779095  1.9539843  0.0000000\n [7,]  9.3119753  1.4199185  0.0000000\n [8,] 10.0977403  0.6341534  0.0000000\n [9,] 10.0977403  0.6341534  0.0000000\n[10,]  9.3119753  1.4199185  0.0000000\n[11,] 10.0977403  0.6341534  0.0000000\n[12,] 10.0977403  0.6341534  0.0000000\n[13,] 10.0977403  0.6341534  0.0000000\n[14,] 10.0977403  0.6341534  0.0000000\n[15,] 10.0977403  0.6341534  0.0000000\n[16,] 10.0977403  0.6341534  0.0000000\n[17,]  9.3119753  1.4199185  0.0000000\n[18,] 10.0977403  0.6341534  0.0000000\n[19,] 10.0977403  0.6341534  0.0000000\n[20,] 10.0977403  0.6341534  0.0000000\n[21,] 10.0977403  0.6341534  0.0000000\n[22,] 10.0977403  0.6341534  0.0000000\n[23,] 10.0977403  0.6341534  0.0000000\n[24,] 10.0977403  0.6341534  0.0000000\n[25,] 10.0977403  0.6341534  0.0000000\n[26,]  0.0000000  7.1987318  3.5331619\n[27,]  0.0000000  7.6866452  3.0452486\n[28,]  0.0000000 10.0977403  0.6341534\n[29,]  1.3198309  8.7779095  0.6341534\n[30,]  1.3198309  8.7779095  0.6341534\n[31,]  1.3198309  7.8766259  1.5354370\n[32,]  1.5820530  9.1498408  0.0000000\n[33,]  1.5820530  8.3207986  0.8290422\n[34,]  0.0000000 10.7318938  0.0000000\n[35,]  0.0000000  6.7853616  3.9465322\n[36,]  0.0000000 10.0977403  0.6341534\n[37,]  0.0000000  4.9825254  5.7493683\n[38,]  0.0000000  9.2686981  1.4631956\n[39,]  1.5820530  8.3207986  0.8290422\n[40,]  0.0000000 10.7318938  0.0000000\n[41,]  0.5340658  9.5636746  0.6341534\n[42,]  0.5340658  9.5636746  0.6341534\n[43,]  0.0000000  4.0812418  6.6506519\n[44,]  0.5340658 10.1978280  0.0000000\n[45,]  0.0000000  9.2686981  1.4631956\n[46,]  0.5340658  9.5636746  0.6341534\n[47,]  0.5340658  9.5636746  0.6341534\n[48,]  1.5820530  8.3207986  0.8290422\n[49,]  1.3198309  8.7779095  0.6341534\n[50,]  0.0000000 10.0977403  0.6341534\n[51,]  0.0000000  0.0000000 10.7318938\n[52,]  0.0000000  0.8980642  9.8338295\n[53,]  0.0000000  0.0000000 10.7318938\n[54,]  1.3198309  7.1958565  2.2162064\n[55,]  0.0000000  0.8980642  9.8338295\n[56,]  0.0000000  0.8980642  9.8338295\n[57,]  0.0000000  0.0000000 10.7318938\n[58,]  0.0000000  0.0000000 10.7318938\n[59,]  0.0000000  0.8980642  9.8338295\n[60,]  0.0000000  0.0000000 10.7318938\n[61,]  0.0000000  0.0000000 10.7318938\n[62,]  0.0000000  0.0000000 10.7318938\n[63,]  0.0000000  0.0000000 10.7318938\n[64,]  0.0000000  4.0812418  6.6506519\n[65,]  0.0000000  0.0000000 10.7318938\n[66,]  0.0000000  4.0812418  6.6506519\n[67,]  0.0000000  4.0812418  6.6506519\n[68,]  0.0000000  0.0000000 10.7318938\n[69,]  0.0000000  0.8980642  9.8338295\n[70,]  0.0000000  6.5277340  4.2041597\n[71,]  0.0000000  0.0000000 10.7318938\n[72,]  0.0000000  0.0000000 10.7318938\n[73,]  0.0000000  0.0000000 10.7318938\n[74,]  0.0000000  0.0000000 10.7318938\n[75,]  0.0000000  0.0000000 10.7318938\n\n$prob\n            [,1]       [,2]       [,3]\n [1,] 0.94090946 0.05909054 0.00000000\n [2,] 0.94090946 0.05909054 0.00000000\n [3,] 0.94090946 0.05909054 0.00000000\n [4,] 0.86769171 0.13230829 0.00000000\n [5,] 0.94090946 0.05909054 0.00000000\n [6,] 0.81792736 0.18207264 0.00000000\n [7,] 0.86769171 0.13230829 0.00000000\n [8,] 0.94090946 0.05909054 0.00000000\n [9,] 0.94090946 0.05909054 0.00000000\n[10,] 0.86769171 0.13230829 0.00000000\n[11,] 0.94090946 0.05909054 0.00000000\n[12,] 0.94090946 0.05909054 0.00000000\n[13,] 0.94090946 0.05909054 0.00000000\n[14,] 0.94090946 0.05909054 0.00000000\n[15,] 0.94090946 0.05909054 0.00000000\n[16,] 0.94090946 0.05909054 0.00000000\n[17,] 0.86769171 0.13230829 0.00000000\n[18,] 0.94090946 0.05909054 0.00000000\n[19,] 0.94090946 0.05909054 0.00000000\n[20,] 0.94090946 0.05909054 0.00000000\n[21,] 0.94090946 0.05909054 0.00000000\n[22,] 0.94090946 0.05909054 0.00000000\n[23,] 0.94090946 0.05909054 0.00000000\n[24,] 0.94090946 0.05909054 0.00000000\n[25,] 0.94090946 0.05909054 0.00000000\n[26,] 0.00000000 0.67077927 0.32922073\n[27,] 0.00000000 0.71624313 0.28375687\n[28,] 0.00000000 0.94090946 0.05909054\n[29,] 0.12298210 0.81792736 0.05909054\n[30,] 0.12298210 0.81792736 0.05909054\n[31,] 0.12298210 0.73394557 0.14307233\n[32,] 0.14741601 0.85258399 0.00000000\n[33,] 0.14741601 0.77533367 0.07725032\n[34,] 0.00000000 1.00000000 0.00000000\n[35,] 0.00000000 0.63226134 0.36773866\n[36,] 0.00000000 0.94090946 0.05909054\n[37,] 0.00000000 0.46427271 0.53572729\n[38,] 0.00000000 0.86365914 0.13634086\n[39,] 0.14741601 0.77533367 0.07725032\n[40,] 0.00000000 1.00000000 0.00000000\n[41,] 0.04976436 0.89114510 0.05909054\n[42,] 0.04976436 0.89114510 0.05909054\n[43,] 0.00000000 0.38029093 0.61970907\n[44,] 0.04976436 0.95023564 0.00000000\n[45,] 0.00000000 0.86365914 0.13634086\n[46,] 0.04976436 0.89114510 0.05909054\n[47,] 0.04976436 0.89114510 0.05909054\n[48,] 0.14741601 0.77533367 0.07725032\n[49,] 0.12298210 0.81792736 0.05909054\n[50,] 0.00000000 0.94090946 0.05909054\n[51,] 0.00000000 0.00000000 1.00000000\n[52,] 0.00000000 0.08368180 0.91631820\n[53,] 0.00000000 0.00000000 1.00000000\n[54,] 0.12298210 0.67051134 0.20650655\n[55,] 0.00000000 0.08368180 0.91631820\n[56,] 0.00000000 0.08368180 0.91631820\n[57,] 0.00000000 0.00000000 1.00000000\n[58,] 0.00000000 0.00000000 1.00000000\n[59,] 0.00000000 0.08368180 0.91631820\n[60,] 0.00000000 0.00000000 1.00000000\n[61,] 0.00000000 0.00000000 1.00000000\n[62,] 0.00000000 0.00000000 1.00000000\n[63,] 0.00000000 0.00000000 1.00000000\n[64,] 0.00000000 0.38029093 0.61970907\n[65,] 0.00000000 0.00000000 1.00000000\n[66,] 0.00000000 0.38029093 0.61970907\n[67,] 0.00000000 0.38029093 0.61970907\n[68,] 0.00000000 0.00000000 1.00000000\n[69,] 0.00000000 0.08368180 0.91631820\n[70,] 0.00000000 0.60825556 0.39174444\n[71,] 0.00000000 0.00000000 1.00000000\n[72,] 0.00000000 0.00000000 1.00000000\n[73,] 0.00000000 0.00000000 1.00000000\n[74,] 0.00000000 0.00000000 1.00000000\n[75,] 0.00000000 0.00000000 1.00000000\n\n$class\n [1] \"setosa\"     \"setosa\"     \"setosa\"     \"setosa\"     \"setosa\"    \n [6] \"setosa\"     \"setosa\"     \"setosa\"     \"setosa\"     \"setosa\"    \n[11] \"setosa\"     \"setosa\"     \"setosa\"     \"setosa\"     \"setosa\"    \n[16] \"setosa\"     \"setosa\"     \"setosa\"     \"setosa\"     \"setosa\"    \n[21] \"setosa\"     \"setosa\"     \"setosa\"     \"setosa\"     \"setosa\"    \n[26] \"versicolor\" \"versicolor\" \"versicolor\" \"versicolor\" \"versicolor\"\n[31] \"versicolor\" \"versicolor\" \"versicolor\" \"versicolor\" \"versicolor\"\n[36] \"versicolor\" \"virginica\"  \"versicolor\" \"versicolor\" \"versicolor\"\n[41] \"versicolor\" \"versicolor\" \"virginica\"  \"versicolor\" \"versicolor\"\n[46] \"versicolor\" \"versicolor\" \"versicolor\" \"versicolor\" \"versicolor\"\n[51] \"virginica\"  \"virginica\"  \"virginica\"  \"versicolor\" \"virginica\" \n[56] \"virginica\"  \"virginica\"  \"virginica\"  \"virginica\"  \"virginica\" \n[61] \"virginica\"  \"virginica\"  \"virginica\"  \"virginica\"  \"virginica\" \n[66] \"virginica\"  \"virginica\"  \"virginica\"  \"virginica\"  \"versicolor\"\n[71] \"virginica\"  \"virginica\"  \"virginica\"  \"virginica\"  \"virginica\" \n\n$confusion\n               Observed Class\nPredicted Class setosa versicolor virginica\n     setosa         25          0         0\n     versicolor      0         23         2\n     virginica       0          2        23\n\n$error\n[1] 0.05333333\n```\n:::\n:::\n\n\n## Máquinas de Soporte Vectorial (SVM)\n\n- La idea principal es usar un **hiperplano** como frontera\npara **realizar clasificaciones**.\n\n### Clasificación usando hiperplanos separadores\n\n- Considere el conjunto de observaciones como una matriz \n$X$ cuyas filas son las observaciones; y, las columnas\nrepresentan a los predictores.\n\n- Trataremos primero el caso de **clasificación binaria**,\nconsiderando que las observaciones pertenecen a dos clases, \n$1$  y $-1$.\n\n- Así, un **hiperplano separador** tendrá la propiedad\n$y_i \\left( \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_p x_{ip} \\right) > 0$ \npara todo $i = 1, \\dots, n$.\n\n- $x^*$ es clasificado basado en el signo de\n$f(x^*) = \\beta_0 + \\beta_1 x_{1}^* + \\cdots + \\beta_p x_{p}^*$:\n    - Si **$f(x^*)$ está lejos de cero**, entonces $x$ \n    cae lejos del hiperplano, por lo\n    que podemos estar **confiados acerca de la clasificación**.\n    - Si **$f(x^*)$ está cerca de cero**, entonces $x$ \n    cae cerca del hiperplano, por lo\n    que podemos estar **menos confiados acerca de la clasificación**.\n\n### Maximal margin classifier\n\n- El **hiperplano de frontera máxima**, también conocido como el \n**hiperplano separador óptimo**, es el hiperplano separador que\nse encuentra **más lejos** de las observaciones de entrenamiento.\n\n- Es posible que usar el **hiperplano de frontera máxima**\ngenere un **modelo sobreajustado**.\n\n::: {#prp-}\n\nSe espera que el clasificador que tenga una mayor frontera para los\ndatos de entrenamiento tambi´en lo tenga para los datos de preuba.\n\n:::\n\n- El **hiperplano de fontera máxima** es **sensible respecto a los outliers**.\n\n- Las observaciones/**vectores** que son **equidistantes** al hiperplano\nde frontera máxima son conocida como los **vectores de soporte**.\n\n::: {#prp-}\n\nUna propiedad muy importante es que el hiperplano de frontera máxima\ndepende directamente de estos vectores de soporte, pero \n**no de las otras observaciones**.\n\n:::\n\n### Construcción del clasificador de frontera máxima\n\n- Tenemos $n$ observaciones de entrenamiento $x_1, \\dots, x_n \\in \\mathbb{R}^p$,\ncon etiquetas de clase $y_1, \\dots, y_n \\in \\left\\{ 1, -1 \\right\\}$ .\n\n- El hiperplano de frontera máxima \n\n\n(LLEEENAAAAR)\n\n### Casos de no separación\n\n(LLEEENAAAAR, also include la diapo comentarios generales)\n\n\n\n## Clasificador de soporte vectorial\n\n- Estos no son las máquinas de soporte vectorial; sino, son\nuna versión con **frontera relajada** del hiperplano de frontera máxima.\n\n- En vez de buscar la mayor frontera, **suavizamos la frontera**, de manera que\nse permite que algunas observaciones se encuentre en el lado incorrecto\nde la frontera e incluso del hiperplano.\n\n(LLEENAAAR)\n\n## Kernels\n\n- Como **no siempre existe al menos un hiperplano separador (perfecto)**,\nlos datos pueden ser **embedded** into un espacio de mayor dimensión,\nvía un **kernel** (es una función). Esto con el fin de que, en aquel\nespacio de mayor dimensión, **sí exista** un hiperplano separador.\n\n- Cuando se emplea algún kernel para un clasificador de soporte\nvectorial, el modelo se denomina **Máquina de Soporte Vectorial**.\n\n## SVM para más de dos categorías\n\n- Para clasificación no binaria, se suelen usar dos alternativas:\n    - **Clasificación uno contra uno**: \n    - **Clasificación uno contra todos**:\n\n\n## Ejemplo práctico: SVM\n\n### Clasificación binaria\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Support Vector Machines\nlibrary(e1071)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'e1071' was built under R version 4.1.3\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generación de datos con fronteras no lineales\nset.seed(1)\nx <- matrix(rnorm(200*2), ncol=2)\nx[1:100,] = x[1:100,] + 2\nx[101:150,] = x[101:150,] - 2\n\ny <- c(rep(1,150), rep(2,50))\ndat <- data.frame(x = x, y = as.factor(y))\n\n# El ploteo de los datos muestra claramente que las fronteras no son lineales\nplot(x, col = y)\n```\n\n::: {.cell-output-display}\n![](clase-11_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Se dividen los datos aleatoriamente en grupos de entrenamiento y prueba\ntrain <- sample(200, 100)\n\n# Se ajusta el svm usando un kernel radial y gamma = 1\nsvmfit <- e1071::svm(\n  y ~., data = dat[train,], \n  kernel = \"radial\", gamma = 1, cost = 1\n)\nplot(svmfit, dat[train,])\n```\n\n::: {.cell-output-display}\n![](clase-11_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Obtener información adicional\nsummary(svmfit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nsvm(formula = y ~ ., data = dat[train, ], kernel = \"radial\", gamma = 1, \n    cost = 1)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  radial \n       cost:  1 \n\nNumber of Support Vectors:  31\n\n ( 16 15 )\n\n\nNumber of Classes:  2 \n\nLevels: \n 1 2\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Incremento del costo para reducir el número de errores de entrenamiento\nsvmfit = e1071::svm(\n  y ~., data = dat[train,], \n  kernel = \"radial\", gamma = 1, cost = 1e5\n)\nplot(svmfit,dat[train,])\n```\n\n::: {.cell-output-display}\n![](clase-11_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Es posible realizar validación cruzada usando tune() para seleccionar la mejor opción de\n# gamma y costo con un kernel radial\nset.seed(1)\ntune.out = e1071::tune(\n  svm, y ~., data = dat[train,], \n  kernel = \"radial\", \n  ranges = list(\n    cost = c(0.1, 1, 10, 100, 1000),\n    gamma = c(0.5, 1, 2 ,3, 4)\n  )\n)\n# La mejor elección resultó ser con costo=1 y gamma=0.5.\n# Se realiza la predicción con el conjunto de entrenamiento\nsummary(tune.out)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nParameter tuning of 'svm':\n\n- sampling method: 10-fold cross validation \n\n- best parameters:\n cost gamma\n    1   0.5\n\n- best performance: 0.07 \n\n- Detailed performance results:\n    cost gamma error dispersion\n1  1e-01   0.5  0.26 0.15776213\n2  1e+00   0.5  0.07 0.08232726\n3  1e+01   0.5  0.07 0.08232726\n4  1e+02   0.5  0.14 0.15055453\n5  1e+03   0.5  0.11 0.07378648\n6  1e-01   1.0  0.22 0.16193277\n7  1e+00   1.0  0.07 0.08232726\n8  1e+01   1.0  0.09 0.07378648\n9  1e+02   1.0  0.12 0.12292726\n10 1e+03   1.0  0.11 0.11005049\n11 1e-01   2.0  0.27 0.15670212\n12 1e+00   2.0  0.07 0.08232726\n13 1e+01   2.0  0.11 0.07378648\n14 1e+02   2.0  0.12 0.13165612\n15 1e+03   2.0  0.16 0.13498971\n16 1e-01   3.0  0.27 0.15670212\n17 1e+00   3.0  0.07 0.08232726\n18 1e+01   3.0  0.08 0.07888106\n19 1e+02   3.0  0.13 0.14181365\n20 1e+03   3.0  0.15 0.13540064\n21 1e-01   4.0  0.27 0.15670212\n22 1e+00   4.0  0.07 0.08232726\n23 1e+01   4.0  0.09 0.07378648\n24 1e+02   4.0  0.13 0.14181365\n25 1e+03   4.0  0.15 0.13540064\n```\n:::\n\n```{.r .cell-code}\ntable(\n  true = dat[-train, \"y\"], \n  pred = predict(tune.out$best.model,newx=dat[-train,])\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    pred\ntrue  1  2\n   1 54 23\n   2 17  6\n```\n:::\n:::\n\n\n### Clasificación no binaria\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\nx <- rbind(x, matrix(rnorm(50*2), ncol=2))\ny <- c(y, rep(0, 50))\nx[y == 0,2]= x [y == 0,2] + 2\n\ndat <- data.frame(x = x, y = as.factor(y))\npar(mfrow = c(1, 1))\nplot(x, col = (y+1))\n```\n\n::: {.cell-output-display}\n![](clase-11_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n\n```{.r .cell-code}\nsvmfit = svm(\n  y ~., data = dat, kernel = \"radial\", cost = 10, gamma = 1\n)\nplot(svmfit, dat)\n```\n\n::: {.cell-output-display}\n![](clase-11_files/figure-html/unnamed-chunk-11-2.png){width=672}\n:::\n:::\n\n\n::: {#prp-}\n\nFijada la data sobre la cual se realiza **validación cruzada**,\nel parámetro **costo**;\nmientras que el parámetro **gamma**\n:::\n\n",
    "supporting": [
      "clase-11_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}