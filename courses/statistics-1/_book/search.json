[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Learning 1",
    "section": "",
    "text": "Sílabo"
  },
  {
    "objectID": "apuntes/clase-01.html#sobre-el-curso",
    "href": "apuntes/clase-01.html#sobre-el-curso",
    "title": "Apuntes de clase",
    "section": "Sobre el curso",
    "text": "Sobre el curso\nCasi toda la teoría del curso está contenida en ISLR, libro principal de la bibliografía del curso.\nLas listas deben ir avanzándose, pues la entrega oficial consistirá de modificaciones sobre algunos problemas de esas listas, con solo uno o dos días de plazo para la entrega.\nSolo habrá, most likely, máximo tres tareas académicas, pero ninguna se elimina.\nLas tareas no son las que están actualmente en Paideia, pero se basarán en las listas ya subidas.\nMáximo grupos de 4 para los grupos de las tareas; aunque puede ser individual también."
  },
  {
    "objectID": "apuntes/clase-01.html#introducción",
    "href": "apuntes/clase-01.html#introducción",
    "title": "Apuntes de clase",
    "section": "Introducción",
    "text": "Introducción\n\nMain goal:\n\nPredicción (no inferencia, eso se ve en otros cursos)\nFrom dataset, saber qué algoritmo de aprendizaje estadístico escoger, además de saber comunicar los hallazgos encontrados.\n\nHay más similitudes que diferencias entre Statistical Learning y Machine Learning (Aprendizaje de Máquina).\n\n\n¿Qué es Statistical Learning?\n\nSet of tools to understand data.\nDistinción principal: Supervisado vs No Supervisado\nCadena de stat. learning.: modelo -&gt; método -&gt; algoritmo -&gt; análisis -&gt; interpretación\nObjetivos principales:\n\nPredecir: Qué va a pasar\nInferencia: Cómo va a pasar\n\nEn Estadística, es muy importante considerar de dónde vienen los datos (muestreados under some probabilistic distribution).\n\n\n\nStatistical Learning vs Data Science\nData Science también busca obtener info a partir de datos, pero siempre require una implementación. Esto pues Data Science combina diversas disciplinas (math, CS, etc) para darles un enfoque pragmático.\nEste enfoque pragmático no es necesario en Statistical Learning.\nAmbos enfoques son importantes en la sociedad."
  },
  {
    "objectID": "apuntes/clase-01.html#aprendizaje-estadístico",
    "href": "apuntes/clase-01.html#aprendizaje-estadístico",
    "title": "Apuntes de clase",
    "section": "Aprendizaje Estadístico",
    "text": "Aprendizaje Estadístico\n\n¿Qué es?\nProceso de aprendizaje a partir de los datos.\n\nA partir de la aplicación de modelos a un conjunto de entrenamiento podemos:\n\nExtraer conclusiones acerca de las relaciones entre las variables inferencia.\nEncontrar una función predictiva para nuevas observaciones predicción.\n\n\n\n\nEl problema del Aprendizaje Supervisado\n\nPunto de partida:\n\nMedición del resultado \\(Y\\) (variable dependiente/respuesta/objetivo)\nVector de \\(p\\) mediciones predictoras \\(X = (X_1, \\cdots, X_p)\\), también llamadas entradas/regresores/covariables/características o variables independientes.\nEn problemas de regresión, \\(Y\\) es cuantitativa; en problemas de clasificación, \\(Y\\) toma valores en un conjunto finito y desordenado de clases o atributos predefinidos.\nTenemos datos de entrenamiento \\((x_1, Y_1), \\cdots, (x_n, Y_n)\\).\n\nA partir del training set, nos gustaría:\n\nPredecir nuevos casos de prueba.\nComprender como se relacionan las variables.\nEvaluar la calidad de predicciones e inferencias.\n\nLa variable respuesta supervisa nuestro análisis.\n\n\n\nEl problema del Aprendizaje No Supervisado\n\nNo hay variable respuesta.\nSe buscan patrones o agrupaciones (ocultas) en los datos, para obtener información y comprensión.\nHay más subjetividad al momento de comparar modelos; por ello importa mucho más el conocimiento sobre el área de aplicación (field experts).\n\n\n\nFilosofía general\n\nLos métodos más simples a menudo funcionan tan bien como los más complicados.\n\n\n\nStatistical Learning vs Machine Learning\n\n\\(\\text{ML} \\subset \\text{AI}\\) (algoritmos)\n\\(\\text{SL} \\subset \\text{Statistics}\\) (modelos)\nPresentan un mayor enfoque en:\n\nML: Precisión de la predicción y aplicaciones a gran escala.\nSL: Modelos, su interpretabilidad, precisión e incertidumbre.\n\nMétodo escalable: Al añadir más datos o más variables, la precisión no se reduce.\nSL y ML emplean casi las mismas técnicas.\nComparado a un curso de ML, en este curso nos enfocaremos más en comprender los modelos que usaremos, cómo funcionan."
  },
  {
    "objectID": "apuntes/clase-01.html#lista-de-ejercicios",
    "href": "apuntes/clase-01.html#lista-de-ejercicios",
    "title": "Apuntes de clase",
    "section": "Lista de ejercicios",
    "text": "Lista de ejercicios\nLista 1"
  },
  {
    "objectID": "apuntes/clase-02.html#objetivo-de-statistical-learning",
    "href": "apuntes/clase-02.html#objetivo-de-statistical-learning",
    "title": "Apuntes de clase",
    "section": "Objetivo de Statistical Learning",
    "text": "Objetivo de Statistical Learning\n\nSuposiciones:\n\nSe observa una variable respuesta cuantitativa \\(Y\\) .\nSe cuentan con \\(p\\) predictores \\(x_1, \\dots, x_n\\) .\nExiste una función \\(f\\) tal que \\(Y = f(x) + \\epsilon\\) .\nDonde \\(\\epsilon\\), aleatorio, es independiente de \\(X\\), y tiene media cero.\n\nEl término \\(\\epsilon\\) nos permite hacer inferencia … las predicciones tienen un intervalo de confianza.\nOBJETIVO: Estimar \\(f\\) .\nRespecto a la inferencia posible tras estimar \\(f\\), se refieren a generalizar estadísiticamente la relación entre los predictors y la response (ejemplo: analizar los coeficientes tras regresión lineal); decir si los atributos son estadísticamente significativos.\n\n\nMotivo 1: Predicción\n\nObjetivo: Predecir, con la mayor precisión posible, la respuesta \\(Y\\), dadas nuevas observaciones \\(x\\) de las covariables.\n\n\\[\n\\hat{Y} = \\hat{f}(x)\n\\]\n\nPara predicción, no requerimos la forma exacta de \\(\\hat{f}\\) .\nExisten dos términos que influyen en la precisión de \\(\\hat{Y}\\), como prediccion de \\(\\hat{Y}\\) :\n\nError reducible: Proviene de nuestra estimación \\(\\hat{f}\\) de \\(f\\) .\nError ireducible: Proviene del término del error \\(\\epsilon\\) y no puede reducirse mejorando \\(\\hat{f}\\) .\n\nFijada la estimación \\(\\hat{f}\\), respeto a la respuesta \\(Y\\) y un conjunto de predictores \\(X\\), se cumple\n\n\\[\nE \\left[ \\left( Y - \\hat{Y} \\right)^2\\right] =\n\\underbrace{E\\left[\\left( f(X) - \\hat{f}(X) \\right)^2\\right]}_{\\text{error reducible}}\n+ \\underbrace{\\text{var}(\\epsilon)}_{\\text{error irreducible}}\n\\]\n\n\nMotivo 2: Inferencia\n\nObjetivo: Comprender cómo la variable respuesta se ve afectada por los diversos predictores (covariables).\nRequerimos la forma exacta de \\(\\hat{f}\\):\n\n¿Qué predictores están asociados con la respuesta?\n¿Cuál es la relación entre la respuesta y cada predictor?\n¿La relación puede ser lineal, o se requiere un modelo más complejo?"
  },
  {
    "objectID": "apuntes/clase-02.html#regresión-y-clasificación",
    "href": "apuntes/clase-02.html#regresión-y-clasificación",
    "title": "Apuntes de clase",
    "section": "Regresión y Clasificación",
    "text": "Regresión y Clasificación\n\nRegresión: Cuando la variable respuesta es numérica.\nClasificación Cuando la variable respuesta es categórica."
  },
  {
    "objectID": "apuntes/clase-02.html#estimación-de-f",
    "href": "apuntes/clase-02.html#estimación-de-f",
    "title": "Apuntes de clase",
    "section": "Estimación de \\(f\\)",
    "text": "Estimación de \\(f\\)\n\nMain idea:\n\nUsar un conjunto de datos de entrenamiento \\(\\left( x_1, y_1 \\right), \\dots, \\left( x_n, y_n \\right)\\) para hallar una estimación \\(\\hat{f}\\), tal que \\(\\hat{f}(X) \\approx Y\\), para cada \\(\\left( X, Y \\right)\\)\n\nPara predicción, NUNCA evaluar la estimación \\(\\hat{f}\\) en una observación de entrenamiento \\(X\\) .\nPresenta dos enfoques principales: Param. y No param. .\n\n\nMétodos paramétricos\n\nSteps:\n\nFijar una forma para \\(f\\) . 1 Estimar los parámetros desconocidos de \\(f\\), unsando el conjunto de entrenamiento.\n\n\n\n\nMétodos no paramétricos\n\nBuscan una estimación de \\(f\\), sin hacer suposiciones explícitas de la función \\(f\\) .\nEn cierto sentido, se consideran infinitos parámetros.\nEjemplo: Algoritmo de los \\(K\\)-vecinos.\nParámetro: Constante del modelo, que se estima.\nHiperparámetro: Constante del modelo, que se escoge libremente. Por ejemplo, el valor \\(K\\) en el algoritmo de \\(K\\)-means.\nLos hiperparámetros se pueden calibrar para obtener un nivel de adecuado de flexibilidad para el modelo.\n\n\n\nParam. vs No param.\n\nSuelen ser de mayor interpretabilidad: Paramétrico.\nTienden a ser más flexibles: No paramétricos.\nSuelen tener mayor complejidad computacional: No paramétricos.\nSuele requerir una mayor cantidad de datos: No paramétricos.\nNo necesariamente un método no paramétrico siempre produce predicciones más precisas, comparado a un método paramétrico.\n\n\n\n\n\n\nComparación"
  },
  {
    "objectID": "apuntes/clase-02.html#prediction-accuracy-vs-interpretabilidad",
    "href": "apuntes/clase-02.html#prediction-accuracy-vs-interpretabilidad",
    "title": "Apuntes de clase",
    "section": "Prediction accuracy vs Interpretabilidad",
    "text": "Prediction accuracy vs Interpretabilidad\n\nMétodos inflexibles (o rígidos), son aquellos que tienen fuerte restricciones sobre la forma de \\(f\\) .\nLa elección de un método flexible o inflexible depende del objetivo en mente:\n\nIf goal is inferencia, then se prefiere métodos inflexibles\nIf goal is predicción, then se prefiere métodos flexibles.\n\nSobreajuste: Ocurre cuando \\(\\hat{f}\\) se ajusta demasiado a los datos observados.\nSubajuste: Ocurre cuando \\(\\hat{f}\\) es demasiado rígida para capturar la estructura subyacente de los datos."
  },
  {
    "objectID": "apuntes/clase-02.html#ejercicio",
    "href": "apuntes/clase-02.html#ejercicio",
    "title": "Apuntes de clase",
    "section": "EJERCICIO",
    "text": "EJERCICIO\nResolver la lista 1 publicada en Paideia, al menos hasta el item c (no included)."
  },
  {
    "objectID": "apuntes/clase-02.html#evaluación-de-la-precisión-del-modelo",
    "href": "apuntes/clase-02.html#evaluación-de-la-precisión-del-modelo",
    "title": "Apuntes de clase",
    "section": "Evaluación de la precisión del modelo",
    "text": "Evaluación de la precisión del modelo\n\nNingún método domina a todos los demás sobre todos conjuntos de datos posibles.\n\n\nFunción pérdida\n\nPara variable respuesta numérica, las métricas \\(L1\\) y \\(L2\\) suelen usarse.\nPara response categórica, se puede usar la asignación 0 (si \\(\\hat{y}_i = y_i\\) ); y, 1, caso contrario.\nEn problemas de regresión, suele emplearse la pérdida cuadrática (\\(L2\\)).\n\n\n\nMSE de entrenamiento\n\nNotación: \\(\\text{MSE}_{\\text{train}} = \\dfrac{1}{n} \\displaystyle{ \\sum_{i=1}^{n}\\left( y_i - \\hat{f}(x_i) \\right)^2}\\)\nCuando se evalúa \\(\\hat{f}\\) en una observación de entrenamiento, no es posible saber si la predicción fue precisa debido a que el modelo aprendió o porque para el modelo se empleó el valor observado para la response variable (caso modelo plagió).\n\n\n\nMSE de prueba\n\nEvaluamos el modelo con una muestra de observaciones que no fue usada para entrenar al modelo. Esta muestra se denomina datos de prueba (test).\nPara un conjunto de \\(n_0\\) observaciones de prueba \\(\\left( x_{0j}, y_{0j} \\right)\\), se define:\n\n\\(\\text{MSE}_{\\text{test}} = \\dfrac{1}{n_0} \\displaystyle{ \\sum_{j=1}^{n_0}\\left( y_{0j} - \\hat{f}(x_{0j}) \\right)^2}\\)"
  },
  {
    "objectID": "apuntes/clase-03.html#bias-variance-tradeoff",
    "href": "apuntes/clase-03.html#bias-variance-tradeoff",
    "title": "Apuntes de clase",
    "section": "Bias-variance tradeoff",
    "text": "Bias-variance tradeoff\nFijando un valor \\(x_0\\) para los predictores, y considerando una familia de training datasets, cada uno de esos training sets produce una estimación \\(\\hat{f}\\) de la función \\(f\\).\nAhora, considere el siguiente resultado:\n\nTheorem 1 \\[\nE \\left[\\left( Y - \\hat{f}\\left( x_o \\right) \\right)^2\\right]\n= \\text{ Var}\\left( \\epsilon \\right) +\n\\text{ Var}\\left( \\hat{f}\\left( x_0 \\right) \\right) +\n\\left[\\text{ Bias}\\left( \\hat{f}\\left( x_0 \\right) \\right)\\right]^2\n\\]\n\nEl sesgo consiste en cómo se aleja el valor real de la variable, en comparacion con el promedio de las estimaciones.\nModelos que suelen tenor menor sesgo, suelen tener mayor varianza, y viceversa.\n\nProposition 1 Para modelos más flexibles, la varianza se incrementa y el sesgo disminuye. (not aaalways true).\n\n\nProposition 2 Resulta que el punto donde el \\(\\text{ MSE }_{test}\\) alcanza un mínimo (en el eje Y, siendo el eje X el nivel de flexibilidad), a vecese coincide con el punto donde se intersectan el bias y varianza de la estimación \\(\\hat{f}\\)\n\n\n\n\n\n\nBias-variance tradeoff\n\n\n\n\n\nResumen:\n\nA medida que aumenta la complejidad (y, por tanto la flexibilidad) de un modelo, el modelo se vuelve más adaptable a estructuras subyacentes y cae el error de entrenaminto (sobreajuste)\nEl error de prueba es el error de predicción sobre una muestra de prueba.\nLos modelos inflexibles (con parámetros para ajustarse) son fáciles de calcular pero pueden producir subajuste (alto sesgo).\nLos modelos flexibles pueden producir sobreajuste."
  },
  {
    "objectID": "apuntes/clase-03.html#clasificación",
    "href": "apuntes/clase-03.html#clasificación",
    "title": "Apuntes de clase",
    "section": "Clasificación",
    "text": "Clasificación\n\n¿Qué es clasificación?\n\nModelo politómico: Cuando la variable respuesta cuenta con más de dos categorías.\nCuando el objetivo es predecir, no suele tomarse en cuenta la jerarquía (en caso exista) entre las categorías de la variable respuesta. Por ejemplo, en caso \\(Y\\) sea una variable ordinal.\nUsualmente construimos modelos que predigan las probabilidades de categorías, dadas ciertas covariables \\(X\\).\nNo siempre los modelos de clasificación te danla clase y probabilidad de pertenencia a la clase. Los modelos por lo general dan solo la probabilidad de pertenencia a las clases. Por ejemplo:\n\nRegresión logística solo te da la probabilidad de pertenencia a la clase. Sin mbargo, ni en el caso binario basta la regla “probabilidad mayor de 50%” para asignar una clase a una nueva observación.\nÁrboles de decisión te da la clase, pero no la probabilidad de pertenencia.\n\n\n\n\nConfiguración de la clasifiación general\n\nContexto:\n\nLa variable respuesta cuenta con una cantidad finita de valores posibles … categorías.\nLa variable respuesta \\(Y\\) es cualitativa.\n\nObjetivo:\n\nConstruir un clasificador que asigne una etiqueta de clasificación a una observación futura sin etiquetar, además de evaluar la incertidumbre en esta clasificación.\n\nMedidas de rendimiento\n\nLa más popular es la tasa de error de clasificación érronea (versión de entrenamiento y prueba).\n\nPérdida 0/1:\n\nLas clasificaciones erróneas reciben la pérdida 1; y las clasificaciones correctas, pérdida 0.\nNo se emplea pérdida cuadrática para la clasificación.\n\n\n\n\nMétodos Estadísticos Tradicionales para Clasificación\n\nTres métodos comúnmente usados para clasificación:\n\nRegresión Logística\nAnálisis Discrimante Lineal (LDA)\nAnálisis Discrimante Cuadrático (QDA)\n\n\n\n\n\n\n\nGeneralized Linear Models\n\n\n\n\n\n\n\n\n\nGeneralized Linear Models\n\n\n\n\n\nLa exponential family es una familia de distribuciones, tales como la Normal, Gamma, Binomial, etc.\nLa función \\(g\\) se denomina función de enlace y debe satisfacer tener inversa, \\(g^{-1}\\), la cual se denomina función de respuesta.\n\n\n\nPredicción con el modelo logit\n\nSe denomina modelo logit al modelo de regresión logística binaria.\nEs un modelo ideal para interpretar.\nNo es un modelo ideal para predecir, en especial si se tienen clases desbalanceadas.\nRecuerde que desbalanceado no implica difícil de predecir. Además, clases desbalanceadas no es un problema de los datos.\nPredicción\n\n\\(\\hat{p}_i = \\dfrac{1}{1 + e^{-\\eta_i}}\\) = \\(\\dfrac{1}{1 + e^{-\\left( \\hat{\\beta}_0 + \\hat{\\beta}_ 1 x_{1i} + \\dots + \\hat{\\beta}_p x_{pi}\\right)}}\\)\n\\[\n  \\begin{equation}\n    \\hat{Y}_i =\n    \\begin{cases*}\n1 & si $\\hat{p}_i \\geq c$ \\\\\n0 & si $\\hat{p}_i &lt; c$\n    \\end{cases*}\n  \\end{equation}\n  \\] donde \\(c\\) se denomina punto de corte (umbral).\nUsualmente se utiliza \\(c = 0.50\\) .\n\n\n\nTheorem 2 Se puede demostrar (es complicado) que con la elección de umbral \\(c = 0.50\\) se minimiza el error de clasificación; además de ser el único umbral que minimiza tal error.\n\n\n\n¿Regresión lineal para una clasificación binaria?\n\nLa regresión lineal puede producir probabilidades menores que cero o mayores que uno.\n\n\n\nRegresión logística binaria\n\nLa response presenta solo dos categorías posibles.\nSe modela entonces \\(Y_i \\sim Bernoulli(p_i)\\) .\nObjetivo: Estimar \\(p_i = P\\left( Y_i = 1 \\mid X_1, \\dots, X_p \\right)\\)"
  },
  {
    "objectID": "apuntes/clase-04.html#ejemplo-práctico",
    "href": "apuntes/clase-04.html#ejemplo-práctico",
    "title": "Apuntes de clase",
    "section": "Ejemplo práctico",
    "text": "Ejemplo práctico\n\nlibrary(ISLR2)\n\nWarning: package 'ISLR2' was built under R version 4.1.3\n\ndata(Default)\n\n# Default$default &lt;- as.numeric(Default$default) - 1\nglm_default &lt;- glm(\n  default ~ balance, \n  data = Default, family = 'binomial'\n)\n\nsummary(glm_default)$coef\n\n                 Estimate   Std. Error   z value      Pr(&gt;|z|)\n(Intercept) -10.651330614 0.3611573721 -29.49221 3.623124e-191\nbalance       0.005498917 0.0002203702  24.95309 1.976602e-137\n\n\n\\[\n  \\log \\left( \\dfrac{\\hat{p}_i}{1- \\hat{p}_i} \\right)  = \\hat{n}_i = -10.6513 + 0.0055*\\text{balance}_i\n\\]\n\nglm_default_2 &lt;- glm(\n  default ~ balance + income + student, \n  data = Default, family = 'binomial'\n)\n\neta &lt;- summary(glm_default_2)$coef[,1] %*% c(1, 2000, 40000, 1)\n1 / (1 + exp(-eta))\n\n          [,1]\n[1,] 0.5196218"
  },
  {
    "objectID": "apuntes/clase-04.html#clasificador-de-bayes",
    "href": "apuntes/clase-04.html#clasificador-de-bayes",
    "title": "Apuntes de clase",
    "section": "Clasificador de Bayes",
    "text": "Clasificador de Bayes\n\nEl clasificador de Bayes asigna una observación a la clase más probable, dado los valores de los predictores.\n\n\nPropiedades\n\nTiene la tasa de error de prueba más pequeña.\nPor lo general, no conocemos la verdadera distribución condicional \\(Pr(Y| X)\\) para datos reales.\nFunción pérdida: A las clasificaciones erróneas se les asigna la pérdida \\(1\\); y, a las clasificaciones correctas, \\(0\\) . Esta es conocida como pérdida-0/1.\n\n\n\nTasas de error\n\nTasa de error de entrenamiento:\n\n\\[\n\\dfrac{1}{n} \\sum_{i=1}^{n}I\\left( y_i \\ne \\hat{y}_i \\right)\n\\]\n\nTasa de error de prueba:\n\n\\[\nAverage \\left( I \\left( y_i \\ne \\hat{y}_i \\right) \\right)\n\\]\n\nSuponemos que un buen clasificador es aquel que tiene un error de prueba bajo."
  },
  {
    "objectID": "apuntes/clase-04.html#dos-paradigmas",
    "href": "apuntes/clase-04.html#dos-paradigmas",
    "title": "Apuntes de clase",
    "section": "Dos paradigmas",
    "text": "Dos paradigmas\n\nParadigma de diagnóstico:\n\nSe estima directamente la distribución a posteriori para las clases: \\(Pr(Y = k \\mid X = x)\\)\nEjemplos: Regresión logística, Clasificación KNN\n\nParadigma de muestreo:\n\nEnfoque indirecto:\n\nModele la distribución condicional de predictores, para cada clase: \\(f_k (x) = Pr(X = x \\mid Y = k)\\)\nConsidere las probabilidades a priori: \\(\\pi_k = Pr(Y = k)\\)\n\nClasificar en la clase con el producto máximo \\(pi_k f_k (x)\\)\n¿Cómo obtenemos \\(Pr(Y 0= k \\mid X = x_0)\\)?\nTeorema de Bayes: \\[\np_k (X) = Pr(Y = k \\mid X = x) =\n\\dfrac{Pr(X = x \\cap Y = k)}{f(x)} =\n\\dfrac{f_k (x) \\pi_k}{\\sum_{l=1}^{k}f_l (x) \\pi_l}\n  \\]\n\n\n\nProposition 1 No es recomendable usar los mismos datos de test para comparar modelos. Esto puesto que un modelo podría presentar menor test-error que otro, para un mismo test dataset, debido al azar."
  },
  {
    "objectID": "apuntes/clase-04.html#análisis-discriminante",
    "href": "apuntes/clase-04.html#análisis-discriminante",
    "title": "Apuntes de clase",
    "section": "Análisis Discriminante",
    "text": "Análisis Discriminante\n\nEl enfoque es modelar la distribución de \\(X\\) en cada una de las clases por separado, y, luego usar el teorema de Bayes para obtener \\(Pr(Y \\mid X)\\)."
  },
  {
    "objectID": "apuntes/clase-04.html#ejemplo-práctico-lda",
    "href": "apuntes/clase-04.html#ejemplo-práctico-lda",
    "title": "Apuntes de clase",
    "section": "Ejemplo práctico LDA",
    "text": "Ejemplo práctico LDA\n\ndiabetes = read.csv(\n  \"../datos/DiabetesTrain.csv\",\n  stringsAsFactors = TRUE\n)\n\nhead(diabetes)\n\n  glucose insulin sspg  class\n1      97     289  117 normal\n2     105     319  143 normal\n3      90     356  199 normal\n4      90     323  240 normal\n5      86     381  157 normal\n6     100     350  221 normal\n\n\n\nAnálisis descriptivo\n\nlibrary(psych)\n\ndescribeBy(diabetes[,-4], diabetes$class)\n\n\n Descriptive statistics by group \ngroup: chemical\n        vars  n   mean     sd median trimmed    mad min max range skew kurtosis\nglucose    1 26  99.46   8.81   98.5   99.32   9.64  85 114    29 0.24    -1.28\ninsulin    2 26 504.12  60.06  497.5  500.77  63.75 423 643   220 0.37    -0.89\nsspg       3 26 291.77 177.65  222.5  272.14 120.83 109 748   639 1.08    -0.01\n           se\nglucose  1.73\ninsulin 11.78\nsspg    34.84\n------------------------------------------------------------ \ngroup: normal\n        vars  n   mean    sd median trimmed   mad min max range  skew kurtosis\nglucose    1 66  91.98  8.16   91.5   92.00  8.15  70 112    42 -0.05     0.10\ninsulin    2 66 351.21 37.70  354.5  351.46 46.70 269 426   157 -0.09    -0.93\nsspg       3 66 169.02 65.49  156.5  163.39 51.89  73 490   417  1.86     6.76\n          se\nglucose 1.00\ninsulin 4.64\nsspg    8.06\n------------------------------------------------------------ \ngroup: overt\n        vars  n    mean     sd median trimmed    mad min  max range skew\nglucose    1 23  207.17  71.84    203  202.79  96.37 120  339   219 0.35\ninsulin    2 23 1002.96 315.85    972  998.21 382.51 538 1520   982 0.16\nsspg       3 23  112.61 106.57     87   94.37  65.23  10  460   450 1.72\n        kurtosis    se\nglucose    -1.28 14.98\ninsulin    -1.34 65.86\nsspg        2.72 22.22\n\npairs.panels(\n  diabetes[,1:3],\n  bg = c(\"red\",\"yellow\",\"blue\")[diabetes$class],\n  pch = 21\n)\n\n\n\npar(mfrow = c(2,2))\nboxplot(glucose ~ class, data = diabetes, main = \"glucose\")\nboxplot(insulin ~ class, data = diabetes, main = \"insulin\")\nboxplot(sspg ~ class, data = diabetes, main = \"sspg\")\npar(mfrow = c(1,1))\n\n\n\n\n\n\nAnálisis Discriminante Lineal\n\n# Estimación\nlibrary(MASS)\n\nWarning: package 'MASS' was built under R version 4.1.3\n\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:ISLR2':\n\n    Boston\n\nlda1 = lda(class ~., data = diabetes)\nlda1\n\nCall:\nlda(class ~ ., data = diabetes)\n\nPrior probabilities of groups:\nchemical   normal    overt \n0.226087 0.573913 0.200000 \n\nGroup means:\n           glucose   insulin     sspg\nchemical  99.46154  504.1154 291.7692\nnormal    91.98485  351.2121 169.0152\novert    207.17391 1002.9565 112.6087\n\nCoefficients of linear discriminants:\n                 LD1          LD2\nglucose -0.035130542 -0.056150421\ninsulin  0.013748266  0.009876402\nsspg     0.001344232  0.005489825\n\nProportion of trace:\n   LD1    LD2 \n0.8526 0.1474 \n\n# Predicción\nplda1 = predict(lda1, diabetes)$class\n\n# Matriz de confusion (entrenamiento)\ntable(plda1, diabetes$class)\n\n          \nplda1      chemical normal overt\n  chemical       19      1     5\n  normal          7     65     2\n  overt           0      0    16\n\n# Error (entrenamiento)\nerror1 = mean(plda1 != diabetes$class)\nerror1\n\n[1] 0.1304348\n\ncaret::confusionMatrix(\n  data = plda1, reference = diabetes$class\n)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction chemical normal overt\n  chemical       19      1     5\n  normal          7     65     2\n  overt           0      0    16\n\nOverall Statistics\n                                         \n               Accuracy : 0.8696         \n                 95% CI : (0.794, 0.9251)\n    No Information Rate : 0.5739         \n    P-Value [Acc &gt; NIR] : 6.334e-12      \n                                         \n                  Kappa : 0.7644         \n                                         \n Mcnemar's Test P-Value : 0.009308       \n\nStatistics by Class:\n\n                     Class: chemical Class: normal Class: overt\nSensitivity                   0.7308        0.9848       0.6957\nSpecificity                   0.9326        0.8163       1.0000\nPos Pred Value                0.7600        0.8784       1.0000\nNeg Pred Value                0.9222        0.9756       0.9293\nPrevalence                    0.2261        0.5739       0.2000\nDetection Rate                0.1652        0.5652       0.1391\nDetection Prevalence          0.2174        0.6435       0.1391\nBalanced Accuracy             0.8317        0.9006       0.8478\n\n\n\n\nSelección de Variables\n\nlibrary(klaR)\n\nWarning: package 'klaR' was built under R version 4.1.3\n\ngreedy.wilks(class ~., data = diabetes)\n\nFormula containing included variables: \n\nclass ~ insulin + glucose + sspg\n&lt;environment: 0x000000002e63c008&gt;\n\n\nValues calculated in each step of the selection procedure: \n\n     vars Wilks.lambda F.statistics.overall p.value.overall F.statistics.diff\n1 insulin    0.2469409            170.77488    9.665240e-35        170.774879\n2 glucose    0.1532280             86.28294    4.190502e-44         33.943348\n3    sspg    0.1311418             64.58470    7.641367e-46          9.262795\n  p.value.diff\n1 9.665240e-35\n2 2.994049e-12\n3 1.904104e-04\n\n# Validación Cruzada\nset.seed(666)\nstepclass(\n  diabetes[,-4], diabetes$class, \n  method = \"lda\", criterion = \"AC\",\n  # Consideramos una mejora significativa \n  # como de 10%, pero esta es una elección arbitraria\n  improvement = 0.10\n)\n\n `stepwise classification', using 10-fold cross-validated accuracy of method lda'.\n\n\n115 observations of 3 variables in 3 classes; direction: both\n\n\nstop criterion: improvement less than 10%.\n\n\naccuracy: 0.49254;  in: \"insulin\";  variables (1): insulin \naccuracy: 0.7221;  in: \"glucose\";  variables (2): insulin, glucose \n\n hr.elapsed min.elapsed sec.elapsed \n       0.00        0.00        0.38 \n\n\nmethod      : lda \nfinal model : diabetes$class ~ glucose + insulin\n&lt;environment: 0x000000002ede17a0&gt;\n\naccuracy = 0.7221 \n\n\n\n\nEstimación\n\nlda2 = lda(class ~ glucose + insulin, data = diabetes)\n\n# Predicción (entrenamiento)\nplda2 = predict(lda2, diabetes[-4])$class\n\n# Matriz de confusion (entrenamiento)\ncaret::confusionMatrix(plda2, diabetes$class)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction chemical normal overt\n  chemical       23      3     4\n  normal          3     63     3\n  overt           0      0    16\n\nOverall Statistics\n                                          \n               Accuracy : 0.887           \n                 95% CI : (0.8145, 0.9384)\n    No Information Rate : 0.5739          \n    P-Value [Acc &gt; NIR] : 2.26e-13        \n                                          \n                  Kappa : 0.8013          \n                                          \n Mcnemar's Test P-Value : 0.0719          \n\nStatistics by Class:\n\n                     Class: chemical Class: normal Class: overt\nSensitivity                   0.8846        0.9545       0.6957\nSpecificity                   0.9213        0.8776       1.0000\nPos Pred Value                0.7667        0.9130       1.0000\nNeg Pred Value                0.9647        0.9348       0.9293\nPrevalence                    0.2261        0.5739       0.2000\nDetection Rate                0.2000        0.5478       0.1391\nDetection Prevalence          0.2609        0.6000       0.1391\nBalanced Accuracy             0.9030        0.9160       0.8478\n\n# Error\nerror2 = mean(plda2 != diabetes$class)\nerror2\n\n[1] 0.1130435\n\npartimat(\n  class ~., data = diabetes, \n  method = \"lda\", nplots.vert = 2\n)\n\n\n\n\n\n\nDatos de Test\n\ndiabetes_test = read.csv(\n  \"../datos/DiabetesTest.csv\", stringsAsFactors = TRUE\n)\nhead(diabetes_test)\n\n  glucose insulin sspg    class\n1      89     472  162 chemical\n2      96     465  237 chemical\n3     112     503  408 chemical\n4     110     477  124 chemical\n5      90     413  344 chemical\n6     102     472  297 chemical\n\n\n\n# Evaluación en el conjunto de test\npredlda = predict(lda2, diabetes_test)$class\ncaret::confusionMatrix(predlda, diabetes_test$class)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction chemical normal overt\n  chemical        3      1     0\n  normal          7      9     0\n  overt           0      0    10\n\nOverall Statistics\n                                          \n               Accuracy : 0.7333          \n                 95% CI : (0.5411, 0.8772)\n    No Information Rate : 0.3333          \n    P-Value [Acc &gt; NIR] : 8.752e-06       \n                                          \n                  Kappa : 0.6             \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: chemical Class: normal Class: overt\nSensitivity                   0.3000        0.9000       1.0000\nSpecificity                   0.9500        0.6500       1.0000\nPos Pred Value                0.7500        0.5625       1.0000\nNeg Pred Value                0.7308        0.9286       1.0000\nPrevalence                    0.3333        0.3333       0.3333\nDetection Rate                0.1000        0.3000       0.3333\nDetection Prevalence          0.1333        0.5333       0.3333\nBalanced Accuracy             0.6250        0.7750       1.0000"
  },
  {
    "objectID": "apuntes/clase-04.html#ejemplo-práctico-qda",
    "href": "apuntes/clase-04.html#ejemplo-práctico-qda",
    "title": "Apuntes de clase",
    "section": "Ejemplo práctico QDA",
    "text": "Ejemplo práctico QDA\n\n# Estimación\nqda1 = qda(class ~., data = diabetes)\n\n# Predicción\npqda1 = predict(qda1, diabetes[-4])$class\n\n# Matriz de confusion (entrenamiento)\ncaret::confusionMatrix(pqda1, diabetes$class)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction chemical normal overt\n  chemical       22      1     2\n  normal          3     65     0\n  overt           1      0    21\n\nOverall Statistics\n                                          \n               Accuracy : 0.9391          \n                 95% CI : (0.8786, 0.9752)\n    No Information Rate : 0.5739          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.8938          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: chemical Class: normal Class: overt\nSensitivity                   0.8462        0.9848       0.9130\nSpecificity                   0.9663        0.9388       0.9891\nPos Pred Value                0.8800        0.9559       0.9545\nNeg Pred Value                0.9556        0.9787       0.9785\nPrevalence                    0.2261        0.5739       0.2000\nDetection Rate                0.1913        0.5652       0.1826\nDetection Prevalence          0.2174        0.5913       0.1913\nBalanced Accuracy             0.9062        0.9618       0.9511\n\n# Error (entrenamiento)\nerror1 = mean(pqda1 != diabetes$class)\nerror1\n\n[1] 0.06086957\n\n\n\nSelección de variables\n\nset.seed(666)\nstepclass(\n  diabetes[,-4], diabetes$class,\n  method = \"qda\", criterion = \"AC\",\n  improvement = 0.03\n)\n\n `stepwise classification', using 10-fold cross-validated accuracy of method qda'.\n\n\n115 observations of 3 variables in 3 classes; direction: both\n\n\nstop criterion: improvement less than 3%.\n\n\naccuracy: 0.82819;  in: \"insulin\";  variables (1): insulin \naccuracy: 0.86379;  in: \"glucose\";  variables (2): insulin, glucose \n\n hr.elapsed min.elapsed sec.elapsed \n       0.00        0.00        0.25 \n\n\nmethod      : qda \nfinal model : diabetes$class ~ glucose + insulin\n&lt;environment: 0x0000000027e19278&gt;\n\naccuracy = 0.8638 \n\n\n\n# Estimación\nqda2 = qda(class ~ glucose + insulin, data = diabetes)\n\n# Predicción (entrenamiento)\npqda2 = predict(qda2, diabetes[-4])$class\n\n# Matriz de confusion\ncaret::confusionMatrix(pqda2, diabetes$class)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction chemical normal overt\n  chemical       25      1     1\n  normal          1     65     0\n  overt           0      0    22\n\nOverall Statistics\n                                          \n               Accuracy : 0.9739          \n                 95% CI : (0.9257, 0.9946)\n    No Information Rate : 0.5739          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.955           \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: chemical Class: normal Class: overt\nSensitivity                   0.9615        0.9848       0.9565\nSpecificity                   0.9775        0.9796       1.0000\nPos Pred Value                0.9259        0.9848       1.0000\nNeg Pred Value                0.9886        0.9796       0.9892\nPrevalence                    0.2261        0.5739       0.2000\nDetection Rate                0.2174        0.5652       0.1913\nDetection Prevalence          0.2348        0.5739       0.1913\nBalanced Accuracy             0.9695        0.9822       0.9783\n\n# Error (entrenamiento)\nerror1 = mean(pqda2 != diabetes$class)\nerror1\n\n[1] 0.02608696\n\npartimat(\n  class ~.,data = diabetes, \n  method=\"qda\", nplots.vert=2\n)\n\n\n\n\n\n\nEvaluación\n\n# Evaluación en el conjunto de test\npredqda = predict(qda2, diabetes_test)$class\ncaret::confusionMatrix(predqda, diabetes_test$class)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction chemical normal overt\n  chemical        7      0     0\n  normal          2     10     0\n  overt           1      0    10\n\nOverall Statistics\n                                          \n               Accuracy : 0.9             \n                 95% CI : (0.7347, 0.9789)\n    No Information Rate : 0.3333          \n    P-Value [Acc &gt; NIR] : 1.665e-10       \n                                          \n                  Kappa : 0.85            \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: chemical Class: normal Class: overt\nSensitivity                   0.7000        1.0000       1.0000\nSpecificity                   1.0000        0.9000       0.9500\nPos Pred Value                1.0000        0.8333       0.9091\nNeg Pred Value                0.8696        1.0000       1.0000\nPrevalence                    0.3333        0.3333       0.3333\nDetection Rate                0.2333        0.3333       0.3333\nDetection Prevalence          0.2333        0.4000       0.3667\nBalanced Accuracy             0.8500        0.9500       0.9750"
  },
  {
    "objectID": "apuntes/clase-04.html#análisis-discriminante-regularizado",
    "href": "apuntes/clase-04.html#análisis-discriminante-regularizado",
    "title": "Apuntes de clase",
    "section": "Análisis Discriminante Regularizado",
    "text": "Análisis Discriminante Regularizado"
  },
  {
    "objectID": "apuntes/clase-04.html#ejemplo-práctico-1",
    "href": "apuntes/clase-04.html#ejemplo-práctico-1",
    "title": "Apuntes de clase",
    "section": "Ejemplo práctico",
    "text": "Ejemplo práctico\n\nrda(class ~., data = diabetes)\n\nCall: \nrda(formula = class ~ ., data = diabetes)\n\nRegularization parameters: \n       gamma       lambda \n6.350887e-04 6.026917e-10 \n\nPrior probabilities of groups: \nchemical   normal    overt \n0.226087 0.573913 0.200000 \n\nMisclassification rate: \n       apparent: 6.087 %\ncross-validated: 7.725 %\n\nrda1 = rda(class ~., data = diabetes)\nrda1 \n\nCall: \nrda(formula = class ~ ., data = diabetes)\n\nRegularization parameters: \n       gamma       lambda \n1.942273e-05 3.392599e-01 \n\nPrior probabilities of groups: \nchemical   normal    overt \n0.226087 0.573913 0.200000 \n\nMisclassification rate: \n       apparent: 9.565 %\ncross-validated: 12.828 %\n\n# Predicción (entrenamiento)\nprda1 = predict(rda1, diabetes[-4])$class\n\n# Matriz de confusion (entrenamiento)\ncaret::confusionMatrix(prda1, diabetes$class)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction chemical normal overt\n  chemical       21      1     3\n  normal          5     65     2\n  overt           0      0    18\n\nOverall Statistics\n                                          \n               Accuracy : 0.9043          \n                 95% CI : (0.8353, 0.9513)\n    No Information Rate : 0.5739          \n    P-Value [Acc &gt; NIR] : 5.776e-15       \n                                          \n                  Kappa : 0.8293          \n                                          \n Mcnemar's Test P-Value : 0.05343         \n\nStatistics by Class:\n\n                     Class: chemical Class: normal Class: overt\nSensitivity                   0.8077        0.9848       0.7826\nSpecificity                   0.9551        0.8571       1.0000\nPos Pred Value                0.8400        0.9028       1.0000\nNeg Pred Value                0.9444        0.9767       0.9485\nPrevalence                    0.2261        0.5739       0.2000\nDetection Rate                0.1826        0.5652       0.1565\nDetection Prevalence          0.2174        0.6261       0.1565\nBalanced Accuracy             0.8814        0.9210       0.8913\n\n# Error (entrenamiento)\nerror1 = mean(prda1 != diabetes$class)\nerror1\n\n[1] 0.09565217\n\n\n\nSelección de variables\n\n# Selección de Variables (validación cruzada)\nstepclass(\n  diabetes[,-4], diabetes$class,\n  method = \"rda\", criterion = \"AC\",\n  improvement = 0.03\n)\n\n `stepwise classification', using 10-fold cross-validated accuracy of method rda'.\n\n\n115 observations of 3 variables in 3 classes; direction: both\n\n\nstop criterion: improvement less than 3%.\n\n\naccuracy: 0.81469;  in: \"insulin\";  variables (1): insulin \naccuracy: 0.85865;  in: \"glucose\";  variables (2): insulin, glucose \n\n hr.elapsed min.elapsed sec.elapsed \n       0.00        0.00       17.97 \n\n\nmethod      : rda \nfinal model : diabetes$class ~ glucose + insulin\n&lt;environment: 0x00000000246baea8&gt;\n\naccuracy = 0.8587 \n\n\n\n\nModelo final\n\nrda2 = rda(class ~ glucose + insulin, data = diabetes)\nrda2\n\nCall: \nrda(formula = class ~ glucose + insulin, data = diabetes)\n\nRegularization parameters: \n     gamma     lambda \n0.96120286 0.06374861 \n\nPrior probabilities of groups: \nchemical   normal    overt \n0.226087 0.573913 0.200000 \n\nMisclassification rate: \n       apparent: 6.087 %\ncross-validated: 7.101 %\n\n# Predicción\nprda2 = predict(rda2, diabetes[-4])$class\n# Matriz de confusion\ncaret::confusionMatrix(prda2, diabetes$class)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction chemical normal overt\n  chemical       23      0     4\n  normal          3     66     0\n  overt           0      0    19\n\nOverall Statistics\n                                          \n               Accuracy : 0.9391          \n                 95% CI : (0.8786, 0.9752)\n    No Information Rate : 0.5739          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.8931          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: chemical Class: normal Class: overt\nSensitivity                   0.8846        1.0000       0.8261\nSpecificity                   0.9551        0.9388       1.0000\nPos Pred Value                0.8519        0.9565       1.0000\nNeg Pred Value                0.9659        1.0000       0.9583\nPrevalence                    0.2261        0.5739       0.2000\nDetection Rate                0.2000        0.5739       0.1652\nDetection Prevalence          0.2348        0.6000       0.1652\nBalanced Accuracy             0.9198        0.9694       0.9130\n\n# Error\nerror1 = mean(prda2 != diabetes$class)\nerror1\n\n[1] 0.06086957\n\n# Evaluación en el conjunto de test\npredrda = predict(rda2, diabetes_test)$class\ncaret::confusionMatrix(diabetes_test$class, predrda)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction chemical normal overt\n  chemical        7      2     1\n  normal          0     10     0\n  overt           0      0    10\n\nOverall Statistics\n                                          \n               Accuracy : 0.9             \n                 95% CI : (0.7347, 0.9789)\n    No Information Rate : 0.4             \n    P-Value [Acc &gt; NIR] : 1.698e-08       \n                                          \n                  Kappa : 0.85            \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: chemical Class: normal Class: overt\nSensitivity                   1.0000        0.8333       0.9091\nSpecificity                   0.8696        1.0000       1.0000\nPos Pred Value                0.7000        1.0000       1.0000\nNeg Pred Value                1.0000        0.9000       0.9500\nPrevalence                    0.2333        0.4000       0.3667\nDetection Rate                0.2333        0.3333       0.3333\nDetection Prevalence          0.3333        0.3333       0.3333\nBalanced Accuracy             0.9348        0.9167       0.9545"
  },
  {
    "objectID": "apuntes/clase-04.html#naive-bayes",
    "href": "apuntes/clase-04.html#naive-bayes",
    "title": "Apuntes de clase",
    "section": "Naive Bayes",
    "text": "Naive Bayes\n\nMétodo popular cuando se tiene una gran cantidad de predictores.\nSuponemos que en cada clase, los predictores son independientes, (supuesto de independencia condicional dentro de clases).\nEs un método escalable, es decir, no pierde eficiencia cuando se aumenta la cantidad de predictores (más columnas).\nRápidamente genera predicciones de clasificaciones, comparado a otros modelos.\nNo es tan útil para inferencia.\nCuando las distribuciones marginales son respecto a una un predictor numérico continuo, se supone que tal predictor sigue una distribución normal univariada."
  },
  {
    "objectID": "apuntes/clase-04.html#ejercicio",
    "href": "apuntes/clase-04.html#ejercicio",
    "title": "Apuntes de clase",
    "section": "Ejercicio",
    "text": "Ejercicio\nIr avanzando la lista 2."
  },
  {
    "objectID": "apuntes/clase-05.html#naive-bayes",
    "href": "apuntes/clase-05.html#naive-bayes",
    "title": "Apuntes de clase",
    "section": "Naive Bayes",
    "text": "Naive Bayes\n\nEs un modelo no paramétrico, donde, entre sus parámetros está la estimación, via frecuencia relativa, de los parámetros \\(Pr[Y=k]\\) de la población.\nNo es un modelo interpetable … no se usa para entender cómo afectan los predictores a la respuesta.\nNo nos permite conocer la verdadera distribución condicional de \\(Pr(Y\\mid X)\\) para datos reales.\nEl supuesto de normalidad multivariada casi nunca se cumple en la vida real, por lo que el modelo Naive Bayes propone, en parte, una menor restricción sobre la distribución de los predictores.\n\n\nCálculo de probabilidades condicionales\n\nEl clasificador Naive Bayes puede ser aplicado también cuando hay predictors continuos. Alternativas:\n\nDiscretización.\nDiscretizar variables numéricas no siempre es una pérdida de información. Más bien, la discretización puede ser una buena alternativa para remover ruido de los datos, lidiar con el hecho que las variables numéricas presentan distintas escalas (min max).\nEstimación no paramétrica de la densidad del kernel.\nSupone una distribución para cada predictora, por lo general Gaussiana, con media y varianza estimada de los datos.\n\n\nSi simplemente asumes normalidad, es parecido (no totalmente) al caso estudiado en Análisis Discriminante.\n\n\nEstimador Naive Bayes\n\nEs importante evitar que haya alguna probabilidad igual a cero, respecto a alguna de las clases.\n\nEn ese tipo de casos, se puede usar una correción de Laplace.\nNo existe un corrector Laplaciano mejor que todos, pero por lo general suele usarse el valor 1.\nAún así, puede tratarse el corrector Laplaciano como un hiperparámetro del modelo.\n\n\n\n\nPredicción\n\nPara predecir la clase a la cual pertenece \\(X\\), \\(Pr[Y = k]Pr[X \\mid Y = k]\\) es evaluado para cada clase \\(k\\) .\nEl clasificador predecirá que los valores de \\(X\\) pertenecen a la clase \\(i\\) si y solo si: \\[\n\\begin{gather}\nPr[Y = i]Pr[X \\mid Y = i] &gt; Pr[Y = j]Pr[X \\mid Y = j], \\\\\n\\; \\forall 1\\leq j\\leq m, \\; j \\ne i\n\\end{gather}\n\\]"
  },
  {
    "objectID": "apuntes/clase-05.html#ejemplo-práctico",
    "href": "apuntes/clase-05.html#ejemplo-práctico",
    "title": "Apuntes de clase",
    "section": "Ejemplo práctico",
    "text": "Ejemplo práctico\n\ndiabetes = read.csv(\n  \"../datos/DiabetesTrain.csv\", stringsAsFactors = TRUE\n)\nhead(diabetes)\n\n  glucose insulin sspg  class\n1      97     289  117 normal\n2     105     319  143 normal\n3      90     356  199 normal\n4      90     323  240 normal\n5      86     381  157 normal\n6     100     350  221 normal\n\n\n\n# ¿Hay normalidad?\ndiabetes$glucose |&gt; hist()\n\n\n\n\n\nSin discretizar y asumiendo normalidad\n\nlibrary(e1071)\n\nWarning: package 'e1071' was built under R version 4.1.3\n\na &lt;- naiveBayes(class ~ .,data = diabetes)\na\n\n\nNaive Bayes Classifier for Discrete Predictors\n\nCall:\nnaiveBayes.default(x = X, y = Y, laplace = laplace)\n\nA-priori probabilities:\nY\nchemical   normal    overt \n0.226087 0.573913 0.200000 \n\nConditional probabilities:\n          glucose\nY               [,1]      [,2]\n  chemical  99.46154  8.805593\n  normal    91.98485  8.164637\n  overt    207.17391 71.837982\n\n          insulin\nY               [,1]      [,2]\n  chemical  504.1154  60.05819\n  normal    351.2121  37.69861\n  overt    1002.9565 315.85288\n\n          sspg\nY              [,1]      [,2]\n  chemical 291.7692 177.65479\n  normal   169.0152  65.48952\n  overt    112.6087 106.57253\n\n\nEsta librería asume que las variables numéricas usadas siguen una distriubución normal marginal.\n\npred = predict(a, diabetes[,-4], type = \"raw\")\npred\n\n            chemical        normal        overt\n  [1,]  5.295056e-04  9.990214e-01 4.491287e-04\n  [2,]  2.339643e-03  9.971217e-01 5.386942e-04\n  [3,]  2.275912e-03  9.976460e-01 7.807859e-05\n  [4,]  1.180417e-03  9.987288e-01 9.074437e-05\n  [5,]  4.361057e-03  9.954717e-01 1.672017e-04\n  [6,]  6.386586e-03  9.934653e-01 1.481366e-04\n  [7,]  2.183812e-04  9.996164e-01 1.652600e-04\n  [8,]  1.109668e-02  9.886942e-01 2.091171e-04\n  [9,]  5.561492e-04  9.991428e-01 3.010012e-04\n [10,]  2.860365e-03  9.970620e-01 7.768089e-05\n [11,]  2.737109e-04  9.995964e-01 1.298984e-04\n [12,]  9.429950e-05  9.990956e-01 8.101451e-04\n [13,]  4.685303e-03  9.952171e-01 9.757051e-05\n [14,]  7.145331e-03  9.924505e-01 4.041340e-04\n [15,]  2.577827e-03  9.973178e-01 1.043346e-04\n [16,]  5.755875e-03  9.941035e-01 1.405755e-04\n [17,]  1.940953e-04  9.994795e-01 3.264158e-04\n [18,]  1.206339e-03  9.986771e-01 1.165314e-04\n [19,]  5.334159e-03  9.945073e-01 1.585289e-04\n [20,]  2.698566e-04  9.995533e-01 1.768468e-04\n [21,]  7.047837e-04  9.992159e-01 7.928524e-05\n [22,]  5.066611e-02  9.491241e-01 2.098169e-04\n [23,]  6.040562e-03  9.937900e-01 1.694724e-04\n [24,]  4.388028e-03  9.954543e-01 1.577203e-04\n [25,]  4.236783e-04  9.994793e-01 9.706961e-05\n [26,]  1.851161e-03  9.980173e-01 1.315720e-04\n [27,]  5.266643e-04  9.969041e-01 2.569231e-03\n [28,]  2.002639e-03  9.978337e-01 1.636298e-04\n [29,]  4.942068e-04  9.990224e-01 4.834124e-04\n [30,]  5.673222e-03  9.942287e-01 9.806927e-05\n [31,]  2.769180e-03  9.970957e-01 1.351083e-04\n [32,]  1.196466e-02  9.877739e-01 2.614334e-04\n [33,]  9.187795e-03  9.906305e-01 1.816555e-04\n [34,]  6.866383e-04  9.990540e-01 2.593738e-04\n [35,]  8.178601e-02  9.165555e-01 1.658472e-03\n [36,]  1.860864e-02  9.810572e-01 3.341775e-04\n [37,]  1.232439e-02  9.875490e-01 1.266344e-04\n [38,]  4.917400e-04  9.990449e-01 4.633993e-04\n [39,]  9.000279e-05  9.982301e-01 1.679900e-03\n [40,]  1.080090e-03  9.986026e-01 3.173065e-04\n [41,]  3.708905e-03  9.961129e-01 1.782156e-04\n [42,]  2.054200e-04  9.995771e-01 2.174545e-04\n [43,]  7.433120e-04  9.991504e-01 1.063132e-04\n [44,]  7.221347e-04  9.991566e-01 1.212746e-04\n [45,]  3.258039e-04  9.995439e-01 1.303026e-04\n [46,]  1.254320e-03  9.986309e-01 1.147530e-04\n [47,]  3.679488e-04  9.995214e-01 1.106108e-04\n [48,]  8.575833e-04  9.988566e-01 2.858096e-04\n [49,]  1.275039e-03  9.983903e-01 3.346247e-04\n [50,]  9.521719e-01  4.412351e-02 3.704616e-03\n [51,]  4.828539e-02  9.510608e-01 6.537688e-04\n [52,]  5.440347e-01  4.450488e-01 1.091648e-02\n [53,]  2.667509e-01  7.317704e-01 1.478681e-03\n [54,]  3.289868e-04  9.994252e-01 2.458509e-04\n [55,]  2.113070e-01  7.871383e-01 1.554676e-03\n [56,]  1.067948e-01  8.923053e-01 8.998199e-04\n [57,]  1.784725e-02  9.818881e-01 2.646238e-04\n [58,]  3.200394e-02  9.671740e-01 8.220511e-04\n [59,]  2.527538e-02  9.744571e-01 2.675100e-04\n [60,]  9.727327e-03  9.899078e-01 3.649193e-04\n [61,]  7.682488e-03  9.922034e-01 1.141017e-04\n [62,]  3.744839e-01  6.238293e-01 1.686796e-03\n [63,]  7.721793e-01  2.230828e-01 4.737862e-03\n [64,]  2.003020e-01  7.989477e-01 7.503410e-04\n [65,]  3.440964e-03  9.964530e-01 1.060147e-04\n [66,]  2.381242e-02  9.760112e-01 1.763595e-04\n [67,]  2.275912e-03  9.976460e-01 7.807859e-05\n [68,]  6.761531e-02  9.312615e-01 1.123204e-03\n [69,]  9.994407e-01  5.208362e-04 3.847674e-05\n [70,]  6.424901e-02  9.345319e-01 1.219088e-03\n [71,]  7.408047e-04  9.989515e-01 3.077439e-04\n [72,]  1.000000e+00  2.338103e-22 9.485134e-09\n [73,]  9.786212e-01  5.814184e-06 2.137299e-02\n [74,]  9.999997e-01  5.502353e-11 3.409691e-07\n [75,]  9.850467e-01  1.231466e-07 1.495318e-02\n [76,]  9.999758e-01  1.838167e-09 2.423367e-05\n [77,]  9.999997e-01  7.331410e-15 3.136628e-07\n [78,]  9.803726e-01  1.880635e-02 8.210894e-04\n [79,]  9.932796e-01  2.821303e-09 6.720425e-03\n [80,]  9.810128e-01  1.852270e-02 4.645416e-04\n [81,]  9.999985e-01  3.314769e-11 1.488563e-06\n [82,]  9.987065e-01  4.618651e-06 1.288886e-03\n [83,]  9.999115e-01  3.207163e-05 5.645985e-05\n [84,]  9.602919e-01  3.594148e-02 3.766582e-03\n [85,]  3.592417e-01  6.378324e-01 2.925914e-03\n [86,]  9.990254e-01  5.759318e-06 9.688743e-04\n [87,]  9.821413e-01  4.745440e-07 1.785827e-02\n [88,]  9.777282e-01  2.132689e-02 9.449512e-04\n [89,]  9.678721e-01  5.272532e-07 3.212741e-02\n [90,]  1.012043e-01  8.981187e-01 6.769971e-04\n [91,]  6.472607e-01  1.473760e-12 3.527393e-01\n [92,]  9.923048e-01  6.191171e-05 7.633263e-03\n [93,] 6.897663e-173  0.000000e+00 1.000000e+00\n [94,]  4.976878e-03  2.580640e-21 9.950231e-01\n [95,] 2.477275e-146 1.702646e-304 1.000000e+00\n [96,]  7.133953e-60 2.662460e-137 1.000000e+00\n [97,]  1.166381e-35  1.111443e-88 1.000000e+00\n [98,] 1.851074e-159 5.163499e-300 1.000000e+00\n [99,]  5.174439e-21  5.308143e-55 1.000000e+00\n[100,]  2.488687e-42  2.626983e-96 1.000000e+00\n[101,]  3.141734e-04  1.022311e-15 9.996858e-01\n[102,] 7.579262e-131 5.081788e-267 1.000000e+00\n[103,]  2.659830e-95 1.653866e-183 1.000000e+00\n[104,]  3.774942e-77 5.127724e-169 1.000000e+00\n[105,]  1.938547e-12  2.930057e-45 1.000000e+00\n[106,]  9.973924e-01  1.162370e-10 2.607642e-03\n[107,]  1.467368e-50 3.974579e-111 1.000000e+00\n[108,] 2.314463e-209  0.000000e+00 1.000000e+00\n[109,]  6.664180e-01  1.305333e-07 3.335819e-01\n[110,]  2.259447e-03  1.233813e-18 9.977406e-01\n[111,]  8.066183e-01  1.579498e-13 1.933817e-01\n[112,]  2.863917e-06  7.337970e-28 9.999971e-01\n[113,]  7.552413e-34  1.074422e-84 1.000000e+00\n[114,] 8.805182e-203  0.000000e+00 1.000000e+00\n[115,]  4.892801e-52 4.067389e-116 1.000000e+00\n\n\n\npred1 = factor(max.col(pred), labels = levels(diabetes$class))\npred1\n\n  [1] normal   normal   normal   normal   normal   normal   normal   normal  \n  [9] normal   normal   normal   normal   normal   normal   normal   normal  \n [17] normal   normal   normal   normal   normal   normal   normal   normal  \n [25] normal   normal   normal   normal   normal   normal   normal   normal  \n [33] normal   normal   normal   normal   normal   normal   normal   normal  \n [41] normal   normal   normal   normal   normal   normal   normal   normal  \n [49] normal   chemical normal   chemical normal   normal   normal   normal  \n [57] normal   normal   normal   normal   normal   normal   chemical normal  \n [65] normal   normal   normal   normal   chemical normal   normal   chemical\n [73] chemical chemical chemical chemical chemical chemical chemical chemical\n [81] chemical chemical chemical chemical normal   chemical chemical chemical\n [89] chemical normal   chemical chemical overt    overt    overt    overt   \n [97] overt    overt    overt    overt    overt    overt    overt    overt   \n[105] overt    chemical overt    overt    chemical overt    chemical overt   \n[113] overt    overt    overt   \nLevels: chemical normal overt\n\n\n\npred1 = predict(a, diabetes[,-4])\npred1\n\n  [1] normal   normal   normal   normal   normal   normal   normal   normal  \n  [9] normal   normal   normal   normal   normal   normal   normal   normal  \n [17] normal   normal   normal   normal   normal   normal   normal   normal  \n [25] normal   normal   normal   normal   normal   normal   normal   normal  \n [33] normal   normal   normal   normal   normal   normal   normal   normal  \n [41] normal   normal   normal   normal   normal   normal   normal   normal  \n [49] normal   chemical normal   chemical normal   normal   normal   normal  \n [57] normal   normal   normal   normal   normal   normal   chemical normal  \n [65] normal   normal   normal   normal   chemical normal   normal   chemical\n [73] chemical chemical chemical chemical chemical chemical chemical chemical\n [81] chemical chemical chemical chemical normal   chemical chemical chemical\n [89] chemical normal   chemical chemical overt    overt    overt    overt   \n [97] overt    overt    overt    overt    overt    overt    overt    overt   \n[105] overt    chemical overt    overt    chemical overt    chemical overt   \n[113] overt    overt    overt   \nLevels: chemical normal overt\n\n\n\ncaret::confusionMatrix(pred1, diabetes[,4])\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction chemical normal overt\n  chemical       20      3     3\n  normal          6     63     0\n  overt           0      0    20\n\nOverall Statistics\n                                          \n               Accuracy : 0.8957          \n                 95% CI : (0.8248, 0.9449)\n    No Information Rate : 0.5739          \n    P-Value [Acc &gt; NIR] : 3.778e-14       \n                                          \n                  Kappa : 0.8169          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: chemical Class: normal Class: overt\nSensitivity                   0.7692        0.9545       0.8696\nSpecificity                   0.9326        0.8776       1.0000\nPos Pred Value                0.7692        0.9130       1.0000\nNeg Pred Value                0.9326        0.9348       0.9684\nPrevalence                    0.2261        0.5739       0.2000\nDetection Rate                0.1739        0.5478       0.1739\nDetection Prevalence          0.2261        0.6000       0.1739\nBalanced Accuracy             0.8509        0.9160       0.9348\n\n\n\n\nSin discretizar ni asumir normalidad\n\nlibrary(naivebayes)\n\nWarning: package 'naivebayes' was built under R version 4.1.3\n\n\nnaivebayes 0.9.7 loaded\n\na &lt;- naive_bayes(class ~ ., data = diabetes)\na\n\n\n================================== Naive Bayes ================================== \n \n Call: \nnaive_bayes.formula(formula = class ~ ., data = diabetes)\n\n--------------------------------------------------------------------------------- \n \nLaplace smoothing: 0\n\n--------------------------------------------------------------------------------- \n \n A priori probabilities: \n\nchemical   normal    overt \n0.226087 0.573913 0.200000 \n\n--------------------------------------------------------------------------------- \n \n Tables: \n\n--------------------------------------------------------------------------------- \n ::: glucose (Gaussian) \n--------------------------------------------------------------------------------- \n       \nglucose   chemical     normal      overt\n   mean  99.461538  91.984848 207.173913\n   sd     8.805593   8.164637  71.837982\n\n--------------------------------------------------------------------------------- \n ::: insulin (Gaussian) \n--------------------------------------------------------------------------------- \n       \ninsulin   chemical     normal      overt\n   mean  504.11538  351.21212 1002.95652\n   sd     60.05819   37.69861  315.85288\n\n--------------------------------------------------------------------------------- \n ::: sspg (Gaussian) \n--------------------------------------------------------------------------------- \n      \nsspg    chemical    normal     overt\n  mean 291.76923 169.01515 112.60870\n  sd   177.65479  65.48952 106.57253\n\n---------------------------------------------------------------------------------\n\n\nPor default, se asumió normalidad marginal, debido al valor FALSE del parámetro usekernel de naive_bayes.\n\npred = predict(a, diabetes[,-4])\npred\n\n  [1] normal   normal   normal   normal   normal   normal   normal   normal  \n  [9] normal   normal   normal   normal   normal   normal   normal   normal  \n [17] normal   normal   normal   normal   normal   normal   normal   normal  \n [25] normal   normal   normal   normal   normal   normal   normal   normal  \n [33] normal   normal   normal   normal   normal   normal   normal   normal  \n [41] normal   normal   normal   normal   normal   normal   normal   normal  \n [49] normal   chemical normal   chemical normal   normal   normal   normal  \n [57] normal   normal   normal   normal   normal   normal   chemical normal  \n [65] normal   normal   normal   normal   chemical normal   normal   chemical\n [73] chemical chemical chemical chemical chemical chemical chemical chemical\n [81] chemical chemical chemical chemical normal   chemical chemical chemical\n [89] chemical normal   chemical chemical overt    overt    overt    overt   \n [97] overt    overt    overt    overt    overt    overt    overt    overt   \n[105] overt    chemical overt    overt    chemical overt    chemical overt   \n[113] overt    overt    overt   \nLevels: chemical normal overt\n\n\n\ncaret::confusionMatrix(pred,diabetes[,4])\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction chemical normal overt\n  chemical       20      3     3\n  normal          6     63     0\n  overt           0      0    20\n\nOverall Statistics\n                                          \n               Accuracy : 0.8957          \n                 95% CI : (0.8248, 0.9449)\n    No Information Rate : 0.5739          \n    P-Value [Acc &gt; NIR] : 3.778e-14       \n                                          \n                  Kappa : 0.8169          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: chemical Class: normal Class: overt\nSensitivity                   0.7692        0.9545       0.8696\nSpecificity                   0.9326        0.8776       1.0000\nPos Pred Value                0.7692        0.9130       1.0000\nNeg Pred Value                0.9326        0.9348       0.9684\nPrevalence                    0.2261        0.5739       0.2000\nDetection Rate                0.1739        0.5478       0.1739\nDetection Prevalence          0.2261        0.6000       0.1739\nBalanced Accuracy             0.8509        0.9160       0.9348\n\npredict(a, diabetes[,-4], type = \"prob\")\n\n            chemical        normal        overt\n  [1,]  5.295056e-04  9.990214e-01 4.491287e-04\n  [2,]  2.339643e-03  9.971217e-01 5.386942e-04\n  [3,]  2.275912e-03  9.976460e-01 7.807859e-05\n  [4,]  1.180417e-03  9.987288e-01 9.074437e-05\n  [5,]  4.361057e-03  9.954717e-01 1.672017e-04\n  [6,]  6.386586e-03  9.934653e-01 1.481366e-04\n  [7,]  2.183812e-04  9.996164e-01 1.652600e-04\n  [8,]  1.109668e-02  9.886942e-01 2.091171e-04\n  [9,]  5.561492e-04  9.991428e-01 3.010012e-04\n [10,]  2.860365e-03  9.970620e-01 7.768089e-05\n [11,]  2.737109e-04  9.995964e-01 1.298984e-04\n [12,]  9.429950e-05  9.990956e-01 8.101451e-04\n [13,]  4.685303e-03  9.952171e-01 9.757051e-05\n [14,]  7.145331e-03  9.924505e-01 4.041340e-04\n [15,]  2.577827e-03  9.973178e-01 1.043346e-04\n [16,]  5.755875e-03  9.941035e-01 1.405755e-04\n [17,]  1.940953e-04  9.994795e-01 3.264158e-04\n [18,]  1.206339e-03  9.986771e-01 1.165314e-04\n [19,]  5.334159e-03  9.945073e-01 1.585289e-04\n [20,]  2.698566e-04  9.995533e-01 1.768468e-04\n [21,]  7.047837e-04  9.992159e-01 7.928524e-05\n [22,]  5.066611e-02  9.491241e-01 2.098169e-04\n [23,]  6.040562e-03  9.937900e-01 1.694724e-04\n [24,]  4.388028e-03  9.954543e-01 1.577203e-04\n [25,]  4.236783e-04  9.994793e-01 9.706961e-05\n [26,]  1.851161e-03  9.980173e-01 1.315720e-04\n [27,]  5.266643e-04  9.969041e-01 2.569231e-03\n [28,]  2.002639e-03  9.978337e-01 1.636298e-04\n [29,]  4.942068e-04  9.990224e-01 4.834124e-04\n [30,]  5.673222e-03  9.942287e-01 9.806927e-05\n [31,]  2.769180e-03  9.970957e-01 1.351083e-04\n [32,]  1.196466e-02  9.877739e-01 2.614334e-04\n [33,]  9.187795e-03  9.906305e-01 1.816555e-04\n [34,]  6.866383e-04  9.990540e-01 2.593738e-04\n [35,]  8.178601e-02  9.165555e-01 1.658472e-03\n [36,]  1.860864e-02  9.810572e-01 3.341775e-04\n [37,]  1.232439e-02  9.875490e-01 1.266344e-04\n [38,]  4.917400e-04  9.990449e-01 4.633993e-04\n [39,]  9.000279e-05  9.982301e-01 1.679900e-03\n [40,]  1.080090e-03  9.986026e-01 3.173065e-04\n [41,]  3.708905e-03  9.961129e-01 1.782156e-04\n [42,]  2.054200e-04  9.995771e-01 2.174545e-04\n [43,]  7.433120e-04  9.991504e-01 1.063132e-04\n [44,]  7.221347e-04  9.991566e-01 1.212746e-04\n [45,]  3.258039e-04  9.995439e-01 1.303026e-04\n [46,]  1.254320e-03  9.986309e-01 1.147530e-04\n [47,]  3.679488e-04  9.995214e-01 1.106108e-04\n [48,]  8.575833e-04  9.988566e-01 2.858096e-04\n [49,]  1.275039e-03  9.983903e-01 3.346247e-04\n [50,]  9.521719e-01  4.412351e-02 3.704616e-03\n [51,]  4.828539e-02  9.510608e-01 6.537688e-04\n [52,]  5.440347e-01  4.450488e-01 1.091648e-02\n [53,]  2.667509e-01  7.317704e-01 1.478681e-03\n [54,]  3.289868e-04  9.994252e-01 2.458509e-04\n [55,]  2.113070e-01  7.871383e-01 1.554676e-03\n [56,]  1.067948e-01  8.923053e-01 8.998199e-04\n [57,]  1.784725e-02  9.818881e-01 2.646238e-04\n [58,]  3.200394e-02  9.671740e-01 8.220511e-04\n [59,]  2.527538e-02  9.744571e-01 2.675100e-04\n [60,]  9.727327e-03  9.899078e-01 3.649193e-04\n [61,]  7.682488e-03  9.922034e-01 1.141017e-04\n [62,]  3.744839e-01  6.238293e-01 1.686796e-03\n [63,]  7.721793e-01  2.230828e-01 4.737862e-03\n [64,]  2.003020e-01  7.989477e-01 7.503410e-04\n [65,]  3.440964e-03  9.964530e-01 1.060147e-04\n [66,]  2.381242e-02  9.760112e-01 1.763595e-04\n [67,]  2.275912e-03  9.976460e-01 7.807859e-05\n [68,]  6.761531e-02  9.312615e-01 1.123204e-03\n [69,]  9.994407e-01  5.208362e-04 3.847674e-05\n [70,]  6.424901e-02  9.345319e-01 1.219088e-03\n [71,]  7.408047e-04  9.989515e-01 3.077439e-04\n [72,]  1.000000e+00  2.338103e-22 9.485134e-09\n [73,]  9.786212e-01  5.814184e-06 2.137299e-02\n [74,]  9.999997e-01  5.502353e-11 3.409691e-07\n [75,]  9.850467e-01  1.231466e-07 1.495318e-02\n [76,]  9.999758e-01  1.838167e-09 2.423367e-05\n [77,]  9.999997e-01  7.331410e-15 3.136628e-07\n [78,]  9.803726e-01  1.880635e-02 8.210894e-04\n [79,]  9.932796e-01  2.821303e-09 6.720425e-03\n [80,]  9.810128e-01  1.852270e-02 4.645416e-04\n [81,]  9.999985e-01  3.314769e-11 1.488563e-06\n [82,]  9.987065e-01  4.618651e-06 1.288886e-03\n [83,]  9.999115e-01  3.207163e-05 5.645985e-05\n [84,]  9.602919e-01  3.594148e-02 3.766582e-03\n [85,]  3.592417e-01  6.378324e-01 2.925914e-03\n [86,]  9.990254e-01  5.759318e-06 9.688743e-04\n [87,]  9.821413e-01  4.745440e-07 1.785827e-02\n [88,]  9.777282e-01  2.132689e-02 9.449512e-04\n [89,]  9.678721e-01  5.272532e-07 3.212741e-02\n [90,]  1.012043e-01  8.981187e-01 6.769971e-04\n [91,]  6.472607e-01  1.473760e-12 3.527393e-01\n [92,]  9.923048e-01  6.191171e-05 7.633263e-03\n [93,] 6.897663e-173  0.000000e+00 1.000000e+00\n [94,]  4.976878e-03  2.580640e-21 9.950231e-01\n [95,] 2.477275e-146 1.702646e-304 1.000000e+00\n [96,]  7.133953e-60 2.662460e-137 1.000000e+00\n [97,]  1.166381e-35  1.111443e-88 1.000000e+00\n [98,] 1.851074e-159 5.163499e-300 1.000000e+00\n [99,]  5.174439e-21  5.308143e-55 1.000000e+00\n[100,]  2.488687e-42  2.626983e-96 1.000000e+00\n[101,]  3.141734e-04  1.022311e-15 9.996858e-01\n[102,] 7.579262e-131 5.081788e-267 1.000000e+00\n[103,]  2.659830e-95 1.653866e-183 1.000000e+00\n[104,]  3.774942e-77 5.127724e-169 1.000000e+00\n[105,]  1.938547e-12  2.930057e-45 1.000000e+00\n[106,]  9.973924e-01  1.162370e-10 2.607642e-03\n[107,]  1.467368e-50 3.974579e-111 1.000000e+00\n[108,] 2.314463e-209  0.000000e+00 1.000000e+00\n[109,]  6.664180e-01  1.305333e-07 3.335819e-01\n[110,]  2.259447e-03  1.233813e-18 9.977406e-01\n[111,]  8.066183e-01  1.579498e-13 1.933817e-01\n[112,]  2.863917e-06  7.337970e-28 9.999971e-01\n[113,]  7.552413e-34  1.074422e-84 1.000000e+00\n[114,] 8.805182e-203  0.000000e+00 1.000000e+00\n[115,]  4.892801e-52 4.067389e-116 1.000000e+00\n\n\nAhora no asumiremos normalidad marginal:\n\na &lt;- naive_bayes(class ~ ., data = diabetes, usekernel = TRUE)\na\n\n\n================================== Naive Bayes ================================== \n \n Call: \nnaive_bayes.formula(formula = class ~ ., data = diabetes, usekernel = TRUE)\n\n--------------------------------------------------------------------------------- \n \nLaplace smoothing: 0\n\n--------------------------------------------------------------------------------- \n \n A priori probabilities: \n\nchemical   normal    overt \n0.226087 0.573913 0.200000 \n\n--------------------------------------------------------------------------------- \n \n Tables: \n\n--------------------------------------------------------------------------------- \n ::: glucose::chemical (KDE)\n--------------------------------------------------------------------------------- \n\nCall:\n    density.default(x = x, na.rm = TRUE)\n\nData: x (26 obs.);  Bandwidth 'bw' = 4.131\n\n       x                y            \n Min.   : 72.61   Min.   :4.949e-05  \n 1st Qu.: 86.05   1st Qu.:3.464e-03  \n Median : 99.50   Median :2.127e-02  \n Mean   : 99.50   Mean   :1.857e-02  \n 3rd Qu.:112.95   3rd Qu.:3.093e-02  \n Max.   :126.39   Max.   :3.998e-02  \n\n--------------------------------------------------------------------------------- \n ::: glucose::normal (KDE)\n--------------------------------------------------------------------------------- \n\nCall:\n    density.default(x = x, na.rm = TRUE)\n\nData: x (66 obs.);  Bandwidth 'bw' = 3.179\n\n       x                y            \n Min.   : 60.46   Min.   :2.145e-05  \n 1st Qu.: 75.73   1st Qu.:2.132e-03  \n Median : 91.00   Median :8.905e-03  \n Mean   : 91.00   Mean   :1.636e-02  \n 3rd Qu.:106.27   3rd Qu.:3.114e-02  \n Max.   :121.54   Max.   :4.846e-02  \n\n--------------------------------------------------------------------------------- \n ::: glucose::overt (KDE)\n--------------------------------------------------------------------------------- \n\nCall:\n    density.default(x = x, na.rm = TRUE)\n\nData: x (23 obs.);  Bandwidth 'bw' = 34.53\n\n       x               y            \n Min.   : 16.4   Min.   :8.447e-06  \n 1st Qu.:122.9   1st Qu.:5.362e-04  \n Median :229.5   Median :2.638e-03  \n Mean   :229.5   Mean   :2.343e-03  \n 3rd Qu.:336.1   3rd Qu.:3.926e-03  \n Max.   :442.6   Max.   :4.754e-03  \n\n--------------------------------------------------------------------------------- \n ::: insulin::chemical (KDE)\n--------------------------------------------------------------------------------- \n\nCall:\n    density.default(x = x, na.rm = TRUE)\n\nData: x (26 obs.);  Bandwidth 'bw' = 28.17\n\n       x               y            \n Min.   :338.5   Min.   :6.131e-06  \n 1st Qu.:435.7   1st Qu.:4.410e-04  \n Median :533.0   Median :2.180e-03  \n Mean   :533.0   Mean   :2.567e-03  \n 3rd Qu.:630.3   3rd Qu.:4.770e-03  \n Max.   :727.5   Max.   :5.687e-03  \n\n--------------------------------------------------------------------------------- \n ::: insulin::normal (KDE)\n--------------------------------------------------------------------------------- \n\nCall:\n    density.default(x = x, na.rm = TRUE)\n\nData: x (66 obs.);  Bandwidth 'bw' = 14.68\n\n       x               y            \n Min.   :225.0   Min.   :4.725e-06  \n 1st Qu.:286.2   1st Qu.:5.085e-04  \n Median :347.5   Median :3.890e-03  \n Mean   :347.5   Mean   :4.076e-03  \n 3rd Qu.:408.8   3rd Qu.:7.314e-03  \n Max.   :470.0   Max.   :9.098e-03  \n\n--------------------------------------------------------------------------------- \n ::: insulin::overt (KDE)\n--------------------------------------------------------------------------------- \n\nCall:\n    density.default(x = x, na.rm = TRUE)\n\nData: x (23 obs.);  Bandwidth 'bw' = 151.8\n\n       x                 y            \n Min.   :  82.48   Min.   :2.495e-06  \n 1st Qu.: 555.74   1st Qu.:1.141e-04  \n Median :1029.00   Median :6.280e-04  \n Mean   :1029.00   Mean   :5.276e-04  \n 3rd Qu.:1502.26   3rd Qu.:8.772e-04  \n Max.   :1975.52   Max.   :1.019e-03  \n\n--------------------------------------------------------------------------------- \n ::: sspg::chemical (KDE)\n--------------------------------------------------------------------------------- \n\nCall:\n    density.default(x = x, na.rm = TRUE)\n\nData: x (26 obs.);  Bandwidth 'bw' = 60.82\n\n       x                y            \n Min.   :-73.47   Min.   :2.835e-06  \n 1st Qu.:177.52   1st Qu.:2.706e-04  \n Median :428.50   Median :6.522e-04  \n Mean   :428.50   Mean   :9.949e-04  \n 3rd Qu.:679.48   3rd Qu.:1.536e-03  \n Max.   :930.47   Max.   :3.155e-03  \n\n--------------------------------------------------------------------------------- \n ::: sspg::normal (KDE)\n--------------------------------------------------------------------------------- \n\nCall:\n    density.default(x = x, na.rm = TRUE)\n\nData: x (66 obs.);  Bandwidth 'bw' = 19.76\n\n       x                y            \n Min.   : 13.73   Min.   :1.000e-09  \n 1st Qu.:147.61   1st Qu.:2.739e-05  \n Median :281.50   Median :3.686e-04  \n Mean   :281.50   Mean   :1.865e-03  \n 3rd Qu.:415.39   3rd Qu.:2.967e-03  \n Max.   :549.27   Max.   :7.958e-03  \n\n--------------------------------------------------------------------------------- \n ::: sspg::overt (KDE)\n--------------------------------------------------------------------------------- \n\nCall:\n    density.default(x = x, na.rm = TRUE)\n\nData: x (23 obs.);  Bandwidth 'bw' = 31.39\n\n       x                y            \n Min.   :-84.17   Min.   :6.215e-06  \n 1st Qu.: 75.41   1st Qu.:2.632e-04  \n Median :235.00   Median :5.806e-04  \n Mean   :235.00   Mean   :1.565e-03  \n 3rd Qu.:394.59   3rd Qu.:2.299e-03  \n Max.   :554.17   Max.   :5.651e-03  \n\n---------------------------------------------------------------------------------\n\nplot(a)\n\n\n\n\n\n\n\n\n\n\n\npred = predict(a, diabetes[,-4])\npred\n\n  [1] normal   normal   normal   normal   normal   normal   normal   normal  \n  [9] normal   normal   normal   normal   normal   normal   normal   normal  \n [17] normal   normal   normal   normal   normal   normal   normal   normal  \n [25] normal   normal   normal   normal   normal   normal   normal   normal  \n [33] normal   normal   normal   normal   normal   normal   normal   normal  \n [41] normal   normal   normal   normal   normal   normal   normal   normal  \n [49] normal   chemical normal   chemical chemical normal   normal   normal  \n [57] normal   normal   normal   normal   normal   normal   chemical normal  \n [65] normal   normal   normal   normal   normal   normal   normal   chemical\n [73] chemical chemical chemical chemical chemical chemical chemical chemical\n [81] chemical chemical chemical chemical chemical chemical chemical chemical\n [89] chemical normal   chemical chemical overt    overt    overt    overt   \n [97] overt    overt    overt    overt    overt    overt    overt    overt   \n[105] overt    chemical overt    overt    chemical overt    chemical overt   \n[113] overt    overt    overt   \nLevels: chemical normal overt\n\ncaret::confusionMatrix(pred, diabetes[,4])\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction chemical normal overt\n  chemical       22      2     3\n  normal          4     64     0\n  overt           0      0    20\n\nOverall Statistics\n                                          \n               Accuracy : 0.9217          \n                 95% CI : (0.8566, 0.9636)\n    No Information Rate : 0.5739          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.8634          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: chemical Class: normal Class: overt\nSensitivity                   0.8462        0.9697       0.8696\nSpecificity                   0.9438        0.9184       1.0000\nPos Pred Value                0.8148        0.9412       1.0000\nNeg Pred Value                0.9545        0.9574       0.9684\nPrevalence                    0.2261        0.5739       0.2000\nDetection Rate                0.1913        0.5565       0.1739\nDetection Prevalence          0.2348        0.5913       0.1739\nBalanced Accuracy             0.8950        0.9440       0.9348\n\n\n\n\nDiscretizar usando Chi-Merge\n\nUna discretización no supervisada no toma en cuenta los valores de la response, al momento de discretizar uno o más predictores.\nPor otro lado, la discretización supervisada sí considera la respuesta al momento de discretizar uno o más predictores, con el fin de crear intervalos de discretización que maximicen la probabilidad de predecir adecuadamente una training observation, no de tipo testing.\nChi-Merge\n\n\n# Discretizando por el método Chi-Merge\nlibrary(discretization)\n\nWarning: package 'discretization' was built under R version 4.1.3\n\n# Discretizamos todas las columnas, salvo la última.\nd_diab = chiM(diabetes, 0.01)$Disc.data\n\n# Discretizamos solo la primera y tercera columnas\n# d_diab = chiM(diabetes[, c(1, 3)], 0.01)$Disc.data\n\n\n# Convertimos a categórica para evitar que la función\n# naive_bayes asuma con variables numéricas (gaussianas o no)\nfor (i in 1:3) {\n  d_diab[,i] &lt;- as.factor(d_diab[,i])\n}\n\nd_diab\n\n    glucose insulin sspg    class\n1         1       1    2   normal\n2         2       1    3   normal\n3         1       1    3   normal\n4         1       1    3   normal\n5         1       1    3   normal\n6         1       1    3   normal\n7         1       1    3   normal\n8         1       1    3   normal\n9         1       1    2   normal\n10        1       1    3   normal\n11        1       1    3   normal\n12        1       1    2   normal\n13        1       1    3   normal\n14        1       1    3   normal\n15        1       1    3   normal\n16        1       1    3   normal\n17        1       1    2   normal\n18        1       1    2   normal\n19        1       1    2   normal\n20        1       1    2   normal\n21        1       1    3   normal\n22        1       1    3   normal\n23        1       1    3   normal\n24        1       1    3   normal\n25        1       1    3   normal\n26        1       1    3   normal\n27        1       1    2   normal\n28        1       1    3   normal\n29        1       1    3   normal\n30        1       1    3   normal\n31        1       1    3   normal\n32        1       1    2   normal\n33        1       1    3   normal\n34        1       1    3   normal\n35        2       1    2   normal\n36        2       1    3   normal\n37        1       1    3   normal\n38        1       1    2   normal\n39        1       1    2   normal\n40        1       1    2   normal\n41        1       1    2   normal\n42        1       1    3   normal\n43        1       1    3   normal\n44        1       1    3   normal\n45        1       1    3   normal\n46        1       1    3   normal\n47        1       1    3   normal\n48        1       1    2   normal\n49        1       1    2   normal\n50        1       2    3 chemical\n51        1       1    2   normal\n52        2       2    2   normal\n53        1       2    3 chemical\n54        1       1    2   normal\n55        1       2    3 chemical\n56        1       1    2   normal\n57        1       1    2   normal\n58        1       1    3   normal\n59        1       1    3   normal\n60        1       1    2   normal\n61        1       1    3   normal\n62        2       1    3   normal\n63        2       1    3   normal\n64        1       2    3 chemical\n65        1       1    3   normal\n66        1       1    3   normal\n67        1       1    3   normal\n68        1       1    2   normal\n69        1       1    4   normal\n70        1       2    3 chemical\n71        1       1    2   normal\n72        2       2    4 chemical\n73        2       2    3 chemical\n74        2       2    4 chemical\n75        2       2    3 chemical\n76        2       2    4 chemical\n77        2       2    4 chemical\n78        1       2    4 chemical\n79        2       2    3 chemical\n80        1       2    4 chemical\n81        1       2    4 chemical\n82        2       2    4 chemical\n83        1       2    4 chemical\n84        2       2    3 chemical\n85        1       2    2 chemical\n86        1       2    4 chemical\n87        1       2    2 chemical\n88        1       2    4 chemical\n89        2       2    2 chemical\n90        1       2    3 chemical\n91        2       2    3 chemical\n92        2       2    2 chemical\n93        3       3    1    overt\n94        3       3    3    overt\n95        3       3    1    overt\n96        3       3    2    overt\n97        3       3    2    overt\n98        3       3    1    overt\n99        3       3    2    overt\n100       3       3    2    overt\n101       3       2    2    overt\n102       3       3    1    overt\n103       3       3    2    overt\n104       3       3    2    overt\n105       3       3    2    overt\n106       3       2    4    overt\n107       3       3    1    overt\n108       3       3    1    overt\n109       3       2    2    overt\n110       3       3    1    overt\n111       3       2    4    overt\n112       3       3    3    overt\n113       3       3    2    overt\n114       3       3    1    overt\n115       3       3    1    overt\n\n\nEsta librería asume que la última columna del data frame es la de la clase que deseamos predecir.\n\n# Sin corrección de Laplace (observar prob. cond.)\nb0 &lt;- naive_bayes(class ~ ., data = d_diab)\n\nWarning: naive_bayes(): Feature glucose - zero probabilities are present.\nConsider Laplace smoothing.\n\n\nWarning: naive_bayes(): Feature insulin - zero probabilities are present.\nConsider Laplace smoothing.\n\n\nWarning: naive_bayes(): Feature sspg - zero probabilities are present. Consider\nLaplace smoothing.\n\nb0\n\n\n================================== Naive Bayes ================================== \n \n Call: \nnaive_bayes.formula(formula = class ~ ., data = d_diab)\n\n--------------------------------------------------------------------------------- \n \nLaplace smoothing: 0\n\n--------------------------------------------------------------------------------- \n \n A priori probabilities: \n\nchemical   normal    overt \n0.226087 0.573913 0.200000 \n\n--------------------------------------------------------------------------------- \n \n Tables: \n\n--------------------------------------------------------------------------------- \n ::: glucose (Categorical) \n--------------------------------------------------------------------------------- \n       \nglucose   chemical     normal      overt\n      1 0.53846154 0.90909091 0.00000000\n      2 0.46153846 0.09090909 0.00000000\n      3 0.00000000 0.00000000 1.00000000\n\n--------------------------------------------------------------------------------- \n ::: insulin (Categorical) \n--------------------------------------------------------------------------------- \n       \ninsulin   chemical     normal      overt\n      1 0.00000000 0.98484848 0.00000000\n      2 1.00000000 0.01515152 0.17391304\n      3 0.00000000 0.00000000 0.82608696\n\n--------------------------------------------------------------------------------- \n ::: sspg (Categorical) \n--------------------------------------------------------------------------------- \n    \nsspg   chemical     normal      overt\n   1 0.00000000 0.00000000 0.39130435\n   2 0.15384615 0.36363636 0.43478261\n   3 0.42307692 0.62121212 0.08695652\n   4 0.42307692 0.01515152 0.08695652\n\n---------------------------------------------------------------------------------\n\n# Aplicando laplaciano = 1\nb1 &lt;- naive_bayes(class ~ ., data = d_diab, laplace = 1)\nb1\n\n\n================================== Naive Bayes ================================== \n \n Call: \nnaive_bayes.formula(formula = class ~ ., data = d_diab, laplace = 1)\n\n--------------------------------------------------------------------------------- \n \nLaplace smoothing: 1\n\n--------------------------------------------------------------------------------- \n \n A priori probabilities: \n\nchemical   normal    overt \n0.226087 0.573913 0.200000 \n\n--------------------------------------------------------------------------------- \n \n Tables: \n\n--------------------------------------------------------------------------------- \n ::: glucose (Categorical) \n--------------------------------------------------------------------------------- \n       \nglucose   chemical     normal      overt\n      1 0.51724138 0.88405797 0.03846154\n      2 0.44827586 0.10144928 0.03846154\n      3 0.03448276 0.01449275 0.92307692\n\n--------------------------------------------------------------------------------- \n ::: insulin (Categorical) \n--------------------------------------------------------------------------------- \n       \ninsulin   chemical     normal      overt\n      1 0.03448276 0.95652174 0.03846154\n      2 0.93103448 0.02898551 0.19230769\n      3 0.03448276 0.01449275 0.76923077\n\n--------------------------------------------------------------------------------- \n ::: sspg (Categorical) \n--------------------------------------------------------------------------------- \n    \nsspg   chemical     normal      overt\n   1 0.03333333 0.01428571 0.37037037\n   2 0.16666667 0.35714286 0.40740741\n   3 0.40000000 0.60000000 0.11111111\n   4 0.40000000 0.02857143 0.11111111\n\n---------------------------------------------------------------------------------\n\n\n\npred0 = predict(b0, d_diab[,-4])\ncaret::confusionMatrix(pred0, d_diab[,4])\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction chemical normal overt\n  chemical       26      1     0\n  normal          0     65     0\n  overt           0      0    23\n\nOverall Statistics\n                                          \n               Accuracy : 0.9913          \n                 95% CI : (0.9525, 0.9998)\n    No Information Rate : 0.5739          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.9851          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: chemical Class: normal Class: overt\nSensitivity                   1.0000        0.9848          1.0\nSpecificity                   0.9888        1.0000          1.0\nPos Pred Value                0.9630        1.0000          1.0\nNeg Pred Value                1.0000        0.9800          1.0\nPrevalence                    0.2261        0.5739          0.2\nDetection Rate                0.2261        0.5652          0.2\nDetection Prevalence          0.2348        0.5652          0.2\nBalanced Accuracy             0.9944        0.9924          1.0\n\n\n\npred1 = predict(b1, d_diab[,-4])\ncaret::confusionMatrix(pred1, d_diab[,4])\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction chemical normal overt\n  chemical       26      1     0\n  normal          0     65     0\n  overt           0      0    23\n\nOverall Statistics\n                                          \n               Accuracy : 0.9913          \n                 95% CI : (0.9525, 0.9998)\n    No Information Rate : 0.5739          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.9851          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: chemical Class: normal Class: overt\nSensitivity                   1.0000        0.9848          1.0\nSpecificity                   0.9888        1.0000          1.0\nPos Pred Value                0.9630        1.0000          1.0\nNeg Pred Value                1.0000        0.9800          1.0\nPrevalence                    0.2261        0.5739          0.2\nDetection Rate                0.2261        0.5652          0.2\nDetection Prevalence          0.2348        0.5652          0.2\nBalanced Accuracy             0.9944        0.9924          1.0\n\n\n\n\nTesting data\n\n# Datos de Prueba\ndiabetes_test = read.csv(\"../datos/DiabetesTest.csv\")\ndiabetes_test$class &lt;- as.factor(diabetes_test$class)\n\nhead(diabetes_test)\n\n  glucose insulin sspg    class\n1      89     472  162 chemical\n2      96     465  237 chemical\n3     112     503  408 chemical\n4     110     477  124 chemical\n5      90     413  344 chemical\n6     102     472  297 chemical\n\ncaret::confusionMatrix(\n  predict(a, diabetes_test[,-4]), diabetes_test[,4]\n)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction chemical normal overt\n  chemical        9      1     0\n  normal          1      9     0\n  overt           0      0    10\n\nOverall Statistics\n                                          \n               Accuracy : 0.9333          \n                 95% CI : (0.7793, 0.9918)\n    No Information Rate : 0.3333          \n    P-Value [Acc &gt; NIR] : 8.747e-12       \n                                          \n                  Kappa : 0.9             \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: chemical Class: normal Class: overt\nSensitivity                   0.9000        0.9000       1.0000\nSpecificity                   0.9500        0.9500       1.0000\nPos Pred Value                0.9000        0.9000       1.0000\nNeg Pred Value                0.9500        0.9500       1.0000\nPrevalence                    0.3333        0.3333       0.3333\nDetection Rate                0.3000        0.3000       0.3333\nDetection Prevalence          0.3333        0.3333       0.3333\nBalanced Accuracy             0.9250        0.9250       1.0000\n\n\n\nTheorem 1 Si se discretizó el training set, usar esos mismos intervalos de discretazicación para el testing set.\n\n\n# Intervalos de discretización\nalpha &lt;- 0.01\nd &lt;- chiM(diabetes, alpha)\nd$cutp\n\n[[1]]\n[1] 100.5 117.0\n\n[[2]]\n[1] 420.5 656.5\n\n[[3]]\n[1]  63.5 140.5 283.5\n\n\nEl nivel de discretización (alpha) también es un hiperparámtro.\n\nProposition 1 Ejercicio\nDiscretizar el testing dataset usando los intervalos de discretización del training dataset, para evaluar la matriz de confusión asociada a los modelos b0 y b1.\n\n\nProposition 2 Ejercicio\nImplementar un for loop para el hiperparámetro alpha, y obtener la matriz de confusión asociada al modelo con discretización Chi-Merge."
  },
  {
    "objectID": "apuntes/clase-05.html#k-vecinos-más-cercanos-knn",
    "href": "apuntes/clase-05.html#k-vecinos-más-cercanos-knn",
    "title": "Apuntes de clase",
    "section": "K-vecinos más cercanos (KNN)",
    "text": "K-vecinos más cercanos (KNN)\n\nNo es eficiente para inferencia, es decir, entender el efecto de predictores sobre la response.\nÚtil para regresión y clasificación.\nDifícil de intepretar.\nNo le afecta si son varias clases para respuesta.\nLlega a demorar computacionalmente para altos volúmenes de datos, por lo que no es un algoritmo escalable.\nSe le considera un lazy algorithm, pues no estima nada, no es que genere una función de predicción, como hemos vimos en otros algoritmos.   Por ello, si se aumenta la cantidad de datos de entrenamiento, habría que ejecutar todo el algoritmo de nuevo.\nSe emplea para predictores numéricos.\nSe deben estandarizar los datos (mapeándolos al intervalo \\(\\left( 0,1 \\right)\\) por ejemplo), puesto que las unidades de los predictores pueden alterar la noción de cercanía.\nSe emplea para predictores numéricos.\nSe deben estandarizar los datos (mapeándolos al intervalo \\(\\left( 0,1 \\right)\\) por ejemplo), puesto que las unidades de los predictores pueden alterar la noción de cercanía.\nEn el caso se desee usar predictores categóricos en el modelo, se tienen algunas alternativas:\n\nEstimar variables latentes asociadas a predictores categóricos (lo veremos en Stats. Learning 2)\nBinarizar las variables categóricas, pero sin descartar las columnas consideradas extra, cuando se hace el tratamiento de dummy variables.\n\n\n\nPasos\n\nElegimos dos parámetros:\n\nMétrica para calcular distancias.\nValor de K (número de vecinos a considerar).\n\nDada una observación \\(x_0\\), buscar los \\(K\\) puntos de datos de entrenamiento más cercanos a \\(x_0\\).\nEstos \\(K\\) puntos forman la vecindad \\(\\mathcal{N}_0\\) de \\(x_0\\).\nLa clasificación se realiza vía algún tipo de promedio. Por ejemplo, media (ponderada o no) para el caso de regresión; y, moda, para el caso de clasificación.\n\n\n\n¿Cómo elegir K?\n\n\\(K = 1\\) genera un modelo muy flexible.\n\\(K\\) muy grande puede generar una separación via un hiperplano, por lo que se espera un resultado similar a usar Análisis Discriminante."
  },
  {
    "objectID": "apuntes/clase-05.html#ejemplo-práctico-1",
    "href": "apuntes/clase-05.html#ejemplo-práctico-1",
    "title": "Apuntes de clase",
    "section": "Ejemplo práctico",
    "text": "Ejemplo práctico\n\nlibrary(class)\n\nWarning: package 'class' was built under R version 4.1.3\n\nb &lt;- knn(\n  # Repetimos los datos, por fines ilustrativos\n  train = diabetes[,1:3], test = diabetes[,1:3],\n  cl = diabetes[,4]\n  # Por default, K vale 1\n)\nb\n\n  [1] normal   normal   normal   normal   normal   normal   normal   normal  \n  [9] normal   normal   normal   normal   normal   normal   normal   normal  \n [17] normal   normal   normal   normal   normal   normal   normal   normal  \n [25] normal   normal   normal   normal   normal   normal   normal   normal  \n [33] normal   normal   normal   normal   normal   normal   normal   normal  \n [41] normal   normal   normal   normal   normal   normal   normal   normal  \n [49] normal   chemical normal   normal   chemical normal   chemical normal  \n [57] normal   normal   normal   normal   normal   normal   normal   chemical\n [65] normal   normal   normal   normal   normal   chemical normal   chemical\n [73] chemical chemical chemical chemical chemical chemical chemical chemical\n [81] chemical chemical chemical chemical chemical chemical chemical chemical\n [89] chemical chemical chemical chemical overt    overt    overt    overt   \n [97] overt    overt    overt    overt    overt    overt    overt    overt   \n[105] overt    overt    overt    overt    overt    overt    overt    overt   \n[113] overt    overt    overt   \nLevels: chemical normal overt\n\n# Estimacion del error por resubstitución\ncaret::confusionMatrix(b, diabetes[,4])\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction chemical normal overt\n  chemical       26      0     0\n  normal          0     66     0\n  overt           0      0    23\n\nOverall Statistics\n                                     \n               Accuracy : 1          \n                 95% CI : (0.9684, 1)\n    No Information Rate : 0.5739     \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16  \n                                     \n                  Kappa : 1          \n                                     \n Mcnemar's Test P-Value : NA         \n\nStatistics by Class:\n\n                     Class: chemical Class: normal Class: overt\nSensitivity                   1.0000        1.0000          1.0\nSpecificity                   1.0000        1.0000          1.0\nPos Pred Value                1.0000        1.0000          1.0\nNeg Pred Value                1.0000        1.0000          1.0\nPrevalence                    0.2261        0.5739          0.2\nDetection Rate                0.2261        0.5739          0.2\nDetection Prevalence          0.2261        0.5739          0.2\nBalanced Accuracy             1.0000        1.0000          1.0\n\n\n\n# con K=3\nk_3 &lt;- knn(\n  train = diabetes[,1:3], test = diabetes[,1:3],\n  cl = diabetes[,4], k = 3\n)\ncaret::confusionMatrix(k_3, diabetes[,4])\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction chemical normal overt\n  chemical       25      1     4\n  normal          1     65     0\n  overt           0      0    19\n\nOverall Statistics\n                                          \n               Accuracy : 0.9478          \n                 95% CI : (0.8899, 0.9806)\n    No Information Rate : 0.5739          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.9098          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: chemical Class: normal Class: overt\nSensitivity                   0.9615        0.9848       0.8261\nSpecificity                   0.9438        0.9796       1.0000\nPos Pred Value                0.8333        0.9848       1.0000\nNeg Pred Value                0.9882        0.9796       0.9583\nPrevalence                    0.2261        0.5739       0.2000\nDetection Rate                0.2174        0.5652       0.1652\nDetection Prevalence          0.2609        0.5739       0.1652\nBalanced Accuracy             0.9527        0.9822       0.9130\n\nmean(k_3 == diabetes[,4])\n\n[1] 0.9478261\n\n# con K=7\nk_7 &lt;- knn(\n  train = diabetes[,1:3], test = diabetes[,1:3],\n  cl = diabetes[,4], k = 7\n)\ncaret::confusionMatrix(k_7, diabetes[,4])\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction chemical normal overt\n  chemical       22      1     5\n  normal          4     65     0\n  overt           0      0    18\n\nOverall Statistics\n                                          \n               Accuracy : 0.913           \n                 95% CI : (0.8459, 0.9575)\n    No Information Rate : 0.5739          \n    P-Value [Acc &gt; NIR] : 8.022e-16       \n                                          \n                  Kappa : 0.8473          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: chemical Class: normal Class: overt\nSensitivity                   0.8462        0.9848       0.7826\nSpecificity                   0.9326        0.9184       1.0000\nPos Pred Value                0.7857        0.9420       1.0000\nNeg Pred Value                0.9540        0.9783       0.9485\nPrevalence                    0.2261        0.5739       0.2000\nDetection Rate                0.1913        0.5652       0.1565\nDetection Prevalence          0.2435        0.6000       0.1565\nBalanced Accuracy             0.8894        0.9516       0.8913\n\nmean(k_7 == diabetes[,4])\n\n[1] 0.9130435\n\n# con K=15\nk_15 &lt;- knn(\n  train = diabetes[,1:3], test = diabetes[,1:3],\n  cl = diabetes[,4], k = 15\n)\ncaret::confusionMatrix(k_15, diabetes[,4])\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction chemical normal overt\n  chemical       20      1     5\n  normal          6     65     0\n  overt           0      0    18\n\nOverall Statistics\n                                          \n               Accuracy : 0.8957          \n                 95% CI : (0.8248, 0.9449)\n    No Information Rate : 0.5739          \n    P-Value [Acc &gt; NIR] : 3.778e-14       \n                                          \n                  Kappa : 0.8147          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: chemical Class: normal Class: overt\nSensitivity                   0.7692        0.9848       0.7826\nSpecificity                   0.9326        0.8776       1.0000\nPos Pred Value                0.7692        0.9155       1.0000\nNeg Pred Value                0.9326        0.9773       0.9485\nPrevalence                    0.2261        0.5739       0.2000\nDetection Rate                0.1739        0.5652       0.1565\nDetection Prevalence          0.2261        0.6174       0.1565\nBalanced Accuracy             0.8509        0.9312       0.8913\n\nmean(k_15 == diabetes[,4])\n\n[1] 0.8956522\n\n\n\nTesting data\n\n# Aplicando K=3 en los datos de test\ndiabetes_test = read.csv(\"../datos/DiabetesTest.csv\")\ndiabetes_test$class &lt;- as.factor(diabetes_test$class)\n\nK_3 &lt;- knn(\n  train = diabetes[,1:3], test = diabetes_test[,-4],\n  cl = diabetes[,4], k = 3, prob = TRUE\n)\n\n# Obtener la proporción de votos para la clase ganadora\nK_3_prob &lt;- attr(K_3, \"prob\")\n\n# Valores predichos\nhead(K_3)\n\n[1] chemical chemical chemical chemical normal   chemical\nLevels: chemical normal overt\n\n# Proporciones de \"votos\"\nhead(K_3_prob)\n\n[1] 1.0000000 1.0000000 0.6666667 1.0000000 0.6666667 1.0000000\n\n\n\ncaret::confusionMatrix(K_3, diabetes_test[,4])\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction chemical normal overt\n  chemical        8      0     0\n  normal          2     10     0\n  overt           0      0    10\n\nOverall Statistics\n                                          \n               Accuracy : 0.9333          \n                 95% CI : (0.7793, 0.9918)\n    No Information Rate : 0.3333          \n    P-Value [Acc &gt; NIR] : 8.747e-12       \n                                          \n                  Kappa : 0.9             \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: chemical Class: normal Class: overt\nSensitivity                   0.8000        1.0000       1.0000\nSpecificity                   1.0000        0.9000       1.0000\nPos Pred Value                1.0000        0.8333       1.0000\nNeg Pred Value                0.9091        1.0000       1.0000\nPrevalence                    0.3333        0.3333       0.3333\nDetection Rate                0.2667        0.3333       0.3333\nDetection Prevalence          0.2667        0.4000       0.3333\nBalanced Accuracy             0.9000        0.9500       1.0000"
  },
  {
    "objectID": "apuntes/clase-06.html#ejemplo-práctico-knn",
    "href": "apuntes/clase-06.html#ejemplo-práctico-knn",
    "title": "Apuntes de clase",
    "section": "Ejemplo práctico: KNN",
    "text": "Ejemplo práctico: KNN\n\nlibrary(class)\n\nWarning: package 'class' was built under R version 4.1.3\n\nadmision &lt;- read.csv(\"../datos/binary.csv\")\nadmision$rank &lt;- factor(admision$rank)\nadmision$admit &lt;- factor(admision$admit)\nhead(admision)\n\n  admit gre  gpa rank\n1     0 380 3.61    3\n2     1 660 3.67    3\n3     1 800 4.00    1\n4     1 640 3.19    4\n5     0 520 2.93    4\n6     1 760 3.00    2\n\n\n\nSin usar predictores categóricos\n\nk_11_a &lt;- knn(\n  train = admision[,2:3], test = admision[,2:3],\n  cl = admision[,1], k = 11\n)\ncaret::confusionMatrix(k_11_a, admision[,1])\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 258  97\n         1  15  30\n                                          \n               Accuracy : 0.72            \n                 95% CI : (0.6732, 0.7635)\n    No Information Rate : 0.6825          \n    P-Value [Acc &gt; NIR] : 0.05849         \n                                          \n                  Kappa : 0.2191          \n                                          \n Mcnemar's Test P-Value : 1.952e-14       \n                                          \n            Sensitivity : 0.9451          \n            Specificity : 0.2362          \n         Pos Pred Value : 0.7268          \n         Neg Pred Value : 0.6667          \n             Prevalence : 0.6825          \n         Detection Rate : 0.6450          \n   Detection Prevalence : 0.8875          \n      Balanced Accuracy : 0.5906          \n                                          \n       'Positive' Class : 0               \n                                          \n\nk_11_b &lt;- knn(\n  train = admision[,2:4], test = admision[,2:4],\n  cl = admision[,1], k = 11\n)\ncaret::confusionMatrix(k_11_b, admision[,1])\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 257  87\n         1  16  40\n                                          \n               Accuracy : 0.7425          \n                 95% CI : (0.6967, 0.7847)\n    No Information Rate : 0.6825          \n    P-Value [Acc &gt; NIR] : 0.005172        \n                                          \n                  Kappa : 0.3014          \n                                          \n Mcnemar's Test P-Value : 5.3e-12         \n                                          \n            Sensitivity : 0.9414          \n            Specificity : 0.3150          \n         Pos Pred Value : 0.7471          \n         Neg Pred Value : 0.7143          \n             Prevalence : 0.6825          \n         Detection Rate : 0.6425          \n   Detection Prevalence : 0.8600          \n      Balanced Accuracy : 0.6282          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\n\nUsando predictores categóricos\n\n# Binarizar variables, sin remover\n# ninguna de las columnas binarizadas.\nrank_d &lt;- fastDummies::dummy_cols(\n  admision, select_columns = \"rank\"\n)\n#rank_e &lt;-model.matrix( ~ rank-1, data=admision)\nadmision_d &lt;- cbind(admision[,-4], rank_d)\n\nk_11_c &lt;- knn(\n  train = admision_d[,-1], test = admision_d[,-1],\n  cl = admision[,1], k = 11\n)\ncaret::confusionMatrix(k_11_c,admision[,1])\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 266  67\n         1   7  60\n                                          \n               Accuracy : 0.815           \n                 95% CI : (0.7734, 0.8519)\n    No Information Rate : 0.6825          \n    P-Value [Acc &gt; NIR] : 1.706e-09       \n                                          \n                  Kappa : 0.5114          \n                                          \n Mcnemar's Test P-Value : 6.953e-12       \n                                          \n            Sensitivity : 0.9744          \n            Specificity : 0.4724          \n         Pos Pred Value : 0.7988          \n         Neg Pred Value : 0.8955          \n             Prevalence : 0.6825          \n         Detection Rate : 0.6650          \n   Detection Prevalence : 0.8325          \n      Balanced Accuracy : 0.7234          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\n\nNormalización Min-Max\n\nadmision_mm &lt;- admision[,2:3]\na &lt;- 0\nb &lt;- 1\nadmision_mm$gre &lt;- \n  (admision_mm$gre - min(admision_mm$gre)) / \n  (max(admision_mm$gre) - min(admision_mm$gre)) *\n  (b - a) + a\n\nadmision_mm$gpa &lt;- \n  (admision_mm$gpa - min(admision_mm$gpa)) /\n  (max(admision_mm$gpa) - min(admision_mm$gpa)) *\n  (b - a) + a\n\nadmision_mm &lt;- cbind(admision_mm, rank_d)\nsummary(admision_mm)\n\n      gre              gpa         admit        gre             gpa       \n Min.   :0.0000   Min.   :0.0000   0:273   Min.   :220.0   Min.   :2.260  \n 1st Qu.:0.5172   1st Qu.:0.5000   1:127   1st Qu.:520.0   1st Qu.:3.130  \n Median :0.6207   Median :0.6523           Median :580.0   Median :3.395  \n Mean   :0.6340   Mean   :0.6494           Mean   :587.7   Mean   :3.390  \n 3rd Qu.:0.7586   3rd Qu.:0.8103           3rd Qu.:660.0   3rd Qu.:3.670  \n Max.   :1.0000   Max.   :1.0000           Max.   :800.0   Max.   :4.000  \n rank        rank_1           rank_2           rank_3           rank_4      \n 1: 61   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 2:151   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n 3:121   Median :0.0000   Median :0.0000   Median :0.0000   Median :0.0000  \n 4: 67   Mean   :0.1525   Mean   :0.3775   Mean   :0.3025   Mean   :0.1675  \n         3rd Qu.:0.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n         Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n\nk_11_mm &lt;- knn(\n  train = admision_mm, test = admision_mm,\n  cl = admision[,1], k = 11\n)\ncaret::confusionMatrix(k_11_mm, admision[,1])\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 266  67\n         1   7  60\n                                          \n               Accuracy : 0.815           \n                 95% CI : (0.7734, 0.8519)\n    No Information Rate : 0.6825          \n    P-Value [Acc &gt; NIR] : 1.706e-09       \n                                          \n                  Kappa : 0.5114          \n                                          \n Mcnemar's Test P-Value : 6.953e-12       \n                                          \n            Sensitivity : 0.9744          \n            Specificity : 0.4724          \n         Pos Pred Value : 0.7988          \n         Neg Pred Value : 0.8955          \n             Prevalence : 0.6825          \n         Detection Rate : 0.6650          \n   Detection Prevalence : 0.8325          \n      Balanced Accuracy : 0.7234          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\n\nNormalización SoftMax\n\nadmision_sm &lt;- admision[,2:3]\nadmision_sm &lt;- DMwR2::SoftMax(admision[,2:3], lambda = 2*pi)\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\nadmision_sm &lt;- cbind(admision_sm, rank_d)\nsummary(admision_sm)\n\n      gre               gpa          admit        gre             gpa       \n Min.   :0.03981   Min.   :0.04885   0:273   Min.   :220.0   Min.   :2.260  \n 1st Qu.:0.35754   1st Qu.:0.33561   1:127   1st Qu.:520.0   1st Qu.:3.130  \n Median :0.48334   Median :0.50335           Median :580.0   Median :3.395  \n Mean   :0.50137   Mean   :0.50218           Mean   :587.7   Mean   :3.390  \n 3rd Qu.:0.65156   3rd Qu.:0.67612           3rd Qu.:660.0   3rd Qu.:3.670  \n Max.   :0.86269   Max.   :0.83246           Max.   :800.0   Max.   :4.000  \n rank        rank_1           rank_2           rank_3           rank_4      \n 1: 61   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 2:151   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n 3:121   Median :0.0000   Median :0.0000   Median :0.0000   Median :0.0000  \n 4: 67   Mean   :0.1525   Mean   :0.3775   Mean   :0.3025   Mean   :0.1675  \n         3rd Qu.:0.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n         Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n\nk_11_sm &lt;- knn(\n  train = admision_sm, test = admision_sm,\n  cl = admision[,1], k = 11\n)\ncaret::confusionMatrix(k_11_sm,admision[,1])\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 266  67\n         1   7  60\n                                          \n               Accuracy : 0.815           \n                 95% CI : (0.7734, 0.8519)\n    No Information Rate : 0.6825          \n    P-Value [Acc &gt; NIR] : 1.706e-09       \n                                          \n                  Kappa : 0.5114          \n                                          \n Mcnemar's Test P-Value : 6.953e-12       \n                                          \n            Sensitivity : 0.9744          \n            Specificity : 0.4724          \n         Pos Pred Value : 0.7988          \n         Neg Pred Value : 0.8955          \n             Prevalence : 0.6825          \n         Detection Rate : 0.6650          \n   Detection Prevalence : 0.8325          \n      Balanced Accuracy : 0.7234          \n                                          \n       'Positive' Class : 0"
  },
  {
    "objectID": "apuntes/clase-06.html#clasificación-medidas-de-evaluación",
    "href": "apuntes/clase-06.html#clasificación-medidas-de-evaluación",
    "title": "Apuntes de clase",
    "section": "Clasificación: Medidas de Evaluación",
    "text": "Clasificación: Medidas de Evaluación\n\nPredicción\n\nLos modelos de clasificación generan dos tipos de predicciones:\n\nContinuas: Probabilidad de pertenencia a una clase.\nCategóricas: Clase predicha.\n\nKNN provee ambos tipos de predicciones; mientras que Regresión Logística solo provee la probabilidad de pertenencia a una clase.\nLas probabilidades de pertenencia a una clase pueden servir incluso como input para modelos.\n\n\n\nEvaluación de las clases predichas\n\nUn método común para tal evaluación es la matriz de confusión.\n\n\n\n\nPredichos\nEventos Observados\nNo Eventos Observados\n\n\n\n\nEventos\nTrue Positive\nFalse Positive\n\n\nNo Eventos\nFalse Negative\nTrue Negative\n\n\n\n\nMétrica más simple:\n\nRatio de la exactitud total (accuracy) \\(\\quad acc(d) = \\dfrac{TP + TN}{P + N}\\)\nRatio de error: \\(\\quad 1 - acc(d)\\)\n\nDesventajas de este par de métricas:\n\nNo realiza distinciones entre el tipo de error cometido.\nPor ejemplo, en filtros de spam, el costo de borrar un e-mail importante es mucho mayor que los costos de permitir un e-mail spam pase el filtro.\nEs importante considerar la frecuencia total de cada clase.\nCuando se tienen clases muy desbalanceadas, el accuracy no debería ser la métrica principal para evaluar el modelo de clasificación.\n\nPara regresión logística binaria, el accuracy se maximiza cuando el punto de corte \\(c\\) (para asignar clase en base a probabilidad estimada) vale 0.5.\nRatio no informativo: Ratio de exactitud que se podría alcanzar sin usar un modelo. Por ejemplo, \\(50\\%\\) en caso de tirar una moneda justa.\nEste último ratio se puede definir de varias formas:\n\nPara un conjunto de datos con \\(C\\) clases, se puede considerar \\(1/C\\).\nPuede usarse el porcentaje de la clase de mayor frecuencia, en el conjunto de entrenamiento.\nSobre el efecto severo de clases no balanceadas y posibles medidas remediales, ver Kuhn y Johnson (2013).\n\nNo necesariamente clases desbalanceadas implica dificultad de predicción. Por ejemplo, si la clase representa alguna condición extremadamente poco común (que una persona sea intersex, o algo así).   En ese contexto, (clases poco comunes pero fáciles de predecir) no se consideran los datos como desbalanceados.\n\n\nCoeficiente Kappa de Cohen\n\nEl coeficiente Kappa fue diseñado para evaluar la concordancia entre dos evaluadores/jueces.\nKappa toma en cuenta la precisión que sería generada por causas aleatorias: \\(Kappa = \\dfrac{O - E}{1 - E}\\), donde \\(O\\) es la precisión observada, y, \\(E\\), la precisión esperada.\n\\(O\\) es \\(acc(.)\\) del modelo usado.\nKappa está entre \\(-1\\) y \\(1\\).\n\n\\(Kappa \\leq 0\\) implica baja concordancia, pudiendo ser generada por haber definido erróneamente algunas categorías.\n\\(Kappa \\approx 0\\) implica no hay concordancia entre las clases observadas y las pronisticadas..\n\\(Kappa \\approx 1\\) implica perfecta concordancia entre las clases observadas y las pronisticadas.\n\n\n\n\n\n\n\nInterpretation of Cohen’s Kappa\n\n\n\n\n\nEjemplo para una matriz de confusión que vimos en un modelo de hoy:\n\n\nmc &lt;- matrix(\n  data = c(256, 84, 17, 43), nrow = 2, byrow = TRUE\n) \nmc\n\n     [,1] [,2]\n[1,]  256   84\n[2,]   17   43\n\nP &lt;- sum(mc[1, ])\nN &lt;- sum(mc[2, ])\n\nO &lt;- sum(diag(mc)) / (P + N)\nO\n\n[1] 0.7475\n\nprop_P &lt;- P / (P + N)\nprop_N &lt;- N / (P + N)\n\nE &lt;- (sum(mc[,1])*prop_P + sum(mc[,2])*prop_N) / (P + N)\nE\n\n[1] 0.62775\n\nkappa &lt;- (O - E) / (1 - E)\nkappa\n\n[1] 0.3216924\n\n\n\n\nClasificación Binaria\n\nLa sensitivity/sensibilidad/recall de un modelo es el ratio en que el evento de interés es predicho correctamente, para todas las muestras que contienen el evento:\n\nEs el ratio de verdaderos positivos.\n\\(sensibilidad(d) = \\dfrac{TP}{TP + FN}\\)\n\nEspecificidad:\n\nEs el ratio de verdaderos negativos.\n\\(Especificidad(d) = \\dfrac{TN}{FP + TN}\\)\n\nEn el caso de predecir si un correo es SPAM, considerando como éxito \\(Y = 1\\), de tratarse de SPAM, es mejor una mayor especificidad, que sensibilidad.\nFalsa alarma:\n\nEs el ratio de los falsos positivos.\n\\(Falarm(d) = 1 - Especificidad(d) = \\dfrac{FP}{FP + TN}\\)\n\nPrecisión:\n\nComparación entre los verdaderos positivos, con las instancias predichas como positivas.\n\\(Precision(d) = \\dfrac{TP}{TP + FP}\\)\n\n\\(F_{\\beta}-score\\)\n\nIndicador muy útil cuando hay datos desbalanceados, aunque aún se puede usar si no hay desbalance.\n\\(\\beta\\) se puede considerar un hiperparámetro.\nF1-score:\n\\[\\begin{gather}\nF_1 (d) = 2 \\dfrac{precision * sensibilidad}{precision + sensibilidad} \\\\\n\nF_{\\beta}(d) = \\dfrac{\\left( 1 + \\beta^2  \\right)*precision * sensibiidad}{\\left( \\beta^2 * precision + sensibilidad \\right)}\n  \\end{gather}\\]\nCuando el desbalance va a favor de la clase positiva, es útil considerar \\(\\beta = 1\\).\nMayor \\(F_1\\)-score significa que balanceas bien la sensibilidad y precisión del modelo … es un promedio de dos indicadores.\nNo hay un punto de corte exacto para el \\(F_1\\)-score. Pero tal indicador es útil para comparar modelos."
  },
  {
    "objectID": "apuntes/clase-06.html#equilibrio-entre-sensibilidad-y-especificdad",
    "href": "apuntes/clase-06.html#equilibrio-entre-sensibilidad-y-especificdad",
    "title": "Apuntes de clase",
    "section": "Equilibrio entre sensibilidad y especificdad",
    "text": "Equilibrio entre sensibilidad y especificdad\nfalta completar apuntes\n\nCurva ROC\nfalta completar apuntes"
  },
  {
    "objectID": "apuntes/clase-06.html#selección-de-medidas",
    "href": "apuntes/clase-06.html#selección-de-medidas",
    "title": "Apuntes de clase",
    "section": "Selección de Medidas",
    "text": "Selección de Medidas\n\nNo existe un único indicador que baste usar para evaluar un modelo.\nEn general, ningún clasificador es óptimo para todas las métricas de evaluación."
  },
  {
    "objectID": "apuntes/clase-07.html#ejemplo-práctico",
    "href": "apuntes/clase-07.html#ejemplo-práctico",
    "title": "Apuntes de clase",
    "section": "Ejemplo práctico",
    "text": "Ejemplo práctico\n\nlibrary(Fahrmeir)\n\nWarning: package 'Fahrmeir' was built under R version 4.1.3\n\n# Cargamos una base de datos ya pre-procesada\ndata(credit)\n# help(credit)\n\nhead(credit)\n\n     Y      Cuenta Mes             Ppag         Uso   DM   Sexo         Estc\n1 buen          no  18 pre buen pagador     privado 1049  mujer    vive solo\n2 buen          no   9 pre buen pagador profesional 2799 hombre no vive solo\n3 buen bad running  12 pre buen pagador profesional  841  mujer    vive solo\n4 buen          no  12 pre buen pagador profesional 2122 hombre no vive solo\n5 buen          no  12 pre buen pagador profesional 2171 hombre no vive solo\n6 buen          no  10 pre buen pagador profesional 2241 hombre no vive solo\n\n\n\nEstimación\n\nmodelo_logistic &lt;- glm(Y ~ ., family = binomial, data = credit)\nsummary(modelo_logistic)\n\n\nCall:\nglm(formula = Y ~ ., family = binomial, data = credit)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.0572  -0.8050  -0.4581   0.9483   2.4655  \n\nCoefficients:\n                      Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)         -1.178e+00  2.693e-01  -4.374 1.22e-05 ***\nCuentagood running  -1.952e+00  2.060e-01  -9.472  &lt; 2e-16 ***\nCuentabad running   -6.346e-01  1.764e-01  -3.598 0.000321 ***\nMes                  3.503e-02  7.849e-03   4.463 8.10e-06 ***\nPpagpre mal pagador  9.884e-01  2.529e-01   3.907 9.33e-05 ***\nUsoprofesional       4.744e-01  1.605e-01   2.956 0.003112 ** \nDM                   3.242e-05  3.335e-05   0.972 0.330893    \nSexohombre          -2.235e-01  2.208e-01  -1.012 0.311453    \nEstcvive solo        3.854e-01  2.194e-01   1.757 0.078921 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1221.7  on 999  degrees of freedom\nResidual deviance: 1017.3  on 991  degrees of freedom\nAIC: 1035.3\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\nCálculo de métricas de evaluación del modelo\n\n# Probabilidades predichas\nyprob &lt;- predict(modelo_logistic, type = 'response')\n\n# Valores predichos\nypred &lt;- as.numeric(yprob &gt;= 0.5) |&gt;\n  factor(labels = levels(credit$Y))\n\nhead(ypred)\n\n[1] buen buen buen buen buen buen\nLevels: buen mal\n\n\n\n# Matriz de confusión\nmc &lt;- table(ypred, credit$Y)\nmc\n\n      \nypred  buen mal\n  buen  636 184\n  mal    64 116\n\ntesterr &lt;- mean(ypred != credit$Y)\ntesterr\n\n[1] 0.248\n\n\n\n# Matriz de confusión, usando caret\ncaret::confusionMatrix(\n  data = ypred, reference = credit$Y, positive = \"mal\"\n)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction buen mal\n      buen  636 184\n      mal    64 116\n                                         \n               Accuracy : 0.752          \n                 95% CI : (0.724, 0.7785)\n    No Information Rate : 0.7            \n    P-Value [Acc &gt; NIR] : 0.000151       \n                                         \n                  Kappa : 0.3333         \n                                         \n Mcnemar's Test P-Value : 4.14e-14       \n                                         \n            Sensitivity : 0.3867         \n            Specificity : 0.9086         \n         Pos Pred Value : 0.6444         \n         Neg Pred Value : 0.7756         \n             Prevalence : 0.3000         \n         Detection Rate : 0.1160         \n   Detection Prevalence : 0.1800         \n      Balanced Accuracy : 0.6476         \n                                         \n       'Positive' Class : mal            \n                                         \n\ncaret::confusionMatrix(\n  data = ypred, reference = credit$Y, positive = \"mal\", \n  # Precision vs recall (incluye F1)\n  mode = \"prec_recall\"\n)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction buen mal\n      buen  636 184\n      mal    64 116\n                                         \n               Accuracy : 0.752          \n                 95% CI : (0.724, 0.7785)\n    No Information Rate : 0.7            \n    P-Value [Acc &gt; NIR] : 0.000151       \n                                         \n                  Kappa : 0.3333         \n                                         \n Mcnemar's Test P-Value : 4.14e-14       \n                                         \n              Precision : 0.6444         \n                 Recall : 0.3867         \n                     F1 : 0.4833         \n             Prevalence : 0.3000         \n         Detection Rate : 0.1160         \n   Detection Prevalence : 0.1800         \n      Balanced Accuracy : 0.6476         \n                                         \n       'Positive' Class : mal            \n                                         \n\ncaret::confusionMatrix(\n  data = ypred, reference = credit$Y, positive = \"mal\", \n  mode = \"everything\"\n)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction buen mal\n      buen  636 184\n      mal    64 116\n                                         \n               Accuracy : 0.752          \n                 95% CI : (0.724, 0.7785)\n    No Information Rate : 0.7            \n    P-Value [Acc &gt; NIR] : 0.000151       \n                                         \n                  Kappa : 0.3333         \n                                         \n Mcnemar's Test P-Value : 4.14e-14       \n                                         \n            Sensitivity : 0.3867         \n            Specificity : 0.9086         \n         Pos Pred Value : 0.6444         \n         Neg Pred Value : 0.7756         \n              Precision : 0.6444         \n                 Recall : 0.3867         \n                     F1 : 0.4833         \n             Prevalence : 0.3000         \n         Detection Rate : 0.1160         \n   Detection Prevalence : 0.1800         \n      Balanced Accuracy : 0.6476         \n                                         \n       'Positive' Class : mal            \n                                         \n\n\n\n\nLogLoss\n\nHasta ahora, no aprovechamos que no predecimos solo clases, sino también probabilidad de pertenencia a una clase.\nEn ese sentido, no hemos empleado que una predicción de clase puede estar asociada a una predicción de probabilidad muy cercana a 1.\nEsta cantidad no es interpretable, pero se usa para comparar modelos de clasificación.\n\n\nunique(as.numeric(credit$Y))\n\n[1] 1 2\n\nMLmetrics::LogLoss(\n  y_pred = yprob, y_true = as.numeric(credit$Y) - 1\n)\n\n[1] 0.5086734\n\n\n\n\nCurvas ROC\n\nlibrary(pROC)\n\nWarning: package 'pROC' was built under R version 4.1.3\n\n\nType 'citation(\"pROC\")' for a citation.\n\n\n\nAttaching package: 'pROC'\n\n\nThe following objects are masked from 'package:stats':\n\n    cov, smooth, var\n\n# Area debajo de la curva ROC\nanalysis &lt;- roc(response = credit$Y, predictor = yprob)\n\nSetting levels: control = buen, case = mal\n\n\nSetting direction: controls &lt; cases\n\nanalysis\n\n\nCall:\nroc.default(response = credit$Y, predictor = yprob)\n\nData: yprob in 700 controls (credit$Y buen) &lt; 300 cases (credit$Y mal).\nArea under the curve: 0.7753\n\nMLmetrics::AUC(y_pred = yprob, y_true = as.numeric(credit$Y) -1)\n\n[1] 0.775319\n\n\n\nCoeficiente Gini\n\n2*analysis$auc - 1\n\n[1] 0.5506381\n\nMLmetrics::Gini(y_pred = yprob, y_true = as.numeric(credit$Y) - 1) \n\n[1] 0.5506381\n\n\n\n\nGráfica de la curva ROC\n\nplot(1 - analysis$specificities, analysis$sensitivities,\n  ylab = \"Sensitividad\", xlab = \"1 - Especificidad\",\n  main = \"Curva ROC para el modelo logistico\",\n  type = \"l\", col = \"blue\", lwd = 2\n)\nabline(a = 0, b = 1, col = \"red\")\n\n\n\n\n\n\nPunto de corte\nPara encontrar el mejor equilibrio posible entre sensibilidad y especificidad (en caso nos interese balancearlos), usamos el criterio del índice J de Youden: \\(J = \\text{ sensitivity } + \\text{ specificity } - 1\\) .\nSe puede usar otro criterio, como encontrar el punto más en la curva ROC más cercano, respecto a distancia euclideana, al punto (1, 0), pues tal vértice representa 100% sensibilidad y 100% especificidad.\n\ne &lt;- cbind(\n  analysis$thresholds,\n  analysis$sensitivities + analysis$specificities - 1\n)\nhead(e)\n\n           [,1]        [,2]\n[1,]       -Inf 0.000000000\n[2,] 0.04054953 0.001428571\n[3,] 0.04142278 0.002857143\n[4,] 0.04262476 0.004285714\n[5,] 0.04298069 0.005714286\n[6,] 0.04305416 0.007142857\n\nopt_t &lt;- subset(e, e[,2] == max(e[,2]))[,1]\nopt_t\n\n[1] 0.2842726\n\n\n\nypred2 &lt;- factor(\n  as.numeric(yprob &gt;= opt_t ), labels = levels(credit$Y)\n)\ncaret::confusionMatrix(ypred2, credit$Y, positive = \"mal\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction buen mal\n      buen  463  67\n      mal   237 233\n                                          \n               Accuracy : 0.696           \n                 95% CI : (0.6664, 0.7244)\n    No Information Rate : 0.7             \n    P-Value [Acc &gt; NIR] : 0.6235          \n                                          \n                  Kappa : 0.377           \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.7767          \n            Specificity : 0.6614          \n         Pos Pred Value : 0.4957          \n         Neg Pred Value : 0.8736          \n             Prevalence : 0.3000          \n         Detection Rate : 0.2330          \n   Detection Prevalence : 0.4700          \n      Balanced Accuracy : 0.7190          \n                                          \n       'Positive' Class : mal             \n                                          \n\n\n\n# Otra forma\ncoords(analysis , \"b\", ret = \"t\", best.method = \"youden\") \n\n  threshold\n1 0.2842726\n\n\n\n\n\nOtros enlaces\n\nfmla &lt;- Y ~ Cuenta + Mes + Ppag + Uso + Estc\n\n\nprobit\n\nmodelo_probit &lt;- glm(fmla, data = credit, \n  family = binomial(link = probit)\n)\nsummary(modelo_probit)\n\n\nCall:\nglm(formula = fmla, family = binomial(link = probit), data = credit)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.9938  -0.8084  -0.4554   0.9565   2.5057  \n\nCoefficients:\n                     Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)         -0.791777   0.124560  -6.357 2.06e-10 ***\nCuentagood running  -1.126848   0.115658  -9.743  &lt; 2e-16 ***\nCuentabad running   -0.379371   0.106709  -3.555 0.000378 ***\nMes                  0.022802   0.003696   6.169 6.88e-10 ***\nPpagpre mal pagador  0.584158   0.151081   3.867 0.000110 ***\nUsoprofesional       0.276439   0.094022   2.940 0.003281 ** \nEstcvive solo        0.309828   0.093472   3.315 0.000918 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1221.7  on 999  degrees of freedom\nResidual deviance: 1019.7  on 993  degrees of freedom\nAIC: 1033.7\n\nNumber of Fisher Scoring iterations: 4\n\nyprob &lt;- predict(modelo_probit, type = \"response\")\nypred &lt;- factor(as.numeric(yprob &gt;= 0.5 ), labels = levels(credit$Y))\n\ncaret::confusionMatrix(ypred, credit$Y, positive = \"mal\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction buen mal\n      buen  641 182\n      mal    59 118\n                                          \n               Accuracy : 0.759           \n                 95% CI : (0.7313, 0.7852)\n    No Information Rate : 0.7             \n    P-Value [Acc &gt; NIR] : 1.921e-05       \n                                          \n                  Kappa : 0.3501          \n                                          \n Mcnemar's Test P-Value : 3.881e-15       \n                                          \n            Sensitivity : 0.3933          \n            Specificity : 0.9157          \n         Pos Pred Value : 0.6667          \n         Neg Pred Value : 0.7789          \n             Prevalence : 0.3000          \n         Detection Rate : 0.1180          \n   Detection Prevalence : 0.1770          \n      Balanced Accuracy : 0.6545          \n                                          \n       'Positive' Class : mal             \n                                          \n\nMLmetrics::LogLoss(yprob, as.numeric(credit$Y ) - 1)\n\n[1] 0.5098517\n\nMLmetrics::AUC(yprob, as.numeric(credit$Y) - 1)\n\n[1] 0.7718333\n\nMLmetrics::Gini(yprob, as.numeric(credit$Y) - 1)\n\n[1] 0.5384095\n\nMLmetrics::KS_Stat(yprob, as.numeric(credit$Y) - 1)\n\n[1] 42.66667\n\n\n\n\ncloglog\n\n#cloglog\nmodelo_cloglog &lt;- glm(fmla, data = credit,  \n  family = binomial(link = cloglog)\n)\nsummary(modelo_cloglog)\n\n\nCall:\nglm(formula = fmla, family = binomial(link = cloglog), data = credit)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.1526  -0.8058  -0.4693   0.9521   2.3490  \n\nCoefficients:\n                     Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)         -1.345386   0.158400  -8.494  &lt; 2e-16 ***\nCuentagood running  -1.635383   0.172965  -9.455  &lt; 2e-16 ***\nCuentabad running   -0.509948   0.130723  -3.901 9.58e-05 ***\nMes                  0.028268   0.004526   6.246 4.21e-10 ***\nPpagpre mal pagador  0.616605   0.166996   3.692 0.000222 ***\nUsoprofesional       0.330992   0.121958   2.714 0.006648 ** \nEstcvive solo        0.391084   0.121486   3.219 0.001286 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1221.7  on 999  degrees of freedom\nResidual deviance: 1022.3  on 993  degrees of freedom\nAIC: 1036.3\n\nNumber of Fisher Scoring iterations: 6\n\nyprob &lt;- predict(modelo_cloglog, type = \"response\")\nypred &lt;- factor(as.numeric(yprob &gt;= 0.5 ), labels = levels(credit$Y))\n\ncaret::confusionMatrix(ypred, credit$Y, positive = \"mal\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction buen mal\n      buen  641 185\n      mal    59 115\n                                          \n               Accuracy : 0.756           \n                 95% CI : (0.7281, 0.7823)\n    No Information Rate : 0.7             \n    P-Value [Acc &gt; NIR] : 4.795e-05       \n                                          \n                  Kappa : 0.3398          \n                                          \n Mcnemar's Test P-Value : 1.221e-15       \n                                          \n            Sensitivity : 0.3833          \n            Specificity : 0.9157          \n         Pos Pred Value : 0.6609          \n         Neg Pred Value : 0.7760          \n             Prevalence : 0.3000          \n         Detection Rate : 0.1150          \n   Detection Prevalence : 0.1740          \n      Balanced Accuracy : 0.6495          \n                                          \n       'Positive' Class : mal             \n                                          \n\nMLmetrics::LogLoss(yprob, as.numeric(credit$Y ) - 1)\n\n[1] 0.5111412\n\nMLmetrics::AUC(yprob, as.numeric(credit$Y) - 1)\n\n[1] 0.7708857\n\nMLmetrics::Gini(yprob, as.numeric(credit$Y) - 1)\n\n[1] 0.5365143\n\nMLmetrics::KS_Stat(yprob, as.numeric(credit$Y) - 1)\n\n[1] 41.47619\n\n\n\n\ncauchit\n\nmodelo_cauchit &lt;- glm(fmla, data = credit,\n  family = binomial(link = cauchit)\n)\nsummary(modelo_cauchit)\n\n\nCall:\nglm(formula = fmla, family = binomial(link = cauchit), data = credit)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.0058  -0.7719  -0.4911   0.8748   2.1980  \n\nCoefficients:\n                     Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)         -1.463846   0.242897  -6.027 1.67e-09 ***\nCuentagood running  -2.389430   0.351330  -6.801 1.04e-11 ***\nCuentabad running   -0.547335   0.171681  -3.188 0.001432 ** \nMes                  0.042563   0.007369   5.776 7.64e-09 ***\nPpagpre mal pagador  1.119789   0.276162   4.055 5.02e-05 ***\nUsoprofesional       0.512370   0.171564   2.986 0.002822 ** \nEstcvive solo        0.573785   0.171457   3.347 0.000818 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1221.7  on 999  degrees of freedom\nResidual deviance: 1020.7  on 993  degrees of freedom\nAIC: 1034.7\n\nNumber of Fisher Scoring iterations: 7\n\nyprob &lt;- predict(modelo_cauchit, type=\"response\")\nypred &lt;- factor(as.numeric(yprob &gt;= 0.5 ), labels = levels(credit$Y))\n\ncaret::confusionMatrix(ypred, credit$Y, positive = \"mal\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction buen mal\n      buen  630 167\n      mal    70 133\n                                          \n               Accuracy : 0.763           \n                 95% CI : (0.7354, 0.7891)\n    No Information Rate : 0.7             \n    P-Value [Acc &gt; NIR] : 5.274e-06       \n                                          \n                  Kappa : 0.3783          \n                                          \n Mcnemar's Test P-Value : 4.493e-10       \n                                          \n            Sensitivity : 0.4433          \n            Specificity : 0.9000          \n         Pos Pred Value : 0.6552          \n         Neg Pred Value : 0.7905          \n             Prevalence : 0.3000          \n         Detection Rate : 0.1330          \n   Detection Prevalence : 0.2030          \n      Balanced Accuracy : 0.6717          \n                                          \n       'Positive' Class : mal             \n                                          \n\nMLmetrics::LogLoss(yprob, as.numeric(credit$Y ) - 1)\n\n[1] 0.5103524\n\nMLmetrics::AUC(yprob, as.numeric(credit$Y) - 1)\n\n[1] 0.7720143\n\nMLmetrics::Gini(yprob, as.numeric(credit$Y) - 1)\n\n[1] 0.5387714\n\nMLmetrics::KS_Stat(yprob, as.numeric(credit$Y) - 1)\n\n[1] 40.85714"
  },
  {
    "objectID": "apuntes/clase-07.html#remuestreo",
    "href": "apuntes/clase-07.html#remuestreo",
    "title": "Apuntes de clase",
    "section": "Remuestreo",
    "text": "Remuestreo\n\n¿Qué aprenderemos?\n\n¿Cómo se evalúa y selecciona un modelo predictivo?\nSolución ideal en una situación de abundancia de datos.\nValidación cruzada (CV):\n\nConjunto de validación\nLOOCV\nk-fold CV\n\nBootstrapping\n\n\n\nEficiencia de un método de aprendizaje\n\nModelo bueno cuando se puede generalizar.\nQueremos un método de aprendizaje que funcione bien con datos nuevos (error de prueba bajo).\nCada modelo tiene una métrica poblacional (accuracy, especificidad, etc), que no depende de los datos de entrenamiento ni test.\nEsto es importante para:\n\nSelección de modelos: Estimar el rendimiento predictivo de diferentes modelos para elegir el mejor.\nEvaluación de modelos: Estimar su rendimiento (error de predicción) del modelo final sobre un nuevo conjunto de datos.\n\n\n\n\nError de entrenamiento vs Error de prueba\n\nEl error de prueba puede subestimar drásticamente el error de prueba.\n\n\n\nFunciones de pérdida\nSe suele usar MSE (error cuadrático medio) y el ratio de mala clasificación.\n\n\nEl reto\nEn los ejemplos anteriores, sabíamos la verdad, por lo que pudimos evaluar el error de entrenamiento y prueba.\nEn la realidad, tal no suele ser el caso.\n\n\nSituación con abundancia de datos\n\nEs una situación ideal, usualmente no realista.\nConsiste en que se cuenta con una muestra tan representativa de la población, que, al dividirla en tres partes, cada una de ellas es también representativa de la población.\nPartición:\n\nConjunto de entrenamiento (training set): Para ajustar (estimar) el modelo .\nConjunto de validación (validation set): Para seleccionar el mejor modelo .\nConjunto de evaluación o prueba (test set): Para evaluar qué tan bien el modelo se ajusta un conjunto nuevo e independiente de datos.\n\nComo esta situación no suele ocurrir, se hace uso eficiente de la muestra aplicando técnicas de remuestro de datos.\nUna estrategia alternativa para seleccionar el modelo* es usando métodos de penalización por complejidad, como Lasso.o AIC.\n\n\nProposition 1 Tras realizar el ajuste de modelo, selección de un único modelo y evaluación de aquel modelo; el modelo se vuelve a estimar usando ahora aquellos tres conjuntos de datos juntos, modelo que sería el que se aplica en producción.\n\n\n\nValidación cruzada (CV)\n\nSituación de selección de modelos, no evaluación.\nNo es necesariamente un muestro simple, puede ser muestreo estratificado, entre otros.\nAlgunas técnicas:\n\nLOOOCV: Validación cruzada dejando uno afuera.\nK-fold Cross Validation: Validación cruzada con \\(K\\) iteraciones; usualmente en 5 o 10 grupos.\n\n\n\nEnfoque usando un conjunto de validación\n\nSuponga que cuenta con un conjunto de datos; y, a parte, un conjunto de evaluación."
  },
  {
    "objectID": "apuntes/clase-08.html#remuestreo",
    "href": "apuntes/clase-08.html#remuestreo",
    "title": "Apuntes de clase",
    "section": "Remuestreo",
    "text": "Remuestreo\n\nValidación Cruzada (CV)\n\nProposition 1 CV no estima modelos; estima métricas, las cuales usamos para comparar modelos.\nCV nos provee de una estimación del valor esperado de una métrica (MSE, Accuracy, etc) asociada a un modelo de Aprendizaje Estadístico.\n\n\nImplica selección, no evaluación, del modelo.\nEs influenciada por la métrica fijada para comparar datos (sea accuracy, etc).\n\n\n\nEnfoque para validation set\n\nDividimos un conjunto de \\(n\\) observaciones en dos partes:\n\nConjunto de entrenaminto (para ajustar modelo)\nConjunto de validación (para predecir la response de cada observación del validation set)"
  },
  {
    "objectID": "apuntes/clase-08.html#método-de-retención",
    "href": "apuntes/clase-08.html#método-de-retención",
    "title": "Apuntes de clase",
    "section": "Método de retención",
    "text": "Método de retención\n\nEjemplo práctico\n\nProposition 2 El método de retención tiene alto sesgo (pues no se considera TODO el conjunto de datos) y alta variabilidad (debido a la aleatoriedad).\n\n\nlibrary(ISLR2)\n\nWarning: package 'ISLR2' was built under R version 4.1.3\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\nlibrary(ggpubr)\n\nWarning: package 'ggpubr' was built under R version 4.1.3\n\nlibrary(reshape2)\n\nWarning: package 'reshape2' was built under R version 4.1.3\n\n\n\nset.seed(123)\nn = dim(Auto)[1]\n\n# 11 veces partiremos los datos en train y test\n# 10 grados de polinomios consideraremos\ntestMSEmat &lt;- matrix(ncol = 11, nrow = 10)\n\nfor (newsample in 1:11) {\n  trainid &lt;- sample(n, n/2)\n  for (polydeg in 1:10) {\n    lm.fit &lt;- lm(\n      mpg ~ poly(horsepower, polydeg),\n      data = Auto,\n      subset = trainid\n    )\n    # Predecimos para los datos de test\n    lm.pred &lt;- predict(lm.fit, Auto)[-trainid]\n    testMSEmat[polydeg, newsample] &lt;- mean((Auto$mpg[-trainid] - lm.pred)^2)\n  }\n}\nyrange &lt;- c(15,28)\n\n# Resultado para la primera selección\nplotdf &lt;- data.frame(\"testMSE\" = testMSEmat[,1], \"degree\" = 1:10)\n\n\ng0 &lt;- ggplot(plotdf, aes(x = degree)) +\n  geom_line(y = testMSEmat[,1]) +\n  scale_y_continuous(limits = yrange) +\n  scale_x_continuous(breaks = 1:10) +\n  labs(\n    y = \"MSE de conjunto de validación\", \n    x = \"Grado del Polinomio\"\n  )\n\ng0 + theme_minimal()\n\n\n\n# Comparando con las otras muestras\ncols=rainbow(10)\ng1=g0+geom_line(aes(x=1:10,y=testMSEmat[,2]),colour=cols[1])\n\nfor (col in 3:11) {\n  g1 &lt;- g1 + \n    geom_line(\n      aes(x = 1:10, y = testMSEmat[,col]), colour = cols[col - 1]\n    )\n}\n\ng1valid &lt;- g1\ng1valid"
  },
  {
    "objectID": "apuntes/clase-08.html#loocv",
    "href": "apuntes/clase-08.html#loocv",
    "title": "Apuntes de clase",
    "section": "LOOCV",
    "text": "LOOCV\n\nProduce una estimación más confiables comparado al método de retención.\n\n\nProposition 3 Poco sesgo, pues se usa casi todo el conjunto de datos para entrenamiento.\nEn este caso, se presenta alta varianza, pues la covarianza entre dos training sets (de los \\(n\\) creados) es muy alta, pues ser la misma muestra, salvo a lo más dos observaciones.\n\n\n\\(MSE_i = \\left( y_i - \\hat{y_i} \\right)^2\\)\nError total de predicción: \\(CV_n = \\dfrac{1}{n} \\displaystyle{ \\sum_{i=1}^{n}MSE_i }\\)\nFunciona bastante bien para regresión lineal.\n\n\nEjemplo práctico\n\nset.seed(123)\nn = dim(Auto)[1]\n\ntestMSEvec &lt;- NULL\nstart &lt;- Sys.time()\n\nfor (polydeg in 1:10) {\n  glm.fit = glm(mpg ~ poly(horsepower,polydeg), data = Auto)\n  glm.cv1 = boot::cv.glm(Auto, glm.fit, K = n)\n  # Error cuadrático medio\n  testMSEvec = c(testMSEvec, glm.cv1$delta[1])\n}\nstop = Sys.time()\n\nyrange = c(15, 28)\nplotdf = data.frame(testMSE = testMSEvec, degree = 1:10)\n\ng0 = ggplot(plotdf, aes(x = degree, y = testMSE)) +\n  geom_line() +\n  geom_point() +\n  scale_y_continuous(limits = yrange) +\n  scale_x_continuous(breaks = 1:10) +\n  labs(y = \"LOOCV\")\n\ng0 + theme_minimal()\n\n\n\n\n\nstop - start\n\nTime difference of 22.83591 secs"
  },
  {
    "objectID": "apuntes/clase-08.html#k-fold-cv",
    "href": "apuntes/clase-08.html#k-fold-cv",
    "title": "Apuntes de clase",
    "section": "k-fold CV",
    "text": "k-fold CV\n\nProcedimiento:\n\nDividir los datos en \\(k\\) partes más o menos iguales.\nUtilizar \\(k-1\\) partes para entrenar la \\(k\\)-ésima parte para validar.\nHacer esto \\(k\\) veces, omitiendo otra parte en cada \\(k\\) ronda.\n\n\\(CV_k = \\displaystyle{ \\dfrac{1}{k}\\sum_{i=1}^{k} MSE_i}\\)\n\\(CV_k = \\displaystyle{ \\dfrac{1}{n}\\sum_{i=1}^{k} n_i * MSE_i}\\)\nNote que el caso \\(k=n\\) consiste es LOOCV.\n\n\nProposition 4 Presenta cierta variabilidad, debido a la aleatoriedad; y puede ser alta cuando \\(k=n\\) (caso determinista) incluso.\nEl sesgo es menor cuando \\(k=n\\), pero suele usarse \\(k=5\\) o \\(k=10\\) .\n\n\nEjemplo práctico\n\nset.seed(123)\n\ntestMSEvec5 = NULL\ntestMSEvec10 = NULL\nstart = Sys.time()\n\nfor (polydeg in 1:10) {\n  glm.fit = glm(mpg ~ poly(horsepower, polydeg), data = Auto)\n  glm.cv5 = boot::cv.glm(Auto, glm.fit, K = 5)\n  glm.cv10 = boot::cv.glm(Auto, glm.fit, K = 10)\n  testMSEvec5 = c(testMSEvec5, glm.cv5$delta[1])\n  testMSEvec10 = c(testMSEvec10, glm.cv10$delta[1])\n}\n\nstop = Sys.time()\nyrange = c(15,28)\n\nplotdf = data.frame(testMSE5  =testMSEvec5, degree = 1:10)\n\ng0 = ggplot(plotdf, aes(x = degree, y = testMSE5))  +\n  geom_line() +\n  geom_point() +\n  scale_y_continuous(limits = yrange) +\n  scale_x_continuous(breaks = 1:10) +\n  labs(y = \"CV\") +\n  ggtitle(\"5 and 10 fold CV\")\n\ng0 +\n  geom_line(aes(y = testMSEvec10), colour = \"red\") +\n  geom_point(aes(y = testMSEvec10), colour = \"red\") +\n  ggtitle(\"5 fold (black), 10 fold (red)\") +\n  theme_minimal()\n\n\n\n\n\nstop - start\n\nTime difference of 1.733926 secs"
  },
  {
    "objectID": "apuntes/clase-08.html#elección-del-mejor-modelo",
    "href": "apuntes/clase-08.html#elección-del-mejor-modelo",
    "title": "Apuntes de clase",
    "section": "Elección del mejor modelo",
    "text": "Elección del mejor modelo\n\nRegla del error estándar"
  },
  {
    "objectID": "apuntes/clase-08.html#comentarios-finales",
    "href": "apuntes/clase-08.html#comentarios-finales",
    "title": "Apuntes de clase",
    "section": "Comentarios finales",
    "text": "Comentarios finales\n\nEs posible usar \\(CV\\) para la evaluación de modelos, tratando a conjuntos de validación como conjuntos de prueba."
  },
  {
    "objectID": "apuntes/clase-08.html#bootstrap",
    "href": "apuntes/clase-08.html#bootstrap",
    "title": "Apuntes de clase",
    "section": "Bootstrap",
    "text": "Bootstrap\n\nEsta técnica cuantifica la incertidumbre asociada a un estimador o método de aprendizaje estadístico.\nSirve para reducir variabilidad."
  },
  {
    "objectID": "apuntes/clase-08.html#ejemplo-práctico-clasificación-de-buenos-pagaderos",
    "href": "apuntes/clase-08.html#ejemplo-práctico-clasificación-de-buenos-pagaderos",
    "title": "Apuntes de clase",
    "section": "Ejemplo práctico: Clasificación de Buenos Pagaderos",
    "text": "Ejemplo práctico: Clasificación de Buenos Pagaderos\nUsaremos validación cruzada para evaluar, no seleccionar modelos.\n\nlibrary(Fahrmeir)\n\nWarning: package 'Fahrmeir' was built under R version 4.1.3\n\ndata(credit)\n# help(credit)\n\nhead(credit)\n\n     Y      Cuenta Mes             Ppag         Uso   DM   Sexo         Estc\n1 buen          no  18 pre buen pagador     privado 1049  mujer    vive solo\n2 buen          no   9 pre buen pagador profesional 2799 hombre no vive solo\n3 buen bad running  12 pre buen pagador profesional  841  mujer    vive solo\n4 buen          no  12 pre buen pagador profesional 2122 hombre no vive solo\n5 buen          no  12 pre buen pagador profesional 2171 hombre no vive solo\n6 buen          no  10 pre buen pagador profesional 2241 hombre no vive solo\n\n\n\nset.seed(1234)\n\n# Número de folds\nk &lt;- 5\n\nsplitPlan &lt;- vtreat::kWayCrossValidation(nRows = dim(credit)[1], k)\nsplitPlan\n\n[[1]]\n[[1]]$train\n  [1]    2    3    4    5    6    8    9   11   12   13   14   15   16   17   18\n [16]   20   21   22   23   24   26   27   28   29   30   32   33   35   36   37\n [31]   38   39   41   43   44   45   46   47   48   49   50   51   52   53   54\n [46]   55   56   57   58   60   62   63   64   65   66   68   69   70   71   73\n [61]   74   75   76   77   78   79   80   81   82   83   84   85   89   90   91\n [76]   92   93   94   95   97   98   99  100  101  102  103  105  107  108  109\n [91]  110  111  112  115  117  118  119  120  122  123  124  125  126  127  128\n[106]  129  130  131  132  133  134  137  138  139  140  141  142  143  144  145\n[121]  146  147  148  149  150  151  152  154  155  156  158  159  160  161  162\n[136]  163  164  165  168  170  171  172  173  174  175  176  177  178  179  180\n[151]  182  183  184  186  187  188  190  192  194  195  198  200  201  202  203\n[166]  204  205  206  207  208  209  210  211  212  213  214  215  216  217  220\n[181]  221  222  223  224  225  227  228  229  231  232  233  235  236  237  238\n[196]  239  240  241  244  245  246  247  248  249  250  252  254  255  256  257\n[211]  258  259  260  262  263  264  267  268  269  270  271  272  276  277  278\n[226]  279  280  282  283  284  285  286  287  288  290  292  293  295  296  298\n[241]  299  300  302  303  304  306  307  308  309  310  311  313  314  317  318\n[256]  319  320  321  322  323  324  325  327  328  330  332  333  335  337  338\n[271]  339  340  341  342  343  344  345  346  347  349  351  352  353  355  356\n[286]  358  359  360  361  362  363  364  365  366  368  369  370  371  372  373\n[301]  374  375  376  377  378  379  381  383  386  387  389  391  392  394  395\n[316]  396  397  398  399  400  401  403  405  406  407  408  409  410  412  413\n[331]  414  416  417  418  419  420  421  423  424  426  427  429  430  431  432\n[346]  433  434  435  436  438  439  440  441  442  444  445  446  447  448  449\n[361]  451  452  453  455  457  458  459  460  461  462  464  468  469  470  471\n[376]  475  476  478  479  480  482  484  485  487  488  489  490  491  494  495\n[391]  496  497  498  499  501  502  503  504  505  506  507  508  509  510  513\n[406]  514  516  517  518  519  520  521  523  525  527  530  531  532  533  534\n[421]  535  536  540  541  542  544  545  546  547  548  549  550  551  552  553\n[436]  554  555  556  557  558  560  561  562  563  564  565  566  567  568  569\n[451]  570  571  572  573  574  575  576  577  579  581  582  584  585  586  588\n[466]  589  590  591  593  595  596  597  600  601  602  603  604  605  606  608\n[481]  610  611  612  613  615  617  618  619  620  621  624  625  626  628  629\n[496]  630  632  633  634  635  637  638  639  640  641  642  644  645  646  647\n[511]  648  649  650  652  653  655  656  657  658  659  660  662  663  664  665\n[526]  666  667  668  670  671  672  673  674  675  676  677  678  679  680  682\n[541]  684  685  686  687  688  689  690  691  693  694  695  696  697  698  700\n[556]  701  702  703  705  709  712  713  714  715  716  717  718  719  720  721\n[571]  722  723  724  726  727  728  729  730  731  733  734  735  736  737  738\n[586]  740  741  742  743  744  745  746  747  748  749  750  752  753  755  756\n[601]  757  758  759  760  761  762  763  765  766  767  769  770  771  772  773\n[616]  775  776  777  778  779  780  782  783  785  786  787  789  790  791  792\n[631]  793  794  795  796  798  799  801  803  804  805  806  807  808  809  810\n[646]  812  813  815  818  819  820  821  822  823  824  825  826  827  828  829\n[661]  830  832  834  835  837  838  839  840  841  842  843  844  845  846  847\n[676]  848  849  850  852  853  854  855  856  857  858  859  860  861  862  864\n[691]  865  866  867  868  869  870  871  873  874  875  876  877  878  879  880\n[706]  881  883  884  885  886  887  888  889  890  891  893  894  896  897  898\n[721]  899  901  902  903  904  905  907  908  910  911  912  913  914  915  916\n[736]  917  918  919  920  921  922  923  925  926  927  928  929  930  931  932\n[751]  933  934  935  938  939  940  941  943  944  945  947  948  950  951  952\n[766]  953  954  956  957  959  960  962  964  966  967  969  970  971  972  973\n[781]  974  975  977  978  979  980  983  984  985  986  987  989  990  991  992\n[796]  993  994  998  999 1000\n\n[[1]]$app\n  [1] 623 900 326 382 661 511 578 816 627 811 305 946 136 234 297 522 643 218\n [19] 169 895 961  19 116 390 529 181 474 265  40 450 909 598 995 538 833 708\n [37] 281  10 592 707 350 543 189 291 456 774  59 243 768 607 104  88 294 512\n [55] 486 428 599 166 587 251 463 692  86 681 968 380 654 882 988 357  61  87\n [73] 526 334 367 274 191  67  34 711  31 580 981 651 937 199 454   1 348  25\n [91] 135 631 836 616 814 892 936 477 924 289 942 331 354 963 493 872 329 537\n[109] 121 500 958 996 609 443 422 437 336 955 704 226 481 275 385  42 788 669\n[127] 253 732 472 965   7 725 106 739 754 402 982 153 411 425 114 404 751 483\n[145] 976 851 185 113 781 906 817 764 219 949 524 802 683  72 301 261 388 515\n[163] 710 415 583 528 316 797 157 614 315 784  96 559 622 196 473 467 393 539\n[181] 312 167 863 465 466 800 831 266 492 699 706 384 197 997 242 594 636 273\n[199] 230 193\n\n\n[[2]]\n[[2]]$train\n  [1]    1    2    3    4    5    6    7    8    9   10   11   12   13   14   15\n [16]   16   17   18   19   21   24   25   26   27   28   30   31   32   34   35\n [31]   37   38   39   40   41   42   43   46   47   48   49   50   52   53   54\n [46]   55   56   57   59   61   62   63   64   66   67   68   69   70   72   73\n [61]   74   75   76   77   78   81   82   83   85   86   87   88   89   90   92\n [76]   93   96  101  102  103  104  105  106  107  110  111  113  114  115  116\n [91]  117  119  120  121  122  123  124  125  127  128  129  130  131  134  135\n[106]  136  137  139  141  143  145  146  147  148  149  150  151  152  153  154\n[121]  155  156  157  158  159  160  161  162  163  164  166  167  168  169  170\n[136]  172  173  174  175  176  178  179  180  181  182  185  187  188  189  190\n[151]  191  192  193  194  195  196  197  198  199  200  202  203  205  206  209\n[166]  211  212  213  214  215  216  218  219  220  225  226  227  230  232  233\n[181]  234  235  236  238  239  242  243  247  248  249  250  251  253  255  256\n[196]  257  259  260  261  262  263  265  266  267  269  270  271  272  273  274\n[211]  275  276  277  278  279  281  282  283  285  286  287  288  289  290  291\n[226]  292  293  294  295  297  298  299  300  301  302  304  305  307  308  309\n[241]  310  312  313  314  315  316  317  319  322  323  324  325  326  327  328\n[256]  329  331  332  333  334  335  336  338  339  342  343  344  345  346  347\n[271]  348  350  351  352  353  354  355  356  357  359  362  363  364  365  367\n[286]  368  371  373  374  375  376  377  378  379  380  381  382  383  384  385\n[301]  386  388  389  390  391  392  393  395  396  397  398  399  400  401  402\n[316]  403  404  405  406  407  408  409  410  411  412  415  417  419  422  423\n[331]  424  425  426  428  429  430  431  432  434  435  437  439  440  441  442\n[346]  443  444  445  447  448  449  450  451  452  453  454  455  456  459  460\n[361]  461  463  464  465  466  467  468  469  470  471  472  473  474  475  476\n[376]  477  480  481  482  483  484  486  488  489  490  491  492  493  495  497\n[391]  499  500  506  508  509  511  512  514  515  519  520  521  522  524  525\n[406]  526  527  528  529  530  531  532  533  534  535  536  537  538  539  540\n[421]  542  543  544  545  546  548  550  551  553  554  555  556  557  558  559\n[436]  561  562  564  565  567  568  569  570  571  573  574  575  576  577  578\n[451]  579  580  581  582  583  584  586  587  588  589  590  591  592  594  596\n[466]  598  599  600  601  602  603  604  605  607  609  610  611  612  613  614\n[481]  616  618  619  620  621  622  623  624  626  627  628  629  630  631  632\n[496]  633  634  635  636  637  638  640  641  642  643  644  645  646  647  648\n[511]  650  651  652  654  655  656  657  658  659  660  661  662  663  664  665\n[526]  667  669  670  671  672  673  677  678  680  681  683  684  685  687  688\n[541]  689  690  692  693  694  695  696  697  698  699  700  701  702  703  704\n[556]  705  706  707  708  709  710  711  712  713  714  715  716  717  718  719\n[571]  721  722  723  724  725  726  728  729  731  732  733  735  736  737  738\n[586]  739  741  742  743  744  745  746  747  748  749  751  752  753  754  755\n[601]  756  757  758  759  760  762  763  764  765  766  767  768  769  770  771\n[616]  772  773  774  775  776  777  778  781  783  784  786  787  788  789  790\n[631]  792  795  796  797  798  799  800  801  802  803  805  807  808  809  810\n[646]  811  813  814  815  816  817  818  819  822  824  825  826  827  828  831\n[661]  832  833  834  835  836  837  838  840  841  842  843  844  845  846  848\n[676]  849  850  851  852  853  854  855  856  858  860  861  862  863  864  865\n[691]  866  868  870  872  873  875  876  877  878  879  880  881  882  884  885\n[706]  886  887  888  891  892  894  895  896  897  898  899  900  901  902  903\n[721]  904  906  907  908  909  910  911  912  915  918  919  922  923  924  926\n[736]  927  928  929  930  931  933  934  935  936  937  938  939  940  941  942\n[751]  943  944  946  947  948  949  950  951  952  953  954  955  956  958  959\n[766]  960  961  962  963  964  965  966  967  968  969  970  971  973  974  975\n[781]  976  978  980  981  982  983  984  985  986  987  988  989  990  991  992\n[796]  995  996  997  999 1000\n\n[[2]]$app\n  [1] 284 905  98  79 184 552 479 510 108 740 258 358 221 794 608 208 998 595\n [19] 727  71 485  60 649 993 597 126 779 228 427 541 806 478 142 457 857 296\n [37]  84 675 830 585 303 823 518 171  58  22 804 916 413 921 501 210 945 421\n [55] 361 254 793 920 207 280 231 100  36 138 682 750  97 782 183 118 252 566\n [73] 224 957  95 420 883 791 679 504 821 547 925 349 859 487 165 372 241 201\n [91] 341 563 917 237 893 321 462  51 222 502 730 867 204  29 829 132 340 229\n[109] 223  23 625 523 133 914 366 112  91 593 306 446 311 839 686 264 245  94\n[127] 606 177 676 761 668 268 516 890 394 240 549 869 653 785 615 244 418 436\n[145] 871 414 517 720 780 217  65 932 874 513  33 617 140 913 494 330 503 572\n[163] 433 370 318 337 186 109 387 820 144  45  80 977 972 458 360 812 496 369\n[181] 246 320 847 438 639 691 734  99  20  44 674 507 994 416 560 505 889 498\n[199] 666 979\n\n\n[[3]]\n[[3]]$train\n  [1]    1    3    4    5    6    7    8    9   10   11   12   13   14   15   16\n [16]   17   18   19   20   21   22   23   25   27   28   29   30   31   32   33\n [31]   34   36   37   38   40   41   42   43   44   45   47   50   51   52   53\n [46]   55   56   57   58   59   60   61   62   63   65   66   67   68   69   70\n [61]   71   72   74   75   76   77   78   79   80   84   85   86   87   88   91\n [76]   92   93   94   95   96   97   98   99  100  101  104  105  106  107  108\n [91]  109  112  113  114  115  116  117  118  119  121  122  123  124  125  126\n[106]  127  129  132  133  134  135  136  137  138  139  140  141  142  143  144\n[121]  146  147  148  149  150  151  152  153  155  156  157  159  160  161  162\n[136]  163  164  165  166  167  169  171  172  173  175  176  177  178  180  181\n[151]  182  183  184  185  186  187  189  190  191  192  193  194  195  196  197\n[166]  199  201  203  204  205  206  207  208  210  211  213  214  216  217  218\n[181]  219  221  222  223  224  225  226  227  228  229  230  231  232  233  234\n[196]  235  236  237  238  239  240  241  242  243  244  245  246  247  249  250\n[211]  251  252  253  254  255  256  257  258  260  261  263  264  265  266  267\n[226]  268  269  270  272  273  274  275  276  277  280  281  282  283  284  286\n[241]  287  288  289  291  293  294  295  296  297  298  299  301  303  304  305\n[256]  306  307  308  309  311  312  313  315  316  317  318  319  320  321  322\n[271]  323  324  325  326  329  330  331  332  333  334  335  336  337  338  340\n[286]  341  342  343  345  346  348  349  350  353  354  355  356  357  358  360\n[301]  361  362  363  365  366  367  369  370  371  372  374  375  376  379  380\n[316]  381  382  383  384  385  386  387  388  389  390  391  392  393  394  397\n[331]  398  399  400  402  404  406  409  410  411  413  414  415  416  418  419\n[346]  420  421  422  423  424  425  427  428  430  431  432  433  436  437  438\n[361]  439  442  443  445  446  448  450  453  454  456  457  458  459  460  461\n[376]  462  463  464  465  466  467  469  472  473  474  475  476  477  478  479\n[391]  480  481  482  483  485  486  487  488  492  493  494  496  498  499  500\n[406]  501  502  503  504  505  507  508  509  510  511  512  513  515  516  517\n[421]  518  519  520  521  522  523  524  525  526  528  529  530  532  533  534\n[436]  535  536  537  538  539  540  541  542  543  545  547  548  549  550  551\n[451]  552  553  554  555  557  558  559  560  561  563  566  568  569  570  571\n[466]  572  575  576  578  580  581  583  584  585  586  587  588  589  590  591\n[481]  592  593  594  595  597  598  599  602  603  604  606  607  608  609  612\n[496]  613  614  615  616  617  618  620  621  622  623  624  625  627  630  631\n[511]  633  634  636  637  639  641  642  643  646  647  648  649  650  651  652\n[526]  653  654  655  657  659  661  662  664  665  666  668  669  670  671  672\n[541]  673  674  675  676  677  678  679  681  682  683  685  686  690  691  692\n[556]  693  697  699  700  701  702  703  704  705  706  707  708  710  711  713\n[571]  714  716  717  718  719  720  721  723  724  725  726  727  728  730  732\n[586]  733  734  735  737  738  739  740  741  742  744  745  746  747  749  750\n[601]  751  752  754  756  757  758  760  761  763  764  767  768  769  770  771\n[616]  773  774  776  777  779  780  781  782  783  784  785  786  787  788  791\n[631]  793  794  796  797  798  799  800  801  802  803  804  805  806  809  810\n[646]  811  812  813  814  816  817  818  819  820  821  822  823  824  825  826\n[661]  827  829  830  831  832  833  835  836  837  839  843  844  846  847  849\n[676]  850  851  852  854  855  856  857  858  859  860  861  863  864  865  866\n[691]  867  868  869  871  872  873  874  875  876  878  881  882  883  884  885\n[706]  887  888  889  890  892  893  894  895  896  897  898  900  901  902  903\n[721]  904  905  906  907  909  910  911  912  913  914  916  917  918  919  920\n[736]  921  922  924  925  926  927  928  929  930  931  932  933  934  936  937\n[751]  938  939  940  942  943  945  946  947  948  949  950  951  952  953  954\n[766]  955  957  958  960  961  962  963  965  968  969  971  972  973  976  977\n[781]  978  979  980  981  982  983  985  986  988  990  991  992  993  994  995\n[796]  996  997  998  999 1000\n\n[[3]]$app\n  [1] 848 645 103 974 574 212 605 687 131 840 629 696 736 145 495 959 248 434\n [19] 838 573 667 449 638 102 880 262 828 789 891 497 600 403 444 188 215 328\n [37]  83 964 441 660 841 658 775  39 395 170 417 484 577 405 429 471 601 715\n [55] 886 491 956 378 468 440 564 834 626 344 899 611  49 174 347 271 712 179\n [73] 842 795 755  54 168 935 970 562 531 908 808 845 944 120 198 285 923 879\n [91] 514 565 640 967 632 579 128 689 352 596 546 762 396 377 279 680 792 915\n[109] 656 862 407 941   2 292 412 489 259  35 688 452 619 278 447 110 765 989\n[127] 373 364 772 853  48 339 684 644 753 815 567 610  82 975  64  24 635 987\n[145]  90 426 290 807 709 368 401 698 984 877 748 455 209 202 556 300 694 200\n[163] 731 506 302 154 544 327 722 870 527 582 130  81 111 729 451 490 743 966\n[181] 435 470 663 790 158 766  73  46 408 759  26 778  89 628 314 359 351 310\n[199] 695 220\n\n\n[[4]]\n[[4]]$train\n  [1]   1   2   5   6   7   8   9  10  13  14  15  16  19  20  21  22  23  24\n [19]  25  26  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44\n [37]  45  46  47  48  49  50  51  53  54  55  58  59  60  61  62  63  64  65\n [55]  66  67  68  70  71  72  73  75  76  77  79  80  81  82  83  84  85  86\n [73]  87  88  89  90  91  92  94  95  96  97  98  99 100 101 102 103 104 106\n [91] 108 109 110 111 112 113 114 116 117 118 120 121 122 126 127 128 130 131\n[109] 132 133 134 135 136 138 140 142 143 144 145 146 148 149 151 152 153 154\n[127] 155 156 157 158 159 163 165 166 167 168 169 170 171 174 177 178 179 181\n[145] 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199\n[163] 200 201 202 204 207 208 209 210 212 215 217 218 219 220 221 222 223 224\n[181] 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242\n[199] 243 244 245 246 247 248 251 252 253 254 257 258 259 260 261 262 264 265\n[217] 266 268 270 271 273 274 275 276 278 279 280 281 283 284 285 287 289 290\n[235] 291 292 293 294 295 296 297 298 299 300 301 302 303 305 306 308 310 311\n[253] 312 313 314 315 316 318 319 320 321 322 323 324 325 326 327 328 329 330\n[271] 331 332 333 334 335 336 337 338 339 340 341 344 345 347 348 349 350 351\n[289] 352 354 356 357 358 359 360 361 362 363 364 366 367 368 369 370 371 372\n[307] 373 377 378 379 380 381 382 384 385 386 387 388 390 391 393 394 395 396\n[325] 398 399 400 401 402 403 404 405 407 408 410 411 412 413 414 415 416 417\n[343] 418 420 421 422 423 425 426 427 428 429 430 433 434 435 436 437 438 439\n[361] 440 441 443 444 445 446 447 449 450 451 452 453 454 455 456 457 458 459\n[379] 461 462 463 464 465 466 467 468 470 471 472 473 474 475 476 477 478 479\n[397] 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498\n[415] 499 500 501 502 503 504 505 506 507 510 511 512 513 514 515 516 517 518\n[433] 520 521 522 523 524 526 527 528 529 530 531 532 536 537 538 539 541 543\n[451] 544 545 546 547 549 552 554 555 556 557 558 559 560 562 563 564 565 566\n[469] 567 569 572 573 574 577 578 579 580 581 582 583 585 586 587 589 590 591\n[487] 592 593 594 595 596 597 598 599 600 601 602 605 606 607 608 609 610 611\n[505] 612 613 614 615 616 617 619 622 623 624 625 626 627 628 629 630 631 632\n[523] 635 636 638 639 640 641 642 643 644 645 646 648 649 651 653 654 656 657\n[541] 658 659 660 661 663 664 665 666 667 668 669 674 675 676 679 680 681 682\n[559] 683 684 686 687 688 689 690 691 692 694 695 696 698 699 701 704 705 706\n[577] 707 708 709 710 711 712 713 715 716 717 718 720 722 723 725 727 729 730\n[595] 731 732 733 734 736 737 738 739 740 741 742 743 746 747 748 749 750 751\n[613] 752 753 754 755 758 759 760 761 762 764 765 766 768 772 773 774 775 776\n[631] 777 778 779 780 781 782 784 785 787 788 789 790 791 792 793 794 795 797\n[649] 798 800 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817\n[667] 818 820 821 822 823 826 827 828 829 830 831 833 834 835 836 837 838 839\n[685] 840 841 842 845 847 848 849 850 851 852 853 857 858 859 861 862 863 866\n[703] 867 869 870 871 872 874 875 876 877 879 880 881 882 883 886 887 889 890\n[721] 891 892 893 895 897 898 899 900 901 902 903 905 906 908 909 913 914 915\n[739] 916 917 919 920 921 923 924 925 927 932 935 936 937 941 942 943 944 945\n[757] 946 947 949 950 951 953 954 955 956 957 958 959 961 962 963 964 965 966\n[775] 967 968 970 972 973 974 975 976 977 979 981 982 984 986 987 988 989 990\n[793] 991 992 993 994 995 996 997 998\n\n[[4]]$app\n  [1]  918  934  726  884    4  952  634  424  343  928  985  307  561  123  534\n [16]  854  365  757  508  864  460  548  670  214  160  442  868  983  249  697\n [31]  888  375  844  432  540  147  250  431  938   12  678  107  978  216  980\n [46]  588  904  203  542  397  205  671  309  878  480  896  677  735  105  389\n [61]  939  141  907  211  948  929  469  885   74  551  652  272  164  576  269\n [76]  383  693  771  277  673  796  724   11  745  930  346  637  535  922  419\n [91]  161  763  206  286  604 1000  603  672  376  150  700  374   57  662  570\n[106]  263  618  655  911  769  176  933  756  719  912  124  999   17  931  304\n[121]  873  940  267  509  894  568  550  584  728    3  448   28  392   27  525\n[136]  926  702  647  353  119  620  832   52  824  721  575  825  355  971  162\n[151]  783  786  125  819  137  553  172  175  846  571  115  770  843  801  409\n[166]  255  519   56  856  342  714  406   93  213  180   78  256  799  129  865\n[181]  633  139  855  621  703  744  317  860  650  533   18   69  969  282  173\n[196]  767  960  288  910  685\n\n\n[[5]]\n[[5]]$train\n  [1]    1    2    3    4    7   10   11   12   17   18   19   20   22   23   24\n [16]   25   26   27   28   29   31   33   34   35   36   39   40   42   44   45\n [31]   46   48   49   51   52   54   56   57   58   59   60   61   64   65   67\n [46]   69   71   72   73   74   78   79   80   81   82   83   84   86   87   88\n [61]   89   90   91   93   94   95   96   97   98   99  100  102  103  104  105\n [76]  106  107  108  109  110  111  112  113  114  115  116  118  119  120  121\n [91]  123  124  125  126  128  129  130  131  132  133  135  136  137  138  139\n[106]  140  141  142  144  145  147  150  153  154  157  158  160  161  162  164\n[121]  165  166  167  168  169  170  171  172  173  174  175  176  177  179  180\n[136]  181  183  184  185  186  188  189  191  193  196  197  198  199  200  201\n[151]  202  203  204  205  206  207  208  209  210  211  212  213  214  215  216\n[166]  217  218  219  220  221  222  223  224  226  228  229  230  231  234  237\n[181]  240  241  242  243  244  245  246  248  249  250  251  252  253  254  255\n[196]  256  258  259  261  262  263  264  265  266  267  268  269  271  272  273\n[211]  274  275  277  278  279  280  281  282  284  285  286  288  289  290  291\n[226]  292  294  296  297  300  301  302  303  304  305  306  307  309  310  311\n[241]  312  314  315  316  317  318  320  321  326  327  328  329  330  331  334\n[256]  336  337  339  340  341  342  343  344  346  347  348  349  350  351  352\n[271]  353  354  355  357  358  359  360  361  364  365  366  367  368  369  370\n[286]  372  373  374  375  376  377  378  380  382  383  384  385  387  388  389\n[301]  390  392  393  394  395  396  397  401  402  403  404  405  406  407  408\n[316]  409  411  412  413  414  415  416  417  418  419  420  421  422  424  425\n[331]  426  427  428  429  431  432  433  434  435  436  437  438  440  441  442\n[346]  443  444  446  447  448  449  450  451  452  454  455  456  457  458  460\n[361]  462  463  465  466  467  468  469  470  471  472  473  474  477  478  479\n[376]  480  481  483  484  485  486  487  489  490  491  492  493  494  495  496\n[391]  497  498  500  501  502  503  504  505  506  507  508  509  510  511  512\n[406]  513  514  515  516  517  518  519  522  523  524  525  526  527  528  529\n[421]  531  533  534  535  537  538  539  540  541  542  543  544  546  547  548\n[436]  549  550  551  552  553  556  559  560  561  562  563  564  565  566  567\n[451]  568  570  571  572  573  574  575  576  577  578  579  580  582  583  584\n[466]  585  587  588  592  593  594  595  596  597  598  599  600  601  603  604\n[481]  605  606  607  608  609  610  611  614  615  616  617  618  619  620  621\n[496]  622  623  625  626  627  628  629  631  632  633  634  635  636  637  638\n[511]  639  640  643  644  645  647  649  650  651  652  653  654  655  656  658\n[526]  660  661  662  663  666  667  668  669  670  671  672  673  674  675  676\n[541]  677  678  679  680  681  682  683  684  685  686  687  688  689  691  692\n[556]  693  694  695  696  697  698  699  700  702  703  704  706  707  708  709\n[571]  710  711  712  714  715  719  720  721  722  724  725  726  727  728  729\n[586]  730  731  732  734  735  736  739  740  743  744  745  748  750  751  753\n[601]  754  755  756  757  759  761  762  763  764  765  766  767  768  769  770\n[616]  771  772  774  775  778  779  780  781  782  783  784  785  786  788  789\n[631]  790  791  792  793  794  795  796  797  799  800  801  802  804  806  807\n[646]  808  811  812  814  815  816  817  819  820  821  823  824  825  828  829\n[661]  830  831  832  833  834  836  838  839  840  841  842  843  844  845  846\n[676]  847  848  851  853  854  855  856  857  859  860  862  863  864  865  867\n[691]  868  869  870  871  872  873  874  877  878  879  880  882  883  884  885\n[706]  886  888  889  890  891  892  893  894  895  896  899  900  904  905  906\n[721]  907  908  909  910  911  912  913  914  915  916  917  918  920  921  922\n[736]  923  924  925  926  928  929  930  931  932  933  934  935  936  937  938\n[751]  939  940  941  942  944  945  946  948  949  952  955  956  957  958  959\n[766]  960  961  963  964  965  966  967  968  969  970  971  972  974  975  976\n[781]  977  978  979  980  981  982  983  984  985  987  988  989  993  994  995\n[796]  996  997  998  999 1000\n\n[[5]]$app\n  [1] 101 400 602 270 991 195 901 379  41 298 182 902 313 776 803 569 665 760\n [19] 276 986 954 887 319 826  77 642 163 646 953 992 363 335 659 488 613 701\n [37] 876  30 345 293 338 752  43 747 737  63  70 227 476 716 475  66 897 430\n [55]  15 178 194 155 482 499 827 962 362 461 758 371 260 554 777 738 459  76\n [73]  13 545  53 657 630 530 947 648 127 308 149 717 235 439 152 399 557 881\n [91]  75 733 192 143 852 586 612 295  85 159 299 391 835  55 190 866 238 809\n[109] 187 536 818 520  38 973  21 356 453 903  32 225 927 837 749 591 423 532\n[127] 705 773  14 122 664 943  37 875 718 742 590 555 283   6 898 287 521 741\n[145] 624 861 805 333 813  62 323  16 117 445 810 850   5 849   8 950 233 787\n[163] 410 581 464 558  68 151 325 257 239 332 148 322 951  47 324  92  50 146\n[181] 746 858 398 919 798 822 381 990 589 386 690 713 134 236   9 156 232 723\n[199] 641 247\n\n\nattr(,\"splitmethod\")\n[1] \"kwaycross\"\n\n\n\ntestAUCCV5 &lt;- NULL\ntestLogLossCV5 &lt;- NULL\nsplitPlan[[1]]\n\n$train\n  [1]    2    3    4    5    6    8    9   11   12   13   14   15   16   17   18\n [16]   20   21   22   23   24   26   27   28   29   30   32   33   35   36   37\n [31]   38   39   41   43   44   45   46   47   48   49   50   51   52   53   54\n [46]   55   56   57   58   60   62   63   64   65   66   68   69   70   71   73\n [61]   74   75   76   77   78   79   80   81   82   83   84   85   89   90   91\n [76]   92   93   94   95   97   98   99  100  101  102  103  105  107  108  109\n [91]  110  111  112  115  117  118  119  120  122  123  124  125  126  127  128\n[106]  129  130  131  132  133  134  137  138  139  140  141  142  143  144  145\n[121]  146  147  148  149  150  151  152  154  155  156  158  159  160  161  162\n[136]  163  164  165  168  170  171  172  173  174  175  176  177  178  179  180\n[151]  182  183  184  186  187  188  190  192  194  195  198  200  201  202  203\n[166]  204  205  206  207  208  209  210  211  212  213  214  215  216  217  220\n[181]  221  222  223  224  225  227  228  229  231  232  233  235  236  237  238\n[196]  239  240  241  244  245  246  247  248  249  250  252  254  255  256  257\n[211]  258  259  260  262  263  264  267  268  269  270  271  272  276  277  278\n[226]  279  280  282  283  284  285  286  287  288  290  292  293  295  296  298\n[241]  299  300  302  303  304  306  307  308  309  310  311  313  314  317  318\n[256]  319  320  321  322  323  324  325  327  328  330  332  333  335  337  338\n[271]  339  340  341  342  343  344  345  346  347  349  351  352  353  355  356\n[286]  358  359  360  361  362  363  364  365  366  368  369  370  371  372  373\n[301]  374  375  376  377  378  379  381  383  386  387  389  391  392  394  395\n[316]  396  397  398  399  400  401  403  405  406  407  408  409  410  412  413\n[331]  414  416  417  418  419  420  421  423  424  426  427  429  430  431  432\n[346]  433  434  435  436  438  439  440  441  442  444  445  446  447  448  449\n[361]  451  452  453  455  457  458  459  460  461  462  464  468  469  470  471\n[376]  475  476  478  479  480  482  484  485  487  488  489  490  491  494  495\n[391]  496  497  498  499  501  502  503  504  505  506  507  508  509  510  513\n[406]  514  516  517  518  519  520  521  523  525  527  530  531  532  533  534\n[421]  535  536  540  541  542  544  545  546  547  548  549  550  551  552  553\n[436]  554  555  556  557  558  560  561  562  563  564  565  566  567  568  569\n[451]  570  571  572  573  574  575  576  577  579  581  582  584  585  586  588\n[466]  589  590  591  593  595  596  597  600  601  602  603  604  605  606  608\n[481]  610  611  612  613  615  617  618  619  620  621  624  625  626  628  629\n[496]  630  632  633  634  635  637  638  639  640  641  642  644  645  646  647\n[511]  648  649  650  652  653  655  656  657  658  659  660  662  663  664  665\n[526]  666  667  668  670  671  672  673  674  675  676  677  678  679  680  682\n[541]  684  685  686  687  688  689  690  691  693  694  695  696  697  698  700\n[556]  701  702  703  705  709  712  713  714  715  716  717  718  719  720  721\n[571]  722  723  724  726  727  728  729  730  731  733  734  735  736  737  738\n[586]  740  741  742  743  744  745  746  747  748  749  750  752  753  755  756\n[601]  757  758  759  760  761  762  763  765  766  767  769  770  771  772  773\n[616]  775  776  777  778  779  780  782  783  785  786  787  789  790  791  792\n[631]  793  794  795  796  798  799  801  803  804  805  806  807  808  809  810\n[646]  812  813  815  818  819  820  821  822  823  824  825  826  827  828  829\n[661]  830  832  834  835  837  838  839  840  841  842  843  844  845  846  847\n[676]  848  849  850  852  853  854  855  856  857  858  859  860  861  862  864\n[691]  865  866  867  868  869  870  871  873  874  875  876  877  878  879  880\n[706]  881  883  884  885  886  887  888  889  890  891  893  894  896  897  898\n[721]  899  901  902  903  904  905  907  908  910  911  912  913  914  915  916\n[736]  917  918  919  920  921  922  923  925  926  927  928  929  930  931  932\n[751]  933  934  935  938  939  940  941  943  944  945  947  948  950  951  952\n[766]  953  954  956  957  959  960  962  964  966  967  969  970  971  972  973\n[781]  974  975  977  978  979  980  983  984  985  986  987  989  990  991  992\n[796]  993  994  998  999 1000\n\n$app\n  [1] 623 900 326 382 661 511 578 816 627 811 305 946 136 234 297 522 643 218\n [19] 169 895 961  19 116 390 529 181 474 265  40 450 909 598 995 538 833 708\n [37] 281  10 592 707 350 543 189 291 456 774  59 243 768 607 104  88 294 512\n [55] 486 428 599 166 587 251 463 692  86 681 968 380 654 882 988 357  61  87\n [73] 526 334 367 274 191  67  34 711  31 580 981 651 937 199 454   1 348  25\n [91] 135 631 836 616 814 892 936 477 924 289 942 331 354 963 493 872 329 537\n[109] 121 500 958 996 609 443 422 437 336 955 704 226 481 275 385  42 788 669\n[127] 253 732 472 965   7 725 106 739 754 402 982 153 411 425 114 404 751 483\n[145] 976 851 185 113 781 906 817 764 219 949 524 802 683  72 301 261 388 515\n[163] 710 415 583 528 316 797 157 614 315 784  96 559 622 196 473 467 393 539\n[181] 312 167 863 465 466 800 831 266 492 699 706 384 197 997 242 594 636 273\n[199] 230 193\n\n\n\nfor(i in 1:k) {\n  split &lt;- splitPlan[[i]]\n  modelo_logistic &lt;- glm(Y ~ ., \n    family = binomial, data = credit[split$train,]\n  )\n  yprob &lt;- predict(modelo_logistic, \n    newdata = credit[split$app,],\n    type = \"response\"\n  )\n  \n  testLogLossCV5[i] &lt;- MLmetrics::LogLoss(\n    yprob, as.numeric(credit[split$app,]$Y) - 1\n  )\n  testAUCCV5[i] &lt;- MLmetrics::AUC(\n    yprob, as.numeric(credit[split$app,]$Y) - 1\n  )\n}\n\n\n# LogLoss\ntestLogLossCV5\n\n[1] 0.5435299 0.4626547 0.4954209 0.5635933 0.5354452\n\nmean(testLogLossCV5)\n\n[1] 0.5201288\n\n# AUC\ntestAUCCV5\n\n[1] 0.7083968 0.8068239 0.8152420 0.7507799 0.7421078\n\nmean(testAUCCV5)\n\n[1] 0.7646701"
  },
  {
    "objectID": "apuntes/clase-08.html#ejemplo-práctico-calibración-de-hiperparámetros-de-knn",
    "href": "apuntes/clase-08.html#ejemplo-práctico-calibración-de-hiperparámetros-de-knn",
    "title": "Apuntes de clase",
    "section": "Ejemplo práctico: Calibración de hiperparámetros de KNN",
    "text": "Ejemplo práctico: Calibración de hiperparámetros de KNN\n\ndiabetes &lt;- read.csv(\n  \"../datos/DiabetesTrain.csv\", stringsAsFactors = TRUE\n)\nhead(diabetes)\n\n  glucose insulin sspg  class\n1      97     289  117 normal\n2     105     319  143 normal\n3      90     356  199 normal\n4      90     323  240 normal\n5      86     381  157 normal\n6     100     350  221 normal\n\n\n\nValidación Cruzada (LOOCV)\n\nset.seed(007)\nmean(\n  class::knn.cv(\n    train = diabetes[,1:3], cl = diabetes[,4], k = 1\n  ) == diabetes[,4]\n)\n\n[1] 0.8869565\n\nmean(\n  class::knn.cv(\n    train = diabetes[,1:3], cl = diabetes[,4], k = 3\n  ) == diabetes[,4]\n)\n\n[1] 0.9043478\n\nmean(\n  class::knn.cv(\n    train = diabetes[,1:3], cl = diabetes[,4], k = 5\n  ) == diabetes[,4]\n)\n\n[1] 0.8956522\n\nmean(\n  class::knn.cv(\n    train = diabetes[,1:3], cl = diabetes[,4], k = 7\n  ) == diabetes[,4]\n)\n\n[1] 0.8956522\n\nmean(\n  class::knn.cv(\n    train = diabetes[,1:3], cl = diabetes[,4], k = 9\n  ) == diabetes[,4]\n)\n\n[1] 0.8956522\n\nmean(\n  class::knn.cv(\n    train = diabetes[,1:3], cl = diabetes[,4], k = 11\n  ) == diabetes[,4]\n)\n\n[1] 0.8869565\n\nmean(\n  class::knn.cv(\n    train = diabetes[,1:3], cl = diabetes[,4], k = 13\n  ) == diabetes[,4]\n)\n\n[1] 0.8956522\n\nmean(\n  class::knn.cv(\n    train = diabetes[,1:3], cl = diabetes[,4], k = 15\n  ) == diabetes[,4]\n)\n\n[1] 0.8782609"
  },
  {
    "objectID": "apuntes/clase-08.html#examen-parcial",
    "href": "apuntes/clase-08.html#examen-parcial",
    "title": "Apuntes de clase",
    "section": "Examen Parcial",
    "text": "Examen Parcial\nHasta este punto viene el examen.\nLa primera parte es de pregunta-respuesta múltiple. La segunda parte es con R (vale 10 puntos o menos).\nEl examen suele durar 3 o menos horas."
  },
  {
    "objectID": "apuntes/clase-09.html#decision-trees",
    "href": "apuntes/clase-09.html#decision-trees",
    "title": "Apuntes de clase",
    "section": "Decision Trees",
    "text": "Decision Trees\n\nSirven para predicción numérica (regresión) y categórica (clasificación).\nEs un modelo muy fácil de interpretar, útil para inferencia.\nEste modelo tiene alta variabilidad (válido para todo modelo de árboles).\nEs tan flexible, que fácilmente produce sobreajustes.\nEs un modelo escalable.\nEste modelo no genera directamente la probabilidad de pertenencia a una clase, sino la clasificación en sí.\n\nAún así, se puede estimar la probabilidad de pertenencia a una clase \\(K\\) , contando la proporción de observaciones (de la muestra) que fue clasificada como clase \\(K\\) .\nSin embargo, cualquier observación pertenciente al mismo nodo posee la misma probabilidad estimada de pertenencia a una clase.\nPor ello, la curva ROC de este modelo es muy poligonal … no funciona tan bien.\n\n\n\nPartes de un Árbol de Decisión\n\nNodo interno (root node): Denota una prueba sobre un atributo.\nRama (branch): Corresponde a un valor de atributo; representa el resultado de una prueba.\nNodo terminal (leaf node): Representa una etiqueta de clase.\nCada camino es una conjunción de valores de atributos.\n\n\n\nConstrucción de un Árbol de Decisión\n\nExisten diversos tree induction algorithms, los cuales pueden generar árboles de decisión muy diferentes."
  },
  {
    "objectID": "apuntes/clase-09.html#características-de-decision-trees",
    "href": "apuntes/clase-09.html#características-de-decision-trees",
    "title": "Apuntes de clase",
    "section": "Características de Decision Trees",
    "text": "Características de Decision Trees\n\nEste modelo se puede construir rápidamente.\nNo requiere que las variables sean independientes.\nPermite evaluar qué variables son importantes y cómo interactúan ente ellos.\nEs robusto a outliers.\nFunciona bien incluso cuando hay variables faltantes en observaciones (datos perdidos).\nLas variables se discretizan al momemto de generar el árbol de decisión. Esto se evidencia para predictores numéricos, pues, nodo por nodo, se discretizan en dos categorías/regiones vía un punto de corte."
  },
  {
    "objectID": "apuntes/clase-09.html#inducción-en-decision-trees",
    "href": "apuntes/clase-09.html#inducción-en-decision-trees",
    "title": "Apuntes de clase",
    "section": "Inducción en Decision Trees",
    "text": "Inducción en Decision Trees\n\nConstrucción del árbol:\n\nA la hora de fijar un punto de corte, podemos calcular la entropía asociada a las nuevas regiones creadas, teniendo en cuenta que entropía igual a cero maximiza la probabilidad de realizar una buena clasificación.\n\nPoda del árbol: Identificar y remover ramas que causen ruido o tengan outliers.\nEl que se divida una región en dos subregiones no depende de si la variable es numérica o categórica, sino del algoritmo de inducción empleado.\n\n\nConstrucción\n\n\nMúltiples árboles de decisión\n\n\nInducción"
  },
  {
    "objectID": "apuntes/clase-09.html#medidas-de-impureza",
    "href": "apuntes/clase-09.html#medidas-de-impureza",
    "title": "Apuntes de clase",
    "section": "Medidas de Impureza",
    "text": "Medidas de Impureza\n\nCoeficiente de Gini\nEntropía\nMayor valor de estas medidas, implica mayor impureza del nodo terminal.\nA mayor impureza, sería más difícil saber a que categoría pertenece una observación presente en un nodo terminal.\nCoeficiente de Gini asociado a una partición que produce nodos terminales es el promedio ponderado de los coeficientes de Gini de aquellos nodos terminales, donde la ponderación es respecto al número de observaciones en cada nodo terminal.\nNodos con medida de impureza igual a cero no se dividen más, pues no es necesario."
  },
  {
    "objectID": "apuntes/clase-09.html#ejemplo-práctico",
    "href": "apuntes/clase-09.html#ejemplo-práctico",
    "title": "Apuntes de clase",
    "section": "Ejemplo práctico",
    "text": "Ejemplo práctico\n\nlibrary(rpart)\n\nWarning: package 'rpart' was built under R version 4.1.3\n\n\n\nminsplit:\n\nMinimum number of observations in a node para que sea dividido.\nDefault value: 20\nSuele no ser necesario alterar si se cuenta con muchos datos.\n\nminbucket:\n\nMinimum number of observations in any terminal/leaf node.\nDefault value: minsplit / 3\n\ncp:\n\nParámetro de complejidad\nValor más común por modificar al usar el modelo Decision Tree.\nIndica que, si el criterio de impureza no es reducido (restando las métricas de complejidad (no quotient)) en más de cp (respecto al nivel anterior) entonces, se para.\nDefault value: cp = 0.01\n\n\n\nbupa &lt;- read.table(\"../datos/bupa.txt\", header = TRUE, sep = \",\") \nhead(bupa)\n\n  V1 V2 V3 V4 V5 V6 V7\n1 85 92 45 27 31  0  1\n2 85 64 59 32 23  0  2\n3 86 54 33 16 54  0  2\n4 91 78 34 24 36  0  2\n5 87 70 12 28 10  0  2\n6 98 55 13 17 17  0  2\n\n# Declarar V7 como un factor\nbupa[,7] &lt;- as.factor(bupa[,7])\n\n\nEjemplo 1\n\nConsideramos:\n\nminbucket = 50\nminsplit = 150\n\n\n\n# Estimar el árbol\narbol1 &lt;- rpart(\n  V7 ~ V3 + V5, data = bupa, method = \"class\", minbucket = 50\n)\narbol1\n\nn= 345 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 345 145 2 (0.4202899 0.5797101)  \n  2) V5&lt; 20.5 140  61 1 (0.5642857 0.4357143) *\n  3) V5&gt;=20.5 205  66 2 (0.3219512 0.6780488) *\n\n\n\nInterpretación:\n\n61 predicciones incorrectas, de un total de 140 observaciones, en el nodo terminal asociado a V5 &lt; 20.5.\n66 predicciones incorrectas, de un total de 205 observaciones, en el nodo terminal asociado a V5 &lt; 20.5.\n\n\n\n# Graficando el arbol\nplot(arbol1, margin = 0.25)\ntext(arbol1, use.n = TRUE)\n\n\n\n\n\n# Mejoramos los gráficos\nlibrary(partykit)\n\nWarning: package 'partykit' was built under R version 4.1.3\n\n\nLoading required package: grid\n\n\nLoading required package: libcoin\n\n\nWarning: package 'libcoin' was built under R version 4.1.3\n\n\nLoading required package: mvtnorm\n\nplot(partykit::as.party(arbol1), tp_args = list(id = FALSE))\n\n\n\n\n\n\nEjemplo 2\nFijamos minbucket = 20 (minsplit = 60) para obtener un árbol con más ramas.\n\narbol2 &lt;- rpart(\n  V7 ~ V3 + V5, data = bupa, method = \"class\", minbucket = 20\n)\nplot(as.party(arbol2), tp_args = list(id = FALSE))"
  },
  {
    "objectID": "apuntes/clase-09.html#ejemplo-3",
    "href": "apuntes/clase-09.html#ejemplo-3",
    "title": "Apuntes de clase",
    "section": "Ejemplo 3",
    "text": "Ejemplo 3\nFijamos cp = 0.05.\n\narbol3 &lt;- rpart(\n  V7 ~ V3 + V5, data = bupa, method = \"class\", cp = 0.05\n)\nplot(as.party(arbol3), tp_args = list(id = FALSE))\n\n\n\n\n\nEjemplo 4\nFijamos cp = 0.001 para obtener un árbol con más ramas.\n\narbol4 &lt;- rpart(\n  V7 ~ V3 + V5, data = bupa, method = \"class\", cp = 0.001\n)\nplot(as.party(arbol4), tp_args = list(id = FALSE))\n\n\n\n\n\n\nEjemplo 5\nFijamos maxdepth = 3.\n\narbol5 &lt;- rpart(\n  V7 ~ V3 + V5, data = bupa, method = \"class\", maxdepth = 3\n)\nplot(as.party(arbol5), tp_args = list(id = FALSE))\n\n\n\n\n\n\nEjemplo 6: Podar el árbol\n\nset.seed(060717)\n# A propósito dejamos crecer mucho el árbol, fijando\n# un valor vajo de cp .\narbol &lt;- rpart(\n  V7~ V3 + V5, data = bupa, method = \"class\", cp = 0.001\n)\n\n# Fijar un criterio para podar el árbol\narbol6 &lt;- rpart::prune(arbol, cp = 0.1)\n\nplot(as.party(arbol), tp_args = list(id = FALSE))\n\n\n\nplot(as.party(arbol6), tp_args = list(id = FALSE))\n\n\n\n\n\nUsamos validación estándar con cp como hiperparámetro\n\n# Elección del modelo con métrica menor (error) o mejor (accuracy, por ejemplo)\nrpart::printcp(arbol)\n\n\nClassification tree:\nrpart(formula = V7 ~ V3 + V5, data = bupa, method = \"class\", \n    cp = 0.001)\n\nVariables actually used in tree construction:\n[1] V3 V5\n\nRoot node error: 145/345 = 0.42029\n\nn= 345 \n\n         CP nsplit rel error  xerror     xstd\n1 0.1379310      0   1.00000 1.00000 0.063230\n2 0.0206897      2   0.72414 0.75862 0.059697\n3 0.0137931      6   0.63448 0.75172 0.059551\n4 0.0114943      7   0.62069 0.78621 0.060253\n5 0.0068966     10   0.58621 0.77241 0.059980\n6 0.0034483     12   0.57241 0.82069 0.060891\n7 0.0010000     16   0.55862 0.87586 0.061781\n\n\nLa columna rel error (relative error) no nos sirve, pues esos valores siempre están decreciendo, ya que la impureza siempre diminuye para niveles más altos del árbol.\n\n# Elección del modelo vía la regla del error estándar\nrpart::plotcp(arbol)\n\n\n\n\nEn el eje X están las medias geométricas, de dos en dos, de valores de CP en la tabla mostrada previamente.\n\n\n\nUsando el criterio del Min xerror\n\narbol7 &lt;- prune(arbol, cp = 0.0137931)\nplot(as.party(arbol7), tp_args = list(id = FALSE))\n\n\n\n\n\n\nUsando el criterio de +-xstd\n\narbol7 &lt;- prune(arbol, cp = 0.053)\nplot(as.party(arbol7), tp_args = list(id = FALSE))\n\n\n\n\n\n\nAutomatizando la selección del Valor óptimo de CP (criterio Min xerror)\n\narbol.completo &lt;- rpart(\n  V7 ~ ., data = bupa, method = \"class\", cp = 0, minbucket = 1\n)\narbol.completo\n\nn= 345 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n  1) root 345 145 2 (0.42028986 0.57971014)  \n    2) V5&lt; 20.5 140  61 1 (0.56428571 0.43571429)  \n      4) V3&gt;=19.5 80  20 1 (0.75000000 0.25000000)  \n        8) V4&lt; 21.5 43   5 1 (0.88372093 0.11627907)  \n         16) V6&lt; 3.5 35   2 1 (0.94285714 0.05714286)  \n           32) V1&gt;=85.5 31   0 1 (1.00000000 0.00000000) *\n           33) V1&lt; 85.5 4   2 1 (0.50000000 0.50000000)  \n             66) V1&lt; 83.5 2   0 1 (1.00000000 0.00000000) *\n             67) V1&gt;=83.5 2   0 2 (0.00000000 1.00000000) *\n         17) V6&gt;=3.5 8   3 1 (0.62500000 0.37500000)  \n           34) V3&gt;=23.5 5   0 1 (1.00000000 0.00000000) *\n           35) V3&lt; 23.5 3   0 2 (0.00000000 1.00000000) *\n        9) V4&gt;=21.5 37  15 1 (0.59459459 0.40540541)  \n         18) V1&gt;=88.5 22   5 1 (0.77272727 0.22727273)  \n           36) V4&gt;=24.5 14   1 1 (0.92857143 0.07142857) *\n           37) V4&lt; 24.5 8   4 1 (0.50000000 0.50000000)  \n             74) V6&gt;=5 2   0 1 (1.00000000 0.00000000) *\n             75) V6&lt; 5 6   2 2 (0.33333333 0.66666667)  \n              150) V2&gt;=63.5 3   1 1 (0.66666667 0.33333333)  \n                300) V2&lt; 74 2   0 1 (1.00000000 0.00000000) *\n                301) V2&gt;=74 1   0 2 (0.00000000 1.00000000) *\n              151) V2&lt; 63.5 3   0 2 (0.00000000 1.00000000) *\n         19) V1&lt; 88.5 15   5 2 (0.33333333 0.66666667)  \n           38) V3&gt;=29.5 3   0 1 (1.00000000 0.00000000) *\n           39) V3&lt; 29.5 12   2 2 (0.16666667 0.83333333)  \n             78) V2&gt;=81.5 1   0 1 (1.00000000 0.00000000) *\n             79) V2&lt; 81.5 11   1 2 (0.09090909 0.90909091) *\n      5) V3&lt; 19.5 60  19 2 (0.31666667 0.68333333)  \n       10) V2&gt;=77 16   5 1 (0.68750000 0.31250000)  \n         20) V1&gt;=88.5 12   2 1 (0.83333333 0.16666667)  \n           40) V2&lt; 125 11   1 1 (0.90909091 0.09090909)  \n             80) V1&gt;=89.5 8   0 1 (1.00000000 0.00000000) *\n             81) V1&lt; 89.5 3   1 1 (0.66666667 0.33333333)  \n              162) V3&lt; 17.5 2   0 1 (1.00000000 0.00000000) *\n              163) V3&gt;=17.5 1   0 2 (0.00000000 1.00000000) *\n           41) V2&gt;=125 1   0 2 (0.00000000 1.00000000) *\n         21) V1&lt; 88.5 4   1 2 (0.25000000 0.75000000)  \n           42) V1&lt; 85.5 1   0 1 (1.00000000 0.00000000) *\n           43) V1&gt;=85.5 3   0 2 (0.00000000 1.00000000) *\n       11) V2&lt; 77 44   8 2 (0.18181818 0.81818182)  \n         22) V4&lt; 14.5 6   2 1 (0.66666667 0.33333333)  \n           44) V1&gt;=91.5 3   0 1 (1.00000000 0.00000000) *\n           45) V1&lt; 91.5 3   1 2 (0.33333333 0.66666667)  \n             90) V2&gt;=60 1   0 1 (1.00000000 0.00000000) *\n             91) V2&lt; 60 2   0 2 (0.00000000 1.00000000) *\n         23) V4&gt;=14.5 38   4 2 (0.10526316 0.89473684)  \n           46) V6&lt; 3.5 23   4 2 (0.17391304 0.82608696)  \n             92) V5&gt;=18.5 1   0 1 (1.00000000 0.00000000) *\n             93) V5&lt; 18.5 22   3 2 (0.13636364 0.86363636)  \n              186) V6&gt;=2.5 1   0 1 (1.00000000 0.00000000) *\n              187) V6&lt; 2.5 21   2 2 (0.09523810 0.90476190)  \n                374) V5&gt;=14.5 8   2 2 (0.25000000 0.75000000)  \n                  748) V5&lt; 15.5 2   0 1 (1.00000000 0.00000000) *\n                  749) V5&gt;=15.5 6   0 2 (0.00000000 1.00000000) *\n                375) V5&lt; 14.5 13   0 2 (0.00000000 1.00000000) *\n           47) V6&gt;=3.5 15   0 2 (0.00000000 1.00000000) *\n    3) V5&gt;=20.5 205  66 2 (0.32195122 0.67804878)  \n      6) V6&gt;=5.5 69  33 2 (0.47826087 0.52173913)  \n       12) V3&gt;=35.5 35  12 1 (0.65714286 0.34285714)  \n         24) V4&lt; 42.5 25   5 1 (0.80000000 0.20000000)  \n           48) V2&lt; 86.5 19   1 1 (0.94736842 0.05263158)  \n             96) V1&gt;=84.5 18   0 1 (1.00000000 0.00000000) *\n             97) V1&lt; 84.5 1   0 2 (0.00000000 1.00000000) *\n           49) V2&gt;=86.5 6   2 2 (0.33333333 0.66666667)  \n             98) V2&gt;=123 1   0 1 (1.00000000 0.00000000) *\n             99) V2&lt; 123 5   1 2 (0.20000000 0.80000000)  \n              198) V3&gt;=57.5 1   0 1 (1.00000000 0.00000000) *\n              199) V3&lt; 57.5 4   0 2 (0.00000000 1.00000000) *\n         25) V4&gt;=42.5 10   3 2 (0.30000000 0.70000000)  \n           50) V1&gt;=96.5 3   0 1 (1.00000000 0.00000000) *\n           51) V1&lt; 96.5 7   0 2 (0.00000000 1.00000000) *\n       13) V3&lt; 35.5 34  10 2 (0.29411765 0.70588235)  \n         26) V4&lt; 22.5 10   4 1 (0.60000000 0.40000000)  \n           52) V2&lt; 75.5 6   1 1 (0.83333333 0.16666667) *\n           53) V2&gt;=75.5 4   1 2 (0.25000000 0.75000000)  \n            106) V1&lt; 85 1   0 1 (1.00000000 0.00000000) *\n            107) V1&gt;=85 3   0 2 (0.00000000 1.00000000) *\n         27) V4&gt;=22.5 24   4 2 (0.16666667 0.83333333)  \n           54) V2&gt;=92.5 6   3 1 (0.50000000 0.50000000)  \n            108) V1&gt;=91.5 4   1 1 (0.75000000 0.25000000)  \n              216) V3&gt;=21.5 3   0 1 (1.00000000 0.00000000) *\n              217) V3&lt; 21.5 1   0 2 (0.00000000 1.00000000) *\n            109) V1&lt; 91.5 2   0 2 (0.00000000 1.00000000) *\n           55) V2&lt; 92.5 18   1 2 (0.05555556 0.94444444) *\n      7) V6&lt; 5.5 136  33 2 (0.24264706 0.75735294)  \n       14) V2&gt;=65.5 75  26 2 (0.34666667 0.65333333)  \n         28) V4&lt; 24.5 41  20 1 (0.51219512 0.48780488)  \n           56) V6&lt; 2.5 27   9 1 (0.66666667 0.33333333)  \n            112) V5&lt; 29.5 11   1 1 (0.90909091 0.09090909) *\n            113) V5&gt;=29.5 16   8 1 (0.50000000 0.50000000)  \n              226) V1&gt;=87.5 10   3 1 (0.70000000 0.30000000)  \n                452) V1&lt; 92.5 8   1 1 (0.87500000 0.12500000)  \n                  904) V6&gt;=0.25 7   0 1 (1.00000000 0.00000000) *\n                  905) V6&lt; 0.25 1   0 2 (0.00000000 1.00000000) *\n                453) V1&gt;=92.5 2   0 2 (0.00000000 1.00000000) *\n              227) V1&lt; 87.5 6   1 2 (0.16666667 0.83333333)  \n                454) V1&lt; 80.5 1   0 1 (1.00000000 0.00000000) *\n                455) V1&gt;=80.5 5   0 2 (0.00000000 1.00000000) *\n           57) V6&gt;=2.5 14   3 2 (0.21428571 0.78571429)  \n            114) V2&lt; 69 2   0 1 (1.00000000 0.00000000) *\n            115) V2&gt;=69 12   1 2 (0.08333333 0.91666667) *\n         29) V4&gt;=24.5 34   5 2 (0.14705882 0.85294118)  \n           58) V3&gt;=39 15   5 2 (0.33333333 0.66666667)  \n            116) V3&lt; 45.5 6   2 1 (0.66666667 0.33333333)  \n              232) V5&lt; 106 5   1 1 (0.80000000 0.20000000) *\n              233) V5&gt;=106 1   0 2 (0.00000000 1.00000000) *\n            117) V3&gt;=45.5 9   1 2 (0.11111111 0.88888889)  \n              234) V1&lt; 85 3   1 2 (0.33333333 0.66666667)  \n                468) V5&gt;=70.5 1   0 1 (1.00000000 0.00000000) *\n                469) V5&lt; 70.5 2   0 2 (0.00000000 1.00000000) *\n              235) V1&gt;=85 6   0 2 (0.00000000 1.00000000) *\n           59) V3&lt; 39 19   0 2 (0.00000000 1.00000000) *\n       15) V2&lt; 65.5 61   7 2 (0.11475410 0.88524590)  \n         30) V5&gt;=26.5 42   7 2 (0.16666667 0.83333333)  \n           60) V5&lt; 35.5 15   5 2 (0.33333333 0.66666667)  \n            120) V6&gt;=0.75 10   5 1 (0.50000000 0.50000000)  \n              240) V1&gt;=89 5   1 1 (0.80000000 0.20000000)  \n                480) V2&gt;=50 4   0 1 (1.00000000 0.00000000) *\n                481) V2&lt; 50 1   0 2 (0.00000000 1.00000000) *\n              241) V1&lt; 89 5   1 2 (0.20000000 0.80000000)  \n                482) V1&lt; 83 1   0 1 (1.00000000 0.00000000) *\n                483) V1&gt;=83 4   0 2 (0.00000000 1.00000000) *\n            121) V6&lt; 0.75 5   0 2 (0.00000000 1.00000000) *\n           61) V5&gt;=35.5 27   2 2 (0.07407407 0.92592593)  \n            122) V4&lt; 23.5 11   2 2 (0.18181818 0.81818182)  \n              244) V4&gt;=22.5 3   1 1 (0.66666667 0.33333333)  \n                488) V1&lt; 92.5 2   0 1 (1.00000000 0.00000000) *\n                489) V1&gt;=92.5 1   0 2 (0.00000000 1.00000000) *\n              245) V4&lt; 22.5 8   0 2 (0.00000000 1.00000000) *\n            123) V4&gt;=23.5 16   0 2 (0.00000000 1.00000000) *\n         31) V5&lt; 26.5 19   0 2 (0.00000000 1.00000000) *\n\nxerr &lt;- arbol.completo$cptable[,\"xerror\"]\nminxerr &lt;- which.min(xerr)\nmincp &lt;- arbol.completo$cptable[minxerr, \"CP\"]\narbol.prune &lt;- prune(arbol.completo, cp = mincp)\nplot(as.party(arbol.prune), tp_args = list(id = FALSE))\n\n\n\n\n\n# Predicción usando el árbol podado\n# Calcular los valores predichos\npred &lt;- predict(arbol.prune, bupa[,c(-7)], type = \"class\")\n\n# Calcular la matriz de confusión\ncaret::confusionMatrix(pred, bupa$V7)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   1   2\n         1  60  20\n         2  85 180\n                                          \n               Accuracy : 0.6957          \n                 95% CI : (0.6441, 0.7438)\n    No Information Rate : 0.5797          \n    P-Value [Acc &gt; NIR] : 5.902e-06       \n                                          \n                  Kappa : 0.3344          \n                                          \n Mcnemar's Test P-Value : 4.217e-10       \n                                          \n            Sensitivity : 0.4138          \n            Specificity : 0.9000          \n         Pos Pred Value : 0.7500          \n         Neg Pred Value : 0.6792          \n             Prevalence : 0.4203          \n         Detection Rate : 0.1739          \n   Detection Prevalence : 0.2319          \n      Balanced Accuracy : 0.6569          \n                                          \n       'Positive' Class : 1"
  },
  {
    "objectID": "apuntes/clase-11.html#boosting",
    "href": "apuntes/clase-11.html#boosting",
    "title": "Apuntes de clase",
    "section": "Boosting",
    "text": "Boosting\n\nLa idea principal es realizar un ponderación con varios modelos.\nEs más complejo que bagging respecto a cómo calibrarlo (la ponderación).\n\n\n¿Cómo funciona?\n\nAsignar pesos a las unidades de entrenamiento.\nEntrenar iterativamente \\(k\\) clasificadores.\nTras entrenar el clasificador \\(M_i\\), actualizar los pesos de manera que el siguiente clasificador \\(M_{i+1}\\) ponga más atención a las unidades entrenamiento que \\(M_i\\) clasificó erróneamente.\nEl clasificador final combina los votos de cada clasificador individual, donde el peso del voto de cada clasificador es una función de su precisión.\n\n\n\nAdaBoost (Adaptative Boosting)\n\nEste algoritmo no surgió de Estadística … no hay supuestos (normalidad, etc) sobre los predictores y la response.\nPropiamnte dicho, el muestro de AdaBoost no es como en Bootstrap, pues las observaciones no siempre tienen el mismo peso al ser sampled with replacement.\nA mayor peso de unidad, más veces fue wrongly classified.\nLa idea es centrarse más en las unidades que en el paso anterior fueron wrongly classified.\nEn cada loop, el error de los modelos se calcula respecto a los mismos datos con los que fue entrenado (no validation ni testing sets).\n\n\nProposition 1 AdaBoost no es adecuado de usar cuando las clases están muy desbalanceadas. Esto debido a que AdaBoost usa el error (complemnto de accuracy) para la ponderación de los clasificadores; pese a que otras métricas, como sensibilidad son más adecuadas para medir la eficiencia del modelo como predictor.\n\n\nAlgoritmo\n\nInput:\n\n\\(D\\): datos con \\(d\\) unidades clasificadas\n\\(k\\) (hiperparámetro): número de iteracions (se genera un clasificador por iteración)\n\nOutput: Modelo compuesto\n\nInicializar peso de unidad como \\(\\dfrac{1}{d}\\) .\nPara cada iteración, muestrear \\(D\\) con reemplazo de acuerdo a los pesos, para obtener \\(D_i\\) .\nUsar \\(D_i\\) para derivar \\(M_i\\) .\nCalcular \\(\\text{ error}\\left( M_i \\right) = \\displaystyle{ \\sum_{j=1}^{d}} w_j \\text{ err}\\left( X_j \\right)\\) , donde \\(\\text{ err }\\left( X_j \\right)\\) indica si la unidad \\(X_j\\) fue mal clasificada.\nSi \\(\\text{ error }\\left( M_i \\right) &gt; 0.5\\), volver al paso 2.\nPara cada unidad en \\(D_i\\) bien clasificada, multiplicar su peso por \\(\\dfrac{\\text{ error}\\left( M_i \\right)}{1- \\text{ error}\\left( M_i \\right)}\\)\nActualizar los pesos y normalizarlos.\n\n\n\n\nPeso de clasificadores\n\nA menor tasa de error, el clasificador es más exacto, así que su voto vale más.\nEl peso del voto del clasificador \\(M_i\\) es \\(\\log \\left( \\dfrac{1- \\text{ error}\\left( M_i \\right)}{\\text{ error}\\left( M_i \\right)} \\right)\\) .\nPara cada clase \\(c\\), se suman los pesos de cada clasificador que asignó la clase \\(c\\) a la unidad de observación \\(X\\). Luego, gana la clase con la mayor suma de pesos, y esa corresponde a la predicción asignada a \\(X\\) ."
  },
  {
    "objectID": "apuntes/clase-11.html#bagging-vs-boosting",
    "href": "apuntes/clase-11.html#bagging-vs-boosting",
    "title": "Apuntes de clase",
    "section": "Bagging vs Boosting",
    "text": "Bagging vs Boosting\n\nComo Boosting se focaliza en las unidades mal clasificadas, corre el riesgo de sobreajustar el modelo resultante a esos datos; mientras que Bagging es menos susceptible a un sobreajuste del modelo.\nAlgunas veces, el modelo resultante puede ser menos preciso que un modelo único para los mismos datos.\nBoosting tiende a lograr mayor precisión que Bagging, del modelo resultante, en comparación a un modelo único."
  },
  {
    "objectID": "apuntes/clase-11.html#ejemplo-práctico-adabag",
    "href": "apuntes/clase-11.html#ejemplo-práctico-adabag",
    "title": "Apuntes de clase",
    "section": "Ejemplo práctico: AdaBag",
    "text": "Ejemplo práctico: AdaBag\n\nlibrary(adabag)\n\nLoading required package: rpart\n\n\nWarning: package 'rpart' was built under R version 4.1.3\n\n\nLoading required package: caret\n\n\nWarning: package 'caret' was built under R version 4.1.3\n\n\nLoading required package: ggplot2\n\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\n\nLoading required package: lattice\n\n\nLoading required package: foreach\n\n\nWarning: package 'foreach' was built under R version 4.1.3\n\n\nLoading required package: doParallel\n\n\nWarning: package 'doParallel' was built under R version 4.1.3\n\n\nLoading required package: iterators\n\n\nWarning: package 'iterators' was built under R version 4.1.3\n\n\nLoading required package: parallel\n\nlibrary(rpart)\ndata(irir)\n\nWarning in data(irir): data set 'irir' not found\n\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\n\n# Seleccion de datos de trabajo o entrenamiento,\n# se seleccionan 25 de cada especie\ntrain &lt;- c(\n  sample(1:50, 25), sample(51:100, 25), sample(101:150, 25)\n)\n\n\n# Modelamos la especie a partir de todas las variables en los datos de entrenamiento\n# mfinal -&gt; N de iteraciones\n# boos -&gt; Permite submuestrear para generar los pesos\niris.adaboost &lt;- adabag::boosting(\n  Species ~ ., data = iris[train,], \n  boos = TRUE, mfinal = 10\n)\niris.adaboost\n\n$formula\nSpecies ~ .\n\n$trees\n$trees[[1]]\nn= 75 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 75 48 virginica (0.29333333 0.34666667 0.36000000)  \n  2) Petal.Length&lt; 4.85 47 22 versicolor (0.46808511 0.53191489 0.00000000)  \n    4) Petal.Length&lt; 2.45 22  0 setosa (1.00000000 0.00000000 0.00000000) *\n    5) Petal.Length&gt;=2.45 25  0 versicolor (0.00000000 1.00000000 0.00000000) *\n  3) Petal.Length&gt;=4.85 28  1 virginica (0.00000000 0.03571429 0.96428571) *\n\n$trees[[2]]\nn= 75 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 75 48 setosa (0.36000000 0.30666667 0.33333333)  \n  2) Petal.Length&lt; 2.6 27  0 setosa (1.00000000 0.00000000 0.00000000) *\n  3) Petal.Length&gt;=2.6 48 23 virginica (0.00000000 0.47916667 0.52083333)  \n    6) Petal.Width&lt; 1.75 23  1 versicolor (0.00000000 0.95652174 0.04347826) *\n    7) Petal.Width&gt;=1.75 25  1 virginica (0.00000000 0.04000000 0.96000000) *\n\n$trees[[3]]\nn= 75 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 75 48 virginica (0.3333333 0.3066667 0.3600000)  \n  2) Petal.Length&lt; 2.8 25  0 setosa (1.0000000 0.0000000 0.0000000) *\n  3) Petal.Length&gt;=2.8 50 23 virginica (0.0000000 0.4600000 0.5400000)  \n    6) Petal.Length&lt; 4.75 16  0 versicolor (0.0000000 1.0000000 0.0000000) *\n    7) Petal.Length&gt;=4.75 34  7 virginica (0.0000000 0.2058824 0.7941176) *\n\n$trees[[4]]\nn= 75 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 75 34 versicolor (0.2000000 0.5466667 0.2533333)  \n  2) Petal.Length&lt; 2.6 15  0 setosa (1.0000000 0.0000000 0.0000000) *\n  3) Petal.Length&gt;=2.6 60 19 versicolor (0.0000000 0.6833333 0.3166667)  \n    6) Petal.Width&lt; 1.9 48  7 versicolor (0.0000000 0.8541667 0.1458333) *\n    7) Petal.Width&gt;=1.9 12  0 virginica (0.0000000 0.0000000 1.0000000) *\n\n$trees[[5]]\nn= 75 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 75 43 virginica (0.17333333 0.40000000 0.42666667)  \n  2) Petal.Width&lt; 1.75 43 16 versicolor (0.30232558 0.62790698 0.06976744)  \n    4) Sepal.Length&lt; 5.55 13  0 setosa (1.00000000 0.00000000 0.00000000) *\n    5) Sepal.Length&gt;=5.55 30  3 versicolor (0.00000000 0.90000000 0.10000000) *\n  3) Petal.Width&gt;=1.75 32  3 virginica (0.00000000 0.09375000 0.90625000) *\n\n$trees[[6]]\nn= 75 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 75 39 versicolor (0.1466667 0.4800000 0.3733333)  \n   2) Petal.Length&lt; 2.45 11  0 setosa (1.0000000 0.0000000 0.0000000) *\n   3) Petal.Length&gt;=2.45 64 28 versicolor (0.0000000 0.5625000 0.4375000)  \n     6) Petal.Length&lt; 4.85 32  4 versicolor (0.0000000 0.8750000 0.1250000)  \n      12) Sepal.Length&lt; 6.05 25  0 versicolor (0.0000000 1.0000000 0.0000000) *\n      13) Sepal.Length&gt;=6.05 7  3 virginica (0.0000000 0.4285714 0.5714286) *\n     7) Petal.Length&gt;=4.85 32  8 virginica (0.0000000 0.2500000 0.7500000) *\n\n$trees[[7]]\nn= 75 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 75 42 versicolor (0.1333333 0.4400000 0.4266667)  \n   2) Petal.Length&lt; 2.45 10  0 setosa (1.0000000 0.0000000 0.0000000) *\n   3) Petal.Length&gt;=2.45 65 32 versicolor (0.0000000 0.5076923 0.4923077)  \n     6) Petal.Length&lt; 5.05 49 16 versicolor (0.0000000 0.6734694 0.3265306)  \n      12) Sepal.Width&gt;=2.25 42  9 versicolor (0.0000000 0.7857143 0.2142857)  \n        24) Petal.Width&lt; 1.75 26  0 versicolor (0.0000000 1.0000000 0.0000000) *\n        25) Petal.Width&gt;=1.75 16  7 virginica (0.0000000 0.4375000 0.5625000) *\n      13) Sepal.Width&lt; 2.25 7  0 virginica (0.0000000 0.0000000 1.0000000) *\n     7) Petal.Length&gt;=5.05 16  0 virginica (0.0000000 0.0000000 1.0000000) *\n\n$trees[[8]]\nn= 75 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 75 26 versicolor (0.06666667 0.65333333 0.28000000)  \n   2) Petal.Length&lt; 5.05 68 19 versicolor (0.07352941 0.72058824 0.20588235)  \n     4) Sepal.Width&gt;=2.85 44  5 versicolor (0.11363636 0.88636364 0.00000000)  \n       8) Petal.Length&lt; 4.5 7  2 setosa (0.71428571 0.28571429 0.00000000) *\n       9) Petal.Length&gt;=4.5 37  0 versicolor (0.00000000 1.00000000 0.00000000) *\n     5) Sepal.Width&lt; 2.85 24 10 virginica (0.00000000 0.41666667 0.58333333)  \n      10) Petal.Width&lt; 1.45 10  0 versicolor (0.00000000 1.00000000 0.00000000) *\n      11) Petal.Width&gt;=1.45 14  0 virginica (0.00000000 0.00000000 1.00000000) *\n   3) Petal.Length&gt;=5.05 7  0 virginica (0.00000000 0.00000000 1.00000000) *\n\n$trees[[9]]\nn= 75 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 75 27 versicolor (0.06666667 0.64000000 0.29333333)  \n  2) Petal.Length&lt; 5.05 66 18 versicolor (0.07575758 0.72727273 0.19696970)  \n    4) Sepal.Width&gt;=2.85 52  8 versicolor (0.07692308 0.84615385 0.07692308) *\n    5) Sepal.Width&lt; 2.85 14  5 virginica (0.07142857 0.28571429 0.64285714) *\n  3) Petal.Length&gt;=5.05 9  0 virginica (0.00000000 0.00000000 1.00000000) *\n\n$trees[[10]]\nn= 75 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 75 37 versicolor (0.18666667 0.50666667 0.30666667)  \n   2) Sepal.Length&lt; 5.35 14  0 setosa (1.00000000 0.00000000 0.00000000) *\n   3) Sepal.Length&gt;=5.35 61 23 versicolor (0.00000000 0.62295082 0.37704918)  \n     6) Petal.Length&lt; 4.85 35  3 versicolor (0.00000000 0.91428571 0.08571429) *\n     7) Petal.Length&gt;=4.85 26  6 virginica (0.00000000 0.23076923 0.76923077)  \n      14) Petal.Width&lt; 1.75 7  1 versicolor (0.00000000 0.85714286 0.14285714) *\n      15) Petal.Width&gt;=1.75 19  0 virginica (0.00000000 0.00000000 1.00000000) *\n\n\n$weights\n [1] 1.7986561 1.8633467 0.9054636 0.8980642 0.5340658 0.8290422 0.9012836\n [8] 1.5820530 0.6341534 0.7857651\n\n$votes\n            [,1]       [,2]       [,3]\n [1,]  8.7779095  1.9539843  0.0000000\n [2,] 10.0977403  0.6341534  0.0000000\n [3,] 10.0977403  0.6341534  0.0000000\n [4,] 10.0977403  0.6341534  0.0000000\n [5,] 10.0977403  0.6341534  0.0000000\n [6,] 10.0977403  0.6341534  0.0000000\n [7,] 10.0977403  0.6341534  0.0000000\n [8,]  9.3119753  1.4199185  0.0000000\n [9,] 10.0977403  0.6341534  0.0000000\n[10,] 10.0977403  0.6341534  0.0000000\n[11,]  8.7779095  1.9539843  0.0000000\n[12,]  9.3119753  1.4199185  0.0000000\n[13,] 10.0977403  0.6341534  0.0000000\n[14,] 10.0977403  0.6341534  0.0000000\n[15,] 10.0977403  0.6341534  0.0000000\n[16,] 10.0977403  0.6341534  0.0000000\n[17,] 10.0977403  0.6341534  0.0000000\n[18,] 10.0977403  0.6341534  0.0000000\n[19,]  8.5156874  1.5820530  0.6341534\n[20,] 10.0977403  0.6341534  0.0000000\n[21,] 10.0977403  0.6341534  0.0000000\n[22,] 10.0977403  0.6341534  0.0000000\n[23,]  9.3119753  1.4199185  0.0000000\n[24,] 10.0977403  0.6341534  0.0000000\n[25,] 10.0977403  0.6341534  0.0000000\n[26,]  0.0000000  6.5277340  4.2041597\n[27,]  0.0000000  9.9028516  0.8290422\n[28,]  0.0000000  9.9028516  0.8290422\n[29,]  0.0000000 10.0977403  0.6341534\n[30,]  1.5820530  8.3207986  0.8290422\n[31,]  0.0000000  9.9028516  0.8290422\n[32,]  0.0000000  9.2686981  1.4631956\n[33,]  0.0000000  8.3632345  2.3686592\n[34,]  0.0000000 10.0977403  0.6341534\n[35,]  0.0000000  9.9028516  0.8290422\n[36,]  0.0000000  9.9028516  0.8290422\n[37,]  1.5820530  9.1498408  0.0000000\n[38,]  1.5820530  9.1498408  0.0000000\n[39,]  1.5820530  9.1498408  0.0000000\n[40,]  0.0000000 10.0977403  0.6341534\n[41,]  0.0000000  9.9028516  0.8290422\n[42,]  0.0000000 10.0977403  0.6341534\n[43,]  0.5340658  9.5636746  0.6341534\n[44,]  0.0000000  9.9028516  0.8290422\n[45,]  0.0000000 10.0977403  0.6341534\n[46,]  0.0000000  9.1964567  1.5354370\n[47,]  0.0000000 10.7318938  0.0000000\n[48,]  0.0000000  7.1987318  3.5331619\n[49,]  1.3198309  8.7779095  0.6341534\n[50,]  1.5820530  9.1498408  0.0000000\n[51,]  0.0000000  0.0000000 10.7318938\n[52,]  0.0000000  0.0000000 10.7318938\n[53,]  0.0000000  0.0000000 10.7318938\n[54,]  0.0000000  0.0000000 10.7318938\n[55,]  0.0000000  0.0000000 10.7318938\n[56,]  0.0000000  0.0000000 10.7318938\n[57,]  0.0000000  0.0000000 10.7318938\n[58,]  0.0000000  0.0000000 10.7318938\n[59,]  0.0000000  0.0000000 10.7318938\n[60,]  0.0000000  0.0000000 10.7318938\n[61,]  0.0000000  0.8980642  9.8338295\n[62,]  0.0000000  0.0000000 10.7318938\n[63,]  0.0000000  0.0000000 10.7318938\n[64,]  0.0000000  0.8980642  9.8338295\n[65,]  0.0000000  0.0000000 10.7318938\n[66,]  0.0000000  0.0000000 10.7318938\n[67,]  0.0000000  0.0000000 10.7318938\n[68,]  0.0000000  3.1142706  7.6176231\n[69,]  0.0000000  0.8980642  9.8338295\n[70,]  0.0000000  0.0000000 10.7318938\n[71,]  0.0000000  4.0812418  6.6506519\n[72,]  0.0000000  3.4824854  7.2494083\n[73,]  0.0000000  0.0000000 10.7318938\n[74,]  0.0000000  0.0000000 10.7318938\n[75,]  0.0000000  0.0000000 10.7318938\n\n$prob\n            [,1]       [,2]       [,3]\n [1,] 0.81792736 0.18207264 0.00000000\n [2,] 0.94090946 0.05909054 0.00000000\n [3,] 0.94090946 0.05909054 0.00000000\n [4,] 0.94090946 0.05909054 0.00000000\n [5,] 0.94090946 0.05909054 0.00000000\n [6,] 0.94090946 0.05909054 0.00000000\n [7,] 0.94090946 0.05909054 0.00000000\n [8,] 0.86769171 0.13230829 0.00000000\n [9,] 0.94090946 0.05909054 0.00000000\n[10,] 0.94090946 0.05909054 0.00000000\n[11,] 0.81792736 0.18207264 0.00000000\n[12,] 0.86769171 0.13230829 0.00000000\n[13,] 0.94090946 0.05909054 0.00000000\n[14,] 0.94090946 0.05909054 0.00000000\n[15,] 0.94090946 0.05909054 0.00000000\n[16,] 0.94090946 0.05909054 0.00000000\n[17,] 0.94090946 0.05909054 0.00000000\n[18,] 0.94090946 0.05909054 0.00000000\n[19,] 0.79349345 0.14741601 0.05909054\n[20,] 0.94090946 0.05909054 0.00000000\n[21,] 0.94090946 0.05909054 0.00000000\n[22,] 0.94090946 0.05909054 0.00000000\n[23,] 0.86769171 0.13230829 0.00000000\n[24,] 0.94090946 0.05909054 0.00000000\n[25,] 0.94090946 0.05909054 0.00000000\n[26,] 0.00000000 0.60825556 0.39174444\n[27,] 0.00000000 0.92274968 0.07725032\n[28,] 0.00000000 0.92274968 0.07725032\n[29,] 0.00000000 0.94090946 0.05909054\n[30,] 0.14741601 0.77533367 0.07725032\n[31,] 0.00000000 0.92274968 0.07725032\n[32,] 0.00000000 0.86365914 0.13634086\n[33,] 0.00000000 0.77928786 0.22071214\n[34,] 0.00000000 0.94090946 0.05909054\n[35,] 0.00000000 0.92274968 0.07725032\n[36,] 0.00000000 0.92274968 0.07725032\n[37,] 0.14741601 0.85258399 0.00000000\n[38,] 0.14741601 0.85258399 0.00000000\n[39,] 0.14741601 0.85258399 0.00000000\n[40,] 0.00000000 0.94090946 0.05909054\n[41,] 0.00000000 0.92274968 0.07725032\n[42,] 0.00000000 0.94090946 0.05909054\n[43,] 0.04976436 0.89114510 0.05909054\n[44,] 0.00000000 0.92274968 0.07725032\n[45,] 0.00000000 0.94090946 0.05909054\n[46,] 0.00000000 0.85692767 0.14307233\n[47,] 0.00000000 1.00000000 0.00000000\n[48,] 0.00000000 0.67077927 0.32922073\n[49,] 0.12298210 0.81792736 0.05909054\n[50,] 0.14741601 0.85258399 0.00000000\n[51,] 0.00000000 0.00000000 1.00000000\n[52,] 0.00000000 0.00000000 1.00000000\n[53,] 0.00000000 0.00000000 1.00000000\n[54,] 0.00000000 0.00000000 1.00000000\n[55,] 0.00000000 0.00000000 1.00000000\n[56,] 0.00000000 0.00000000 1.00000000\n[57,] 0.00000000 0.00000000 1.00000000\n[58,] 0.00000000 0.00000000 1.00000000\n[59,] 0.00000000 0.00000000 1.00000000\n[60,] 0.00000000 0.00000000 1.00000000\n[61,] 0.00000000 0.08368180 0.91631820\n[62,] 0.00000000 0.00000000 1.00000000\n[63,] 0.00000000 0.00000000 1.00000000\n[64,] 0.00000000 0.08368180 0.91631820\n[65,] 0.00000000 0.00000000 1.00000000\n[66,] 0.00000000 0.00000000 1.00000000\n[67,] 0.00000000 0.00000000 1.00000000\n[68,] 0.00000000 0.29018836 0.70981164\n[69,] 0.00000000 0.08368180 0.91631820\n[70,] 0.00000000 0.00000000 1.00000000\n[71,] 0.00000000 0.38029093 0.61970907\n[72,] 0.00000000 0.32449869 0.67550131\n[73,] 0.00000000 0.00000000 1.00000000\n[74,] 0.00000000 0.00000000 1.00000000\n[75,] 0.00000000 0.00000000 1.00000000\n\n$class\n [1] \"setosa\"     \"setosa\"     \"setosa\"     \"setosa\"     \"setosa\"    \n [6] \"setosa\"     \"setosa\"     \"setosa\"     \"setosa\"     \"setosa\"    \n[11] \"setosa\"     \"setosa\"     \"setosa\"     \"setosa\"     \"setosa\"    \n[16] \"setosa\"     \"setosa\"     \"setosa\"     \"setosa\"     \"setosa\"    \n[21] \"setosa\"     \"setosa\"     \"setosa\"     \"setosa\"     \"setosa\"    \n[26] \"versicolor\" \"versicolor\" \"versicolor\" \"versicolor\" \"versicolor\"\n[31] \"versicolor\" \"versicolor\" \"versicolor\" \"versicolor\" \"versicolor\"\n[36] \"versicolor\" \"versicolor\" \"versicolor\" \"versicolor\" \"versicolor\"\n[41] \"versicolor\" \"versicolor\" \"versicolor\" \"versicolor\" \"versicolor\"\n[46] \"versicolor\" \"versicolor\" \"versicolor\" \"versicolor\" \"versicolor\"\n[51] \"virginica\"  \"virginica\"  \"virginica\"  \"virginica\"  \"virginica\" \n[56] \"virginica\"  \"virginica\"  \"virginica\"  \"virginica\"  \"virginica\" \n[61] \"virginica\"  \"virginica\"  \"virginica\"  \"virginica\"  \"virginica\" \n[66] \"virginica\"  \"virginica\"  \"virginica\"  \"virginica\"  \"virginica\" \n[71] \"virginica\"  \"virginica\"  \"virginica\"  \"virginica\"  \"virginica\" \n\n$importance\nPetal.Length  Petal.Width Sepal.Length  Sepal.Width \n   65.483536    22.138266     6.220378     6.157821 \n\n$terms\nSpecies ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width\nattr(,\"variables\")\nlist(Species, Sepal.Length, Sepal.Width, Petal.Length, Petal.Width)\nattr(,\"factors\")\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSpecies                 0           0            0           0\nSepal.Length            1           0            0           0\nSepal.Width             0           1            0           0\nPetal.Length            0           0            1           0\nPetal.Width             0           0            0           1\nattr(,\"term.labels\")\n[1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\" \nattr(,\"order\")\n[1] 1 1 1 1\nattr(,\"intercept\")\n[1] 1\nattr(,\"response\")\n[1] 1\nattr(,\".Environment\")\n&lt;environment: R_GlobalEnv&gt;\nattr(,\"predvars\")\nlist(Species, Sepal.Length, Sepal.Width, Petal.Length, Petal.Width)\nattr(,\"dataClasses\")\n     Species Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n    \"factor\"    \"numeric\"    \"numeric\"    \"numeric\"    \"numeric\" \n\n$call\nadabag::boosting(formula = Species ~ ., data = iris[train, ], \n    boos = TRUE, mfinal = 10)\n\nattr(,\"vardep.summary\")\n    setosa versicolor  virginica \n        25         25         25 \nattr(,\"class\")\n[1] \"boosting\"\n\n\n\n# Graficamos las variables según su porcentaje de relevancia  \n# El largo de pétalo es la más relevante.\nbarplot(\n  iris.adaboost$imp[order(iris.adaboost$imp, decreasing = TRUE)],\n  ylim = c(0, 100), main = \"Importancia Relativa de las Variables\",\n  col = \"lightblue\"\n)\n\n\n\n\n\n# Ajustamos a los nuevos datos, y vemos el ajuste\niris.predboosting &lt;- adabag::predict.boosting(\n  iris.adaboost, newdata = iris[-train,]\n)\niris.predboosting\n\n$formula\nSpecies ~ .\n\n$votes\n            [,1]       [,2]       [,3]\n [1,] 10.0977403  0.6341534  0.0000000\n [2,] 10.0977403  0.6341534  0.0000000\n [3,] 10.0977403  0.6341534  0.0000000\n [4,]  9.3119753  1.4199185  0.0000000\n [5,] 10.0977403  0.6341534  0.0000000\n [6,]  8.7779095  1.9539843  0.0000000\n [7,]  9.3119753  1.4199185  0.0000000\n [8,] 10.0977403  0.6341534  0.0000000\n [9,] 10.0977403  0.6341534  0.0000000\n[10,]  9.3119753  1.4199185  0.0000000\n[11,] 10.0977403  0.6341534  0.0000000\n[12,] 10.0977403  0.6341534  0.0000000\n[13,] 10.0977403  0.6341534  0.0000000\n[14,] 10.0977403  0.6341534  0.0000000\n[15,] 10.0977403  0.6341534  0.0000000\n[16,] 10.0977403  0.6341534  0.0000000\n[17,]  9.3119753  1.4199185  0.0000000\n[18,] 10.0977403  0.6341534  0.0000000\n[19,] 10.0977403  0.6341534  0.0000000\n[20,] 10.0977403  0.6341534  0.0000000\n[21,] 10.0977403  0.6341534  0.0000000\n[22,] 10.0977403  0.6341534  0.0000000\n[23,] 10.0977403  0.6341534  0.0000000\n[24,] 10.0977403  0.6341534  0.0000000\n[25,] 10.0977403  0.6341534  0.0000000\n[26,]  0.0000000  7.1987318  3.5331619\n[27,]  0.0000000  7.6866452  3.0452486\n[28,]  0.0000000 10.0977403  0.6341534\n[29,]  1.3198309  8.7779095  0.6341534\n[30,]  1.3198309  8.7779095  0.6341534\n[31,]  1.3198309  7.8766259  1.5354370\n[32,]  1.5820530  9.1498408  0.0000000\n[33,]  1.5820530  8.3207986  0.8290422\n[34,]  0.0000000 10.7318938  0.0000000\n[35,]  0.0000000  6.7853616  3.9465322\n[36,]  0.0000000 10.0977403  0.6341534\n[37,]  0.0000000  4.9825254  5.7493683\n[38,]  0.0000000  9.2686981  1.4631956\n[39,]  1.5820530  8.3207986  0.8290422\n[40,]  0.0000000 10.7318938  0.0000000\n[41,]  0.5340658  9.5636746  0.6341534\n[42,]  0.5340658  9.5636746  0.6341534\n[43,]  0.0000000  4.0812418  6.6506519\n[44,]  0.5340658 10.1978280  0.0000000\n[45,]  0.0000000  9.2686981  1.4631956\n[46,]  0.5340658  9.5636746  0.6341534\n[47,]  0.5340658  9.5636746  0.6341534\n[48,]  1.5820530  8.3207986  0.8290422\n[49,]  1.3198309  8.7779095  0.6341534\n[50,]  0.0000000 10.0977403  0.6341534\n[51,]  0.0000000  0.0000000 10.7318938\n[52,]  0.0000000  0.8980642  9.8338295\n[53,]  0.0000000  0.0000000 10.7318938\n[54,]  1.3198309  7.1958565  2.2162064\n[55,]  0.0000000  0.8980642  9.8338295\n[56,]  0.0000000  0.8980642  9.8338295\n[57,]  0.0000000  0.0000000 10.7318938\n[58,]  0.0000000  0.0000000 10.7318938\n[59,]  0.0000000  0.8980642  9.8338295\n[60,]  0.0000000  0.0000000 10.7318938\n[61,]  0.0000000  0.0000000 10.7318938\n[62,]  0.0000000  0.0000000 10.7318938\n[63,]  0.0000000  0.0000000 10.7318938\n[64,]  0.0000000  4.0812418  6.6506519\n[65,]  0.0000000  0.0000000 10.7318938\n[66,]  0.0000000  4.0812418  6.6506519\n[67,]  0.0000000  4.0812418  6.6506519\n[68,]  0.0000000  0.0000000 10.7318938\n[69,]  0.0000000  0.8980642  9.8338295\n[70,]  0.0000000  6.5277340  4.2041597\n[71,]  0.0000000  0.0000000 10.7318938\n[72,]  0.0000000  0.0000000 10.7318938\n[73,]  0.0000000  0.0000000 10.7318938\n[74,]  0.0000000  0.0000000 10.7318938\n[75,]  0.0000000  0.0000000 10.7318938\n\n$prob\n            [,1]       [,2]       [,3]\n [1,] 0.94090946 0.05909054 0.00000000\n [2,] 0.94090946 0.05909054 0.00000000\n [3,] 0.94090946 0.05909054 0.00000000\n [4,] 0.86769171 0.13230829 0.00000000\n [5,] 0.94090946 0.05909054 0.00000000\n [6,] 0.81792736 0.18207264 0.00000000\n [7,] 0.86769171 0.13230829 0.00000000\n [8,] 0.94090946 0.05909054 0.00000000\n [9,] 0.94090946 0.05909054 0.00000000\n[10,] 0.86769171 0.13230829 0.00000000\n[11,] 0.94090946 0.05909054 0.00000000\n[12,] 0.94090946 0.05909054 0.00000000\n[13,] 0.94090946 0.05909054 0.00000000\n[14,] 0.94090946 0.05909054 0.00000000\n[15,] 0.94090946 0.05909054 0.00000000\n[16,] 0.94090946 0.05909054 0.00000000\n[17,] 0.86769171 0.13230829 0.00000000\n[18,] 0.94090946 0.05909054 0.00000000\n[19,] 0.94090946 0.05909054 0.00000000\n[20,] 0.94090946 0.05909054 0.00000000\n[21,] 0.94090946 0.05909054 0.00000000\n[22,] 0.94090946 0.05909054 0.00000000\n[23,] 0.94090946 0.05909054 0.00000000\n[24,] 0.94090946 0.05909054 0.00000000\n[25,] 0.94090946 0.05909054 0.00000000\n[26,] 0.00000000 0.67077927 0.32922073\n[27,] 0.00000000 0.71624313 0.28375687\n[28,] 0.00000000 0.94090946 0.05909054\n[29,] 0.12298210 0.81792736 0.05909054\n[30,] 0.12298210 0.81792736 0.05909054\n[31,] 0.12298210 0.73394557 0.14307233\n[32,] 0.14741601 0.85258399 0.00000000\n[33,] 0.14741601 0.77533367 0.07725032\n[34,] 0.00000000 1.00000000 0.00000000\n[35,] 0.00000000 0.63226134 0.36773866\n[36,] 0.00000000 0.94090946 0.05909054\n[37,] 0.00000000 0.46427271 0.53572729\n[38,] 0.00000000 0.86365914 0.13634086\n[39,] 0.14741601 0.77533367 0.07725032\n[40,] 0.00000000 1.00000000 0.00000000\n[41,] 0.04976436 0.89114510 0.05909054\n[42,] 0.04976436 0.89114510 0.05909054\n[43,] 0.00000000 0.38029093 0.61970907\n[44,] 0.04976436 0.95023564 0.00000000\n[45,] 0.00000000 0.86365914 0.13634086\n[46,] 0.04976436 0.89114510 0.05909054\n[47,] 0.04976436 0.89114510 0.05909054\n[48,] 0.14741601 0.77533367 0.07725032\n[49,] 0.12298210 0.81792736 0.05909054\n[50,] 0.00000000 0.94090946 0.05909054\n[51,] 0.00000000 0.00000000 1.00000000\n[52,] 0.00000000 0.08368180 0.91631820\n[53,] 0.00000000 0.00000000 1.00000000\n[54,] 0.12298210 0.67051134 0.20650655\n[55,] 0.00000000 0.08368180 0.91631820\n[56,] 0.00000000 0.08368180 0.91631820\n[57,] 0.00000000 0.00000000 1.00000000\n[58,] 0.00000000 0.00000000 1.00000000\n[59,] 0.00000000 0.08368180 0.91631820\n[60,] 0.00000000 0.00000000 1.00000000\n[61,] 0.00000000 0.00000000 1.00000000\n[62,] 0.00000000 0.00000000 1.00000000\n[63,] 0.00000000 0.00000000 1.00000000\n[64,] 0.00000000 0.38029093 0.61970907\n[65,] 0.00000000 0.00000000 1.00000000\n[66,] 0.00000000 0.38029093 0.61970907\n[67,] 0.00000000 0.38029093 0.61970907\n[68,] 0.00000000 0.00000000 1.00000000\n[69,] 0.00000000 0.08368180 0.91631820\n[70,] 0.00000000 0.60825556 0.39174444\n[71,] 0.00000000 0.00000000 1.00000000\n[72,] 0.00000000 0.00000000 1.00000000\n[73,] 0.00000000 0.00000000 1.00000000\n[74,] 0.00000000 0.00000000 1.00000000\n[75,] 0.00000000 0.00000000 1.00000000\n\n$class\n [1] \"setosa\"     \"setosa\"     \"setosa\"     \"setosa\"     \"setosa\"    \n [6] \"setosa\"     \"setosa\"     \"setosa\"     \"setosa\"     \"setosa\"    \n[11] \"setosa\"     \"setosa\"     \"setosa\"     \"setosa\"     \"setosa\"    \n[16] \"setosa\"     \"setosa\"     \"setosa\"     \"setosa\"     \"setosa\"    \n[21] \"setosa\"     \"setosa\"     \"setosa\"     \"setosa\"     \"setosa\"    \n[26] \"versicolor\" \"versicolor\" \"versicolor\" \"versicolor\" \"versicolor\"\n[31] \"versicolor\" \"versicolor\" \"versicolor\" \"versicolor\" \"versicolor\"\n[36] \"versicolor\" \"virginica\"  \"versicolor\" \"versicolor\" \"versicolor\"\n[41] \"versicolor\" \"versicolor\" \"virginica\"  \"versicolor\" \"versicolor\"\n[46] \"versicolor\" \"versicolor\" \"versicolor\" \"versicolor\" \"versicolor\"\n[51] \"virginica\"  \"virginica\"  \"virginica\"  \"versicolor\" \"virginica\" \n[56] \"virginica\"  \"virginica\"  \"virginica\"  \"virginica\"  \"virginica\" \n[61] \"virginica\"  \"virginica\"  \"virginica\"  \"virginica\"  \"virginica\" \n[66] \"virginica\"  \"virginica\"  \"virginica\"  \"virginica\"  \"versicolor\"\n[71] \"virginica\"  \"virginica\"  \"virginica\"  \"virginica\"  \"virginica\" \n\n$confusion\n               Observed Class\nPredicted Class setosa versicolor virginica\n     setosa         25          0         0\n     versicolor      0         23         2\n     virginica       0          2        23\n\n$error\n[1] 0.05333333"
  },
  {
    "objectID": "apuntes/clase-11.html#máquinas-de-soporte-vectorial-svm",
    "href": "apuntes/clase-11.html#máquinas-de-soporte-vectorial-svm",
    "title": "Apuntes de clase",
    "section": "Máquinas de Soporte Vectorial (SVM)",
    "text": "Máquinas de Soporte Vectorial (SVM)\n\nLa idea principal es usar un hiperplano como frontera para realizar clasificaciones.\n\n\nClasificación usando hiperplanos separadores\n\nConsidere el conjunto de observaciones como una matriz \\(X\\) cuyas filas son las observaciones; y, las columnas representan a los predictores.\nTrataremos primero el caso de clasificación binaria, considerando que las observaciones pertenecen a dos clases, \\(1\\) y \\(-1\\).\nAsí, un hiperplano separador tendrá la propiedad \\(y_i \\left( \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_p x_{ip} \\right) &gt; 0\\) para todo \\(i = 1, \\dots, n\\).\n\\(x^*\\) es clasificado basado en el signo de \\(f(x^*) = \\beta_0 + \\beta_1 x_{1}^* + \\cdots + \\beta_p x_{p}^*\\):\n\nSi \\(f(x^*)\\) está lejos de cero, entonces \\(x\\) cae lejos del hiperplano, por lo que podemos estar confiados acerca de la clasificación.\nSi \\(f(x^*)\\) está cerca de cero, entonces \\(x\\) cae cerca del hiperplano, por lo que podemos estar menos confiados acerca de la clasificación.\n\n\n\n\nMaximal margin classifier\n\nEl hiperplano de frontera máxima, también conocido como el hiperplano separador óptimo, es el hiperplano separador que se encuentra más lejos de las observaciones de entrenamiento.\nEs posible que usar el hiperplano de frontera máxima genere un modelo sobreajustado.\n\n\nProposition 2 Se espera que el clasificador que tenga una mayor frontera para los datos de entrenamiento tambi´en lo tenga para los datos de preuba.\n\n\nEl hiperplano de fontera máxima es sensible respecto a los outliers.\nLas observaciones/vectores que son equidistantes al hiperplano de frontera máxima son conocida como los vectores de soporte.\n\n\nProposition 3 Una propiedad muy importante es que el hiperplano de frontera máxima depende directamente de estos vectores de soporte, pero no de las otras observaciones.\n\n\n\nConstrucción del clasificador de frontera máxima\n\nTenemos \\(n\\) observaciones de entrenamiento \\(x_1, \\dots, x_n \\in \\mathbb{R}^p\\), con etiquetas de clase \\(y_1, \\dots, y_n \\in \\left\\{ 1, -1 \\right\\}\\) .\nEl hiperplano de frontera máxima\n\n(LLEEENAAAAR)\n\n\nCasos de no separación\n(LLEEENAAAAR, also include la diapo comentarios generales)"
  },
  {
    "objectID": "apuntes/clase-11.html#clasificador-de-soporte-vectorial",
    "href": "apuntes/clase-11.html#clasificador-de-soporte-vectorial",
    "title": "Apuntes de clase",
    "section": "Clasificador de soporte vectorial",
    "text": "Clasificador de soporte vectorial\n\nEstos no son las máquinas de soporte vectorial; sino, son una versión con frontera relajada del hiperplano de frontera máxima.\nEn vez de buscar la mayor frontera, suavizamos la frontera, de manera que se permite que algunas observaciones se encuentre en el lado incorrecto de la frontera e incluso del hiperplano.\n\n(LLEENAAAR)"
  },
  {
    "objectID": "apuntes/clase-11.html#kernels",
    "href": "apuntes/clase-11.html#kernels",
    "title": "Apuntes de clase",
    "section": "Kernels",
    "text": "Kernels\n\nComo no siempre existe al menos un hiperplano separador (perfecto), los datos pueden ser embedded into un espacio de mayor dimensión, vía un kernel (es una función). Esto con el fin de que, en aquel espacio de mayor dimensión, sí exista un hiperplano separador.\nCuando se emplea algún kernel para un clasificador de soporte vectorial, el modelo se denomina Máquina de Soporte Vectorial."
  },
  {
    "objectID": "apuntes/clase-11.html#svm-para-más-de-dos-categorías",
    "href": "apuntes/clase-11.html#svm-para-más-de-dos-categorías",
    "title": "Apuntes de clase",
    "section": "SVM para más de dos categorías",
    "text": "SVM para más de dos categorías\n\nPara clasificación no binaria, se suelen usar dos alternativas:\n\nClasificación uno contra uno:\nClasificación uno contra todos:"
  },
  {
    "objectID": "apuntes/clase-11.html#ejemplo-práctico-svm",
    "href": "apuntes/clase-11.html#ejemplo-práctico-svm",
    "title": "Apuntes de clase",
    "section": "Ejemplo práctico: SVM",
    "text": "Ejemplo práctico: SVM\n\nClasificación binaria\n\n# Support Vector Machines\nlibrary(e1071)\n\nWarning: package 'e1071' was built under R version 4.1.3\n\n\n\n# Generación de datos con fronteras no lineales\nset.seed(1)\nx &lt;- matrix(rnorm(200*2), ncol=2)\nx[1:100,] = x[1:100,] + 2\nx[101:150,] = x[101:150,] - 2\n\ny &lt;- c(rep(1,150), rep(2,50))\ndat &lt;- data.frame(x = x, y = as.factor(y))\n\n# El ploteo de los datos muestra claramente que las fronteras no son lineales\nplot(x, col = y)\n\n\n\n\n\n# Se dividen los datos aleatoriamente en grupos de entrenamiento y prueba\ntrain &lt;- sample(200, 100)\n\n# Se ajusta el svm usando un kernel radial y gamma = 1\nsvmfit &lt;- e1071::svm(\n  y ~., data = dat[train,], \n  kernel = \"radial\", gamma = 1, cost = 1\n)\nplot(svmfit, dat[train,])\n\n\n\n# Obtener información adicional\nsummary(svmfit)\n\n\nCall:\nsvm(formula = y ~ ., data = dat[train, ], kernel = \"radial\", gamma = 1, \n    cost = 1)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  radial \n       cost:  1 \n\nNumber of Support Vectors:  31\n\n ( 16 15 )\n\n\nNumber of Classes:  2 \n\nLevels: \n 1 2\n\n\n\n# Incremento del costo para reducir el número de errores de entrenamiento\nsvmfit = e1071::svm(\n  y ~., data = dat[train,], \n  kernel = \"radial\", gamma = 1, cost = 1e5\n)\nplot(svmfit,dat[train,])\n\n\n\n\n\n# Es posible realizar validación cruzada usando tune() para seleccionar la mejor opción de\n# gamma y costo con un kernel radial\nset.seed(1)\ntune.out = e1071::tune(\n  svm, y ~., data = dat[train,], \n  kernel = \"radial\", \n  ranges = list(\n    cost = c(0.1, 1, 10, 100, 1000),\n    gamma = c(0.5, 1, 2 ,3, 4)\n  )\n)\n# La mejor elección resultó ser con costo=1 y gamma=0.5.\n# Se realiza la predicción con el conjunto de entrenamiento\nsummary(tune.out)\n\n\nParameter tuning of 'svm':\n\n- sampling method: 10-fold cross validation \n\n- best parameters:\n cost gamma\n    1   0.5\n\n- best performance: 0.07 \n\n- Detailed performance results:\n    cost gamma error dispersion\n1  1e-01   0.5  0.26 0.15776213\n2  1e+00   0.5  0.07 0.08232726\n3  1e+01   0.5  0.07 0.08232726\n4  1e+02   0.5  0.14 0.15055453\n5  1e+03   0.5  0.11 0.07378648\n6  1e-01   1.0  0.22 0.16193277\n7  1e+00   1.0  0.07 0.08232726\n8  1e+01   1.0  0.09 0.07378648\n9  1e+02   1.0  0.12 0.12292726\n10 1e+03   1.0  0.11 0.11005049\n11 1e-01   2.0  0.27 0.15670212\n12 1e+00   2.0  0.07 0.08232726\n13 1e+01   2.0  0.11 0.07378648\n14 1e+02   2.0  0.12 0.13165612\n15 1e+03   2.0  0.16 0.13498971\n16 1e-01   3.0  0.27 0.15670212\n17 1e+00   3.0  0.07 0.08232726\n18 1e+01   3.0  0.08 0.07888106\n19 1e+02   3.0  0.13 0.14181365\n20 1e+03   3.0  0.15 0.13540064\n21 1e-01   4.0  0.27 0.15670212\n22 1e+00   4.0  0.07 0.08232726\n23 1e+01   4.0  0.09 0.07378648\n24 1e+02   4.0  0.13 0.14181365\n25 1e+03   4.0  0.15 0.13540064\n\ntable(\n  true = dat[-train, \"y\"], \n  pred = predict(tune.out$best.model,newx=dat[-train,])\n)\n\n    pred\ntrue  1  2\n   1 54 23\n   2 17  6\n\n\n\n\nClasificación no binaria\n\nset.seed(1)\nx &lt;- rbind(x, matrix(rnorm(50*2), ncol=2))\ny &lt;- c(y, rep(0, 50))\nx[y == 0,2]= x [y == 0,2] + 2\n\ndat &lt;- data.frame(x = x, y = as.factor(y))\npar(mfrow = c(1, 1))\nplot(x, col = (y+1))\n\n\n\nsvmfit = svm(\n  y ~., data = dat, kernel = \"radial\", cost = 10, gamma = 1\n)\nplot(svmfit, dat)\n\n\n\n\n\nProposition 4 Fijada la data sobre la cual se realiza validación cruzada, el parámetro costo; mientras que el parámetro gamma"
  },
  {
    "objectID": "apuntes/clase-12.html#aprendizaje-no-supervisado",
    "href": "apuntes/clase-12.html#aprendizaje-no-supervisado",
    "title": "Apuntes de clase",
    "section": "Aprendizaje No Supervisado",
    "text": "Aprendizaje No Supervisado\n\nComo no hay variable response, la evaluación de los modelos es más subjetiva. Por ello, para modelos no supervisados, es de mayor importancia el field expertise.\n\n\nReglas de asociación\n\nSon útiles para descubrir relaciones de interés en los datos.\nBase de datos de transacciones:\n\n\n\n\nTransaction ID\nItems\n\n\n\n\n1\nA, B, C\n\n\n2\nA, C\n\n\n3\nB, C, D, E\n\n\n\n\nRegla de asociación:\n\nEs una implicancia de la forma \\(X \\Rightarrow Y\\), donde \\(X\\subset I, Y\\subset I\\) y \\(X\\cap Y = \\emptyset\\) .\n\\(I\\) es el conjunto de todos los items en la base de datos de transaccciones.\n\\(X\\) se denomina regla antecedente; Y, regla consecuente.\nDe la tabla anterior, se tiene la siguiente regla de asociación \\(\\left\\{ A \\right\\} \\rightarrow \\left\\{ B \\right\\}\\) .\n\n\n\nProposition 1 Las reglas de asociación no deben ser interpretadas como verdades universales; simpemente son caracterizadas por medidas estadísticas que cuantifican la fuerza de la asociación.\n\n\n\nConfianza y soporte\n\nItemset: Colección de uno o más items.\nSoporte: Frecuencia con la que ocurre un itemset.\n\nEjemplo de soporte: \\(s\\left( \\left\\{ B, C \\right\\} \\right) = \\dfrac{2}{3}\\) .\n\nRegla de asociación: Es una relación entre dos itemsets. \\(X\\rightarrow Y\\) representa el patrón de que cuando X ocurre; Y también.\nMedidas de evaluación de una regla \\(X \\rightarrow Y\\):\n\nSoporte:\n\n\\(s\\left( X \\rightarrow Y \\right) = s \\left( X \\cup Y \\right)\\) \\(= \\dfrac{\\text{ \\# de trans. que contienen a (X U Y) }}{\\text{ \\# total de transacciones }}\\)\n\nConfianza:\n\n\\(c \\left( X \\rightarrow Y \\right) = P(Y \\mid X ) = \\dfrac{s\\left( X \\cup Y \\right)}{s(X)}\\)\n\nLift:\n\n\\(lift \\left( X \\rightarrow Y \\right) = \\dfrac{s\\left( X \\cup Y \\right)}{s\\left( X \\right) s\\left( Y \\right)}\\)\nlift = 1 significa que los items antecendentes y consecuentes son independientes.\nlift &gt; 1 implica que existe mayor asociación entre los itemsets.\nUn valor de lift cercano 0 implica que los itemsets son sustitutorios; es decir, puede reemplazarse el uno por el otro."
  },
  {
    "objectID": "apuntes/clase-14.html#conglomerados",
    "href": "apuntes/clase-14.html#conglomerados",
    "title": "Apuntes de clase",
    "section": "Conglomerados",
    "text": "Conglomerados\n\nGoal: Agrupar observaciones, de modo que cierta distancia entre observaciones, por grupo, sea minimizada.\n\n\nIntro\n\nClustering es una herramienta para anáisis descriptivo.\nEl objetivo principal es encontrar subgrupos homogéneos.\nSe busca que observaciones del mismo grupo sean similares; mientras que, de grupos distintos, sean diferentes.\nClustering no es clasificar, pues no hay variables dependiente.\nPor lo general, no es adecuado comparar cluster algorithms diferentes.\nClustering también puede servir para reducir el tamaño de un conjunto de datos.\n\n\n\nTipos de Análisis Cluster\n\nCluster jerárquico\nCluster Bayesiano\nCluster particional\n\n\n\nProblemas comunes\n\nProblemas principales:\n\nAglutinación: Ocurre cuando un objeto puede pertenecer a más de un cluster.\nPara mitigar esta situación, se usan algoritmos difusos.\nDisección: Ocurre cuando la población asociada a los datos no contiene clusters significativos.\n\nPresencia de outliers.\nDatos no normalizados (no tener misma escala).\n\n\n\nSimilaridad, distancia y proximidad\n\nMedida o función de similaridad:\n\nCuantifica similaridad entre objetos.\nA mayor valor, objetos más similares.\nUsualmente es un valor entre 0 (no similaridad) y 1 (objetos completamente similares.)\n\nDistancia (medida de disimilaridad):\n\nIndica qué tan diferentes son los datos.\nMenor valor, objeto smás parecidos.\nMínimo valor de disimilaridad suele ser 0.\nRango suele ser \\(\\left[0; 1\\right]\\) o \\(\\left[0; \\infty \\right(\\)\n\nProximidad: Se refiere usualmente tanto a la similaridad como a la disimilaridad.\n\n\n\nMatriz de distancias (disimilaridad)\n\nMatriz \\(n\\times n\\) donde se registran las distancias \\(d \\left( i, j \\right)\\) .\nUsualmente simétrica (no siempre).\n\n\nLas funciones de distancias son usualmente distintas para variables de tipo cuantitativa o categórica.\n\n\n\nDistancias con datos numéricos:\n\nDistancias de Minkowski: Sí es una métrica matemática. \\[\nd \\left( i, j \\right) = \\left( \\sum_{k=1}^{M} | x_{ik} - x_{jk} |^p \\right)^{\\dfrac{1}{p}},\n\\]\n\ndonde los datos pertenecen a \\(\\mathbb{R}^M\\), y \\(p\\) es el orden de la distancia (\\(L_p norm\\) ).\n\nDistancia de Canberra\n\n\n\nDistancias con datos mixtos\n\nDistancia de Gower (el profesor no recomienda usar esta distancia, dice que no tiene un sustento robusto, a menos que no tengamos una mejor opción)\nComo alternativa, el profesor recomienda usar un MD Cluster."
  },
  {
    "objectID": "apuntes/clase-14.html#ejemplo-práctico",
    "href": "apuntes/clase-14.html#ejemplo-práctico",
    "title": "Apuntes de clase",
    "section": "Ejemplo práctico",
    "text": "Ejemplo práctico\n\nDistancias\n\nDatos numéricos\n\n# Base R\nset.seed(666)\n\nx &lt;- matrix(rnorm(20), nrow = 5)\ndist(x)\n\n         1        2        3        4\n2 4.700411                           \n3 2.344002 3.615848                  \n4 4.833851 1.521420 3.768408         \n5 3.777519 4.809694 2.184675 5.137616\n\ndist(x, method = \"manhattan\", diag = TRUE)\n\n         1        2        3        4        5\n1 0.000000                                    \n2 8.180026 0.000000                           \n3 4.375477 6.021439 0.000000                  \n4 8.202318 1.990697 6.885186 0.000000         \n5 6.648528 7.471869 3.794025 9.218155 0.000000\n\ndist(x, method = \"maximum\", upper = TRUE)\n\n         1        2        3        4        5\n1          3.920273 1.560916 3.870199 2.970185\n2 3.920273          2.634884 1.440753 4.231229\n3 1.560916 2.634884          2.584809 1.861740\n4 3.870199 1.440753 2.584809          4.245042\n5 2.970185 4.231229 1.861740 4.245042         \n\n\nLa función dist solo funciona con datos numéricos.\n\n\nDatos numéricos y categóricos\n\nlibrary(cluster)\n\nWarning: package 'cluster' was built under R version 4.1.3\n\ndaisy(x)\n\nDissimilarities :\n         1        2        3        4\n2 4.700411                           \n3 2.344002 3.615848                  \n4 4.833851 1.521420 3.768408         \n5 3.777519 4.809694 2.184675 5.137616\n\nMetric :  euclidean \nNumber of objects : 5\n\nx &lt;- cbind(rnorm(10), sample(1:3,10, replace = TRUE))\nx &lt;- as.data.frame(x)\nx[,2] &lt;- as.factor(x[,2])\ndaisy(x)\n\nDissimilarities :\n             1           2           3           4           5           6\n2  0.080941937                                                            \n3  0.323301658 0.404243594                                                \n4  0.062576004 0.143517940 0.260725654                                    \n5  0.619146631 0.700088568 0.704155027 0.556570628                        \n6  0.630276188 0.549334251 0.953577846 0.692852192 0.249422819            \n7  0.071644082 0.009297854 0.394945740 0.134220086 0.690790714 0.558632106\n8  0.676698342 0.595756406 1.000000000 0.739274346 0.295844973 0.046422154\n9  0.061087567 0.019854370 0.384389225 0.123663570 0.680234198 0.569188621\n10 0.107313820 0.026371883 0.430615478 0.169889823 0.726460451 0.522962368\n             7           8           9\n2                                     \n3                                     \n4                                     \n5                                     \n6                                     \n7                                     \n8  0.605054260                        \n9  0.010556516 0.615610775            \n10 0.035669737 0.569384522 0.046226253\n\nMetric :  mixed ;  Types = I, N \nNumber of objects : 10\n\n\n\n\n\nMétodos de Particionamiento\n\nLos datos son particionados en un número \\(K\\) fijado de conglomerados.\nLuego, iterativamente se va reasignando las observaciones a los conglomerados hasta que algún criterio de parada (función a optimizar) se satisface, o, se cumplió el número máximo fijado de iteraciones.\n\n\nMétodo K-means\n\nSe recomienda normalizar las variables antes de implementar este algoritmo.\n\n\nCentroide es un representate del cluster al que pertenece.\nMetodología:\n\nAsigna aleatoriamente un número, de 1 a \\(K\\) , a cada una de las observaciones.\nIterar hasta que la asignación de los cluster deje de cambiar\n\nPara cada uno de los \\(K\\) cluster, calcular el centroide. El k-ésimo centroide es el vector con las \\(p\\) medias de las variables para las observaciones en el k-ésimo cluster.\nAsignar cada observación al cluster donde el entroide esté más cerca (donde cercanía se encuentra definida por la distancia Euclidiana).\n\n\nAl inicio, el centroide es una observación; para las siguientes iteraciones, most likely ya no.\nEl algoritmo anterior garantiza que el valor del objetivo asociado a este modelo decrezca en cada etapa.\nEste algoritmo es computacionalmente rápido; usualmente se fija 20 o 25 como el máximo de iteraciones.\nCambiando la métrica usada para la función objetivo, se pueden obtener diferentes clusters.\nDesventajas:\n\nUsualmente no se encuentra un óptimo global, solo óptimo local; pero esto puede mitigarse permitiendo un mayor número de iteraciones del loop del algoritmo.\nEs sensible a outliers, debido al uso de medias; pero se puede mitigar vía el uso de mediodes (usar mediana en vez de media), empleando la métrica \\(L_1\\) en vez de \\(L_2\\) también."
  },
  {
    "objectID": "apuntes/clase-14.html#ejemplo-práctico-k-means",
    "href": "apuntes/clase-14.html#ejemplo-práctico-k-means",
    "title": "Apuntes de clase",
    "section": "Ejemplo práctico: K-means",
    "text": "Ejemplo práctico: K-means\n\nDatos no normalizados\n\nset.seed(007)\nx &lt;- cbind(\n  rnorm(100,1000,100), c(rnorm(50), rnorm(50,10,1))\n)\nplot(x, pch = 16)\n\n\n\n#k-means datos originales\nres &lt;- kmeans(x, 2)\nplot(x, col = c(\"green\",\"red\")[res$cluster], pch = 16)\n\n\n\n\n\n\nDatos normalizados\n\n# Estandarización\nxs &lt;- scale(x)\nplot(xs, pch = 16)\n\n\n\n#k-means datos estandarizados\nres &lt;- kmeans(xs, 2)\nplot(x, col = c(\"green\",\"red\")[res$cluster], pch = 16)"
  },
  {
    "objectID": "apuntes/clase-14.html#ejemplo-práctico-k-means-1",
    "href": "apuntes/clase-14.html#ejemplo-práctico-k-means-1",
    "title": "Apuntes de clase",
    "section": "Ejemplo práctico: K-means",
    "text": "Ejemplo práctico: K-means\n\nlibrary(foreign)\ndistritos=read.spss(\n  \"../datos/distritos.sav\",\n  use.value.labels = TRUE, max.value.labels = Inf, \n  to.data.frame = TRUE\n)\n\nre-encoding from CP1252\n\ncolnames(distritos) &lt;- tolower(colnames(distritos))\nnombres  = distritos[,1]\ndistritos = distritos[,-1]\nrownames(distritos) = nombres\nhead(distritos)\n\n                               ocu_vivi pobpjov sinelect sinagua pea1619\nAte                                1.15     5.3    27.60   51.10     3.9\nBarranco                           1.09     4.5     1.59    8.32     0.8\nBreña                              1.08     4.4     2.20   23.15     0.9\nCarabayllo                         1.10     5.1    30.13   38.09     4.5\nComas                              1.20     5.9    10.92   24.27     3.8\nChorrillos                         1.15     5.5    16.77   37.11     3.2\n                               pocprin peam15\nAte                                1.1  63.48\nBarranco                           3.9  33.48\nBreña                              4.0  37.89\nCarabayllo                        12.6  63.65\nComas                              9.4  60.37\nChorrillos                        10.6  18.78\n\nres &lt;- kmeans(scale(distritos), 2)\nres\n\nK-means clustering with 2 clusters of sizes 15, 19\n\nCluster means:\n    ocu_vivi    pobpjov   sinelect    sinagua    pea1619    pocprin     peam15\n1  0.6635316  0.7536352  0.8957621  0.7981240  0.9978041  0.7677947  0.8420238\n2 -0.5238407 -0.5949751 -0.7071806 -0.6300979 -0.7877401 -0.6061537 -0.6647556\n\nClustering vector:\nAte                            Barranco                       \n                             1                              2 \nBreña                          Carabayllo                     \n                             2                              1 \nComas                          Chorrillos                     \n                             1                              1 \nAgustino                       Independencia                  \n                             1                              1 \nJesús María                    La Victoria                    \n                             2                              2 \nLima                           Lince                          \n                             2                              2 \nLos Olivos                     Lurigancho-Chosica             \n                             1                              1 \nMagdalena del Mar              Pueblo Libre                   \n                             2                              2 \nMiraflores                     Rímac                          \n                             2                              2 \nSan Borja                      San Isidro                     \n                             2                              2 \nSan Juan de Lurigancho         San Juan de Miraflores         \n                             1                              1 \nSan Luis                       San Martin de Porres           \n                             2                              1 \nSan Miguel                     Santa Anita                    \n                             2                              1 \nSurco                          Surquillo                      \n                             2                              2 \nVilla el Salvador              Villa María del Triunfo        \n                             1                              2 \nCallao                         Carmen de la Legua             \n                             1                              1 \nLa Perla                       La Punta                       \n                             2                              2 \n\nWithin cluster sum of squares by cluster:\n[1] 67.25282 36.46842\n (between_SS / total_SS =  55.1 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\n\nDeterminar número de conglomerados\n\n# Suma de cuadrados dentro de clusters\nwss &lt;- numeric()\nfor (h in 2:10) {\n  # Importante el argumento nstart ene este caso,\n  # para que los elementos de wss decrezcan\n  b &lt;- kmeans(scale(distritos),h, nstart = 50)\n  wss[h-1] &lt;- b$tot.withinss\n}\nplot(2:10, wss, type = \"b\")\n\n\n\n# Silueta\ndiss.distritos = daisy(scale(distritos))\npar(mfrow = c(1,3))\nfor(h in 2:4) {\n  res &lt;- kmeans(scale(distritos), h)\n  plot(silhouette(\n    # Vector que indica a qué conglomerado pertenece cada observación\n    res$cluster,\n    # Matriz de disimilaridad (que usaste en K-means)\n    diss.distritos\n  ))\n\n  # En caso se tengan demasiadas observaciones\n  print(summary(silhouette(res$cluster, diss.distritos)))\n}\n\nSilhouette of 34 units in 2 clusters from silhouette.default(x = res$cluster, dist = diss.distritos) :\n Cluster sizes and average silhouette widths:\n       19        15 \n0.5706235 0.3463122 \nIndividual silhouette widths:\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.00153  0.39015  0.46612  0.47166  0.63262  0.70602 \n\n\nSilhouette of 34 units in 3 clusters from silhouette.default(x = res$cluster, dist = diss.distritos) :\n Cluster sizes and average silhouette widths:\n        12          9         13 \n0.51025714 0.09804962 0.28793020 \nIndividual silhouette widths:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-0.1829  0.1781  0.3525  0.3161  0.4641  0.6413 \n\n\n\n\n\nSilhouette of 34 units in 4 clusters from silhouette.default(x = res$cluster, dist = diss.distritos) :\n Cluster sizes and average silhouette widths:\n        9         9         5        11 \n0.2352400 0.2527997 0.4144512 0.4409134 \nIndividual silhouette widths:\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.02978  0.22634  0.35484  0.33278  0.44000  0.59277 \n\npar(mfrow = c(1,1))\n\n\nEl valor Mean de la sección Individual silhouette widths del resultado de summary, es el indicador que usaríamos para saber qué tan bien se realizó el clustering.\n\n\n# Criterio de Calinski-Harabasz\nlibrary(fpc)\n\nWarning: package 'fpc' was built under R version 4.1.3\n\nch &lt;- numeric()\nfor (h in 2:10) {\n  res &lt;- kmeans(scale(distritos),h,nstart = 100)\n  ch[h-1] &lt;- calinhara(scale(distritos),res$cluster)\n}\nplot(\n  2:10,ch,type=\"b\",xlab=\"k\",\n  ylab=\"Criterio de Calinski-Harabasz\"\n)\n\n\n\n# Criterio de Calinski-Harabasz\nkmeansruns(scale(distritos),criterion=\"ch\")\n\nK-means clustering with 2 clusters of sizes 15, 19\n\nCluster means:\n    ocu_vivi    pobpjov   sinelect    sinagua    pea1619    pocprin     peam15\n1  0.6635316  0.7536352  0.8957621  0.7981240  0.9978041  0.7677947  0.8420238\n2 -0.5238407 -0.5949751 -0.7071806 -0.6300979 -0.7877401 -0.6061537 -0.6647556\n\nClustering vector:\nAte                            Barranco                       \n                             1                              2 \nBreña                          Carabayllo                     \n                             2                              1 \nComas                          Chorrillos                     \n                             1                              1 \nAgustino                       Independencia                  \n                             1                              1 \nJesús María                    La Victoria                    \n                             2                              2 \nLima                           Lince                          \n                             2                              2 \nLos Olivos                     Lurigancho-Chosica             \n                             1                              1 \nMagdalena del Mar              Pueblo Libre                   \n                             2                              2 \nMiraflores                     Rímac                          \n                             2                              2 \nSan Borja                      San Isidro                     \n                             2                              2 \nSan Juan de Lurigancho         San Juan de Miraflores         \n                             1                              1 \nSan Luis                       San Martin de Porres           \n                             2                              1 \nSan Miguel                     Santa Anita                    \n                             2                              1 \nSurco                          Surquillo                      \n                             2                              2 \nVilla el Salvador              Villa María del Triunfo        \n                             1                              2 \nCallao                         Carmen de la Legua             \n                             1                              1 \nLa Perla                       La Punta                       \n                             2                              2 \n\nWithin cluster sum of squares by cluster:\n[1] 67.25282 36.46842\n (between_SS / total_SS =  55.1 %)\n\nAvailable components:\n\n [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n [6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"       \"crit\"        \n[11] \"bestk\"       \n\n# Criterio anchura de silueta\nkmeansruns(scale(distritos),criterion=\"asw\")\n\nK-means clustering with 2 clusters of sizes 19, 15\n\nCluster means:\n    ocu_vivi    pobpjov   sinelect    sinagua    pea1619    pocprin     peam15\n1 -0.5238407 -0.5949751 -0.7071806 -0.6300979 -0.7877401 -0.6061537 -0.6647556\n2  0.6635316  0.7536352  0.8957621  0.7981240  0.9978041  0.7677947  0.8420238\n\nClustering vector:\nAte                            Barranco                       \n                             2                              1 \nBreña                          Carabayllo                     \n                             1                              2 \nComas                          Chorrillos                     \n                             2                              2 \nAgustino                       Independencia                  \n                             2                              2 \nJesús María                    La Victoria                    \n                             1                              1 \nLima                           Lince                          \n                             1                              1 \nLos Olivos                     Lurigancho-Chosica             \n                             2                              2 \nMagdalena del Mar              Pueblo Libre                   \n                             1                              1 \nMiraflores                     Rímac                          \n                             1                              1 \nSan Borja                      San Isidro                     \n                             1                              1 \nSan Juan de Lurigancho         San Juan de Miraflores         \n                             2                              2 \nSan Luis                       San Martin de Porres           \n                             1                              2 \nSan Miguel                     Santa Anita                    \n                             1                              2 \nSurco                          Surquillo                      \n                             1                              1 \nVilla el Salvador              Villa María del Triunfo        \n                             2                              1 \nCallao                         Carmen de la Legua             \n                             2                              2 \nLa Perla                       La Punta                       \n                             1                              1 \n\nWithin cluster sum of squares by cluster:\n[1] 36.46842 67.25282\n (between_SS / total_SS =  55.1 %)\n\nAvailable components:\n\n [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n [6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"       \"crit\"        \n[11] \"bestk\"       \n\nres=kmeans(scale(distritos),2, nstart = 50)\nplotcluster(distritos,res$cluster)\n\n\n\nclusplot(\n  distritos,res$cluster, color = TRUE,\n  shade = TRUE, labels =2,lines=0,\n  main =\"Gráfico de Conglomerados\"\n)\n\n\n\n\nK=2 sería el valor idóneo para el número de clusters, según este criterio.\n\n\nPerfilado y caracterización de clusters\n\n# Adicionar los cluster a la base de datos\ndistritos.new&lt;-cbind(distritos,res$cluster)\ncolnames(distritos.new)&lt;-c(colnames(distritos.new[,-length(distritos.new)]), \"cluster.km\")\nhead(distritos.new)\n\n                               ocu_vivi pobpjov sinelect sinagua pea1619\nAte                                1.15     5.3    27.60   51.10     3.9\nBarranco                           1.09     4.5     1.59    8.32     0.8\nBreña                              1.08     4.4     2.20   23.15     0.9\nCarabayllo                         1.10     5.1    30.13   38.09     4.5\nComas                              1.20     5.9    10.92   24.27     3.8\nChorrillos                         1.15     5.5    16.77   37.11     3.2\n                               pocprin peam15 cluster.km\nAte                                1.1  63.48          2\nBarranco                           3.9  33.48          1\nBreña                              4.0  37.89          1\nCarabayllo                        12.6  63.65          2\nComas                              9.4  60.37          2\nChorrillos                        10.6  18.78          2\n\n# Tabla de medias\nmed&lt;-aggregate(x = distritos.new[,1:7],by = list(distritos.new$cluster.km),FUN = mean)\nmed\n\n  Group.1 ocu_vivi  pobpjov  sinelect  sinagua   pea1619  pocprin   peam15\n1       1 1.080526 4.631579  2.447895 15.80895 0.8736842 5.057895 32.16526\n2       2 1.160000 5.520000 18.856000 37.02400 3.7946667 9.780000 56.84000\n\n# Describir variables\npar(mfrow=c(2,4))\nfor (i in 1:length(distritos.new[,1:7])) {\nboxplot(distritos.new[,i]~distritos.new$cluster.km, main=names(distritos.new[i]), type=\"l\")\n}\npar(mfrow=c(1,1))\n\n\n\n\n\nPara el caso de dos conglomerados, la conglomeración es más adecuada cuando el ratio between_SS / total_SS es muy cercano a \\(1\\) .\n\n\nres$betweenss / (res$betweenss + res$tot.withins)\n\n[1] 0.5509903\n\n\n\n\nSilueta\n\nRepresenta qué tan bien está agrupada la observación, en su conglomerado respectivo.\nMayor silueta, mayor similitud de la observación a las observaciones en su conglomerado.\nEstá entre \\(-1\\) y \\(1\\), pero un valor negativo indica que la observación no pertenece a un cluster adecuado.\nEl ancho de silueta de la i-ésima observación es definida por \\(\\text{ sil }_i = \\dfrac{b_i - a_i}{max \\left( a_i, b_i \\right)}\\) .\nDefiniciones:\n\n\\(a_i\\) : Distancia promedio entre la observación \\(i\\) y todas las otras observaciones en el mismo conglomerado que i.\n\\(b_i\\): Denota la mínima distancia promedio de \\(i\\) a las observaciones que están en otros conglomerados.\n\nSilueta con valor 1 indica que esa observación está perfectamente asignada en su cluster respectivo.\nLa medida de silueta se define como \\(\\bar{s} = \\dfrac{1}{n} \\displaystyle{ \\sum_{1}^{n} \\text{ sil}_i }\\)\n\n\n\nCriterio de Calinski-Harabasz\n\nValor por calcular:\n\n\\[\n\\dfrac{(n-k) \\times \\text{ Suma de cuadrados entre los grupos }}{(k-1) \\times \\text{ Suma de cuadrados dentro de los grupos }}\n\\]\n\nMayor valor, mejor agrupación de los objetos en conglomerados.\n\n\nAmbos criterios, silueta y Calinski-Harabasz, sirven para cualquier tipo de conglomeración, no solo para K-means."
  },
  {
    "objectID": "apuntes/clase-14.html#notas-finales",
    "href": "apuntes/clase-14.html#notas-finales",
    "title": "Apuntes de clase",
    "section": "Notas finales",
    "text": "Notas finales\n\nComo falta cubrir el tema conglomerados jerárquicos, el profesor lo grabará y nos compartirá la grabación, como parte del material del curso por ser evaluado.\nPróxima semana nos pasan la nota de la Tarea 1.\nSábado 9, a la hora de clase, de manera sincrónica, será el examen final. La metodología es parecida al parcial.\nLa tarea 2 se puede entregar hasta el domingo 10."
  }
]