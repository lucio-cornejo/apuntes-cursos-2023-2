[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Learning 1",
    "section": "",
    "text": "Sílabo"
  },
  {
    "objectID": "apuntes/clase-01.html#sobre-el-curso",
    "href": "apuntes/clase-01.html#sobre-el-curso",
    "title": "Apuntes de clase",
    "section": "Sobre el curso",
    "text": "Sobre el curso\nCasi toda la teoría del curso está contenida en ISLR, libro principal de la bibliografía del curso.\nLas listas deben ir avanzándose, pues la entrega oficial consistirá de modificaciones sobre algunos problemas de esas listas, con solo uno o dos días de plazo para la entrega.\nSolo habrá, most likely, máximo tres tareas académicas, pero ninguna se elimina.\nLas tareas no son las que están actualmente en Paideia, pero se basarán en las listas ya subidas.\nMáximo grupos de 4 para los grupos de las tareas; aunque puede ser individual también."
  },
  {
    "objectID": "apuntes/clase-01.html#introducción",
    "href": "apuntes/clase-01.html#introducción",
    "title": "Apuntes de clase",
    "section": "Introducción",
    "text": "Introducción\n\nMain goal:\n\nPredicción (no inferencia, eso se ve en otros cursos)\nFrom dataset, saber qué algoritmo de aprendizaje estadístico escoger, además de saber comunicar los hallazgos encontrados.\n\nHay más similitudes que diferencias entre Statistical Learning y Machine Learning (Aprendizaje de Máquina).\n\n\n¿Qué es Statistical Learning?\n\nSet of tools to understand data.\nDistinción principal: Supervisado vs No Supervisado\nCadena de stat. learning.: modelo -&gt; método -&gt; algoritmo -&gt; análisis -&gt; interpretación\nObjetivos principales:\n\nPredecir: Qué va a pasar\nInferencia: Cómo va a pasar\n\nEn Estadística, es muy importante considerar de dónde vienen los datos (muestreados under some probabilistic distribution).\n\n\n\nStatistical Learning vs Data Science\nData Science también busca obtener info a partir de datos, pero siempre require una implementación. Esto pues Data Science combina diversas disciplinas (math, CS, etc) para darles un enfoque pragmático.\nEste enfoque pragmático no es necesario en Statistical Learning.\nAmbos enfoques son importantes en la sociedad."
  },
  {
    "objectID": "apuntes/clase-01.html#aprendizaje-estadístico",
    "href": "apuntes/clase-01.html#aprendizaje-estadístico",
    "title": "Apuntes de clase",
    "section": "Aprendizaje Estadístico",
    "text": "Aprendizaje Estadístico\n\n¿Qué es?\nProceso de aprendizaje a partir de los datos.\n\nA partir de la aplicación de modelos a un conjunto de entrenamiento podemos:\n\nExtraer conclusiones acerca de las relaciones entre las variables inferencia.\nEncontrar una función predictiva para nuevas observaciones predicción.\n\n\n\n\nEl problema del Aprendizaje Supervisado\n\nPunto de partida:\n\nMedición del resultado \\(Y\\) (variable dependiente/respuesta/objetivo)\nVector de \\(p\\) mediciones predictoras \\(X = (X_1, \\cdots, X_p)\\), también llamadas entradas/regresores/covariables/características o variables independientes.\nEn problemas de regresión, \\(Y\\) es cuantitativa; en problemas de clasificación, \\(Y\\) toma valores en un conjunto finito y desordenado de clases o atributos predefinidos.\nTenemos datos de entrenamiento \\((x_1, Y_1), \\cdots, (x_n, Y_n)\\).\n\nA partir del training set, nos gustaría:\n\nPredecir nuevos casos de prueba.\nComprender como se relacionan las variables.\nEvaluar la calidad de predicciones e inferencias.\n\nLa variable respuesta supervisa nuestro análisis.\n\n\n\nEl problema del Aprendizaje No Supervisado\n\nNo hay variable respuesta.\nSe buscan patrones o agrupaciones (ocultas) en los datos, para obtener información y comprensión.\nHay más subjetividad al momento de comparar modelos; por ello importa mucho más el conocimiento sobre el área de aplicación (field experts).\n\n\n\nFilosofía general\n\nLos métodos más simples a menudo funcionan tan bien como los más complicados.\n\n\n\nStatistical Learning vs Machine Learning\n\n\\(\\text{ML} \\subset \\text{AI}\\) (algoritmos)\n\\(\\text{SL} \\subset \\text{Statistics}\\) (modelos)\nPresentan un mayor enfoque en:\n\nML: Precisión de la predicción y aplicaciones a gran escala.\nSL: Modelos, su interpretabilidad, precisión e incertidumbre.\n\nMétodo escalable: Al añadir más datos o más variables, la precisión no se reduce.\nSL y ML emplean casi las mismas técnicas.\nComparado a un curso de ML, en este curso nos enfocaremos más en comprender los modelos que usaremos, cómo funcionan."
  },
  {
    "objectID": "apuntes/clase-01.html#lista-de-ejercicios",
    "href": "apuntes/clase-01.html#lista-de-ejercicios",
    "title": "Apuntes de clase",
    "section": "Lista de ejercicios",
    "text": "Lista de ejercicios\nLista 1"
  },
  {
    "objectID": "apuntes/clase-02.html#objetivo-de-statistical-learning",
    "href": "apuntes/clase-02.html#objetivo-de-statistical-learning",
    "title": "Apuntes de clase",
    "section": "Objetivo de Statistical Learning",
    "text": "Objetivo de Statistical Learning\n\nSuposiciones:\n\nSe observa una variable respuesta cuantitativa \\(Y\\) .\nSe cuentan con \\(p\\) predictores \\(x_1, \\dots, x_n\\) .\nExiste una función \\(f\\) tal que \\(Y = f(x) + \\epsilon\\) .\nDonde \\(\\epsilon\\), aleatorio, es independiente de \\(X\\), y tiene media cero.\n\nEl término \\(\\epsilon\\) nos permite hacer inferencia … las predicciones tienen un intervalo de confianza.\nOBJETIVO: Estimar \\(f\\) .\nRespecto a la inferencia posible tras estimar \\(f\\), se refieren a generalizar estadísiticamente la relación entre los predictors y la response (ejemplo: analizar los coeficientes tras regresión lineal); decir si los atributos son estadísticamente significativos.\n\n\nMotivo 1: Predicción\n\nObjetivo: Predecir, con la mayor precisión posible, la respuesta \\(Y\\), dadas nuevas observaciones \\(x\\) de las covariables.\n\n\\[\n\\hat{Y} = \\hat{f}(x)\n\\]\n\nPara predicción, no requerimos la forma exacta de \\(\\hat{f}\\) .\nExisten dos términos que influyen en la precisión de \\(\\hat{Y}\\), como prediccion de \\(\\hat{Y}\\) :\n\nError reducible: Proviene de nuestra estimación \\(\\hat{f}\\) de \\(f\\) .\nError ireducible: Proviene del término del error \\(\\epsilon\\) y no puede reducirse mejorando \\(\\hat{f}\\) .\n\nFijada la estimación \\(\\hat{f}\\), respeto a la respuesta \\(Y\\) y un conjunto de predictores \\(X\\), se cumple\n\n\\[\nE \\left[ \\left( Y - \\hat{Y} \\right)^2\\right] =\n\\underbrace{E\\left[\\left( f(X) - \\hat{f}(X) \\right)^2\\right]}_{\\text{error reducible}}\n+ \\underbrace{\\text{var}(\\epsilon)}_{\\text{error irreducible}}\n\\]\n\n\nMotivo 2: Inferencia\n\nObjetivo: Comprender cómo la variable respuesta se ve afectada por los diversos predictores (covariables).\nRequerimos la forma exacta de \\(\\hat{f}\\):\n\n¿Qué predictores están asociados con la respuesta?\n¿Cuál es la relación entre la respuesta y cada predictor?\n¿La relación puede ser lineal, o se requiere un modelo más complejo?"
  },
  {
    "objectID": "apuntes/clase-02.html#regresión-y-clasificación",
    "href": "apuntes/clase-02.html#regresión-y-clasificación",
    "title": "Apuntes de clase",
    "section": "Regresión y Clasificación",
    "text": "Regresión y Clasificación\n\nRegresión: Cuando la variable respuesta es numérica.\nClasificación Cuando la variable respuesta es categórica."
  },
  {
    "objectID": "apuntes/clase-02.html#estimación-de-f",
    "href": "apuntes/clase-02.html#estimación-de-f",
    "title": "Apuntes de clase",
    "section": "Estimación de \\(f\\)",
    "text": "Estimación de \\(f\\)\n\nMain idea:\n\nUsar un conjunto de datos de entrenamiento \\(\\left( x_1, y_1 \\right), \\dots, \\left( x_n, y_n \\right)\\) para hallar una estimación \\(\\hat{f}\\), tal que \\(\\hat{f}(X) \\approx Y\\), para cada \\(\\left( X, Y \\right)\\)\n\nPara predicción, NUNCA evaluar la estimación \\(\\hat{f}\\) en una observación de entrenamiento \\(X\\) .\nPresenta dos enfoques principales: Param. y No param. .\n\n\nMétodos paramétricos\n\nSteps:\n\nFijar una forma para \\(f\\) . 1 Estimar los parámetros desconocidos de \\(f\\), unsando el conjunto de entrenamiento.\n\n\n\n\nMétodos no paramétricos\n\nBuscan una estimación de \\(f\\), sin hacer suposiciones explícitas de la función \\(f\\) .\nEn cierto sentido, se consideran infinitos parámetros.\nEjemplo: Algoritmo de los \\(K\\)-vecinos.\nParámetro: Constante del modelo, que se estima.\nHiperparámetro: Constante del modelo, que se escoge libremente. Por ejemplo, el valor \\(K\\) en el algoritmo de \\(K\\)-means.\nLos hiperparámetros se pueden calibrar para obtener un nivel de adecuado de flexibilidad para el modelo.\n\n\n\nParam. vs No param.\n\nSuelen ser de mayor interpretabilidad: Paramétrico.\nTienden a ser más flexibles: No paramétricos.\nSuelen tener mayor complejidad computacional: No paramétricos.\nSuele requerir una mayor cantidad de datos: No paramétricos.\nNo necesariamente un método no paramétrico siempre produce predicciones más precisas, comparado a un método paramétrico.\n\n\n\n\n\n\nComparación"
  },
  {
    "objectID": "apuntes/clase-02.html#prediction-accuracy-vs-interpretabilidad",
    "href": "apuntes/clase-02.html#prediction-accuracy-vs-interpretabilidad",
    "title": "Apuntes de clase",
    "section": "Prediction accuracy vs Interpretabilidad",
    "text": "Prediction accuracy vs Interpretabilidad\n\nMétodos inflexibles (o rígidos), son aquellos que tienen fuerte restricciones sobre la forma de \\(f\\) .\nLa elección de un método flexible o inflexible depende del objetivo en mente:\n\nIf goal is inferencia, then se prefiere métodos inflexibles\nIf goal is predicción, then se prefiere métodos flexibles.\n\nSobreajuste: Ocurre cuando \\(\\hat{f}\\) se ajusta demasiado a los datos observados.\nSubajuste: Ocurre cuando \\(\\hat{f}\\) es demasiado rígida para capturar la estructura subyacente de los datos."
  },
  {
    "objectID": "apuntes/clase-02.html#ejercicio",
    "href": "apuntes/clase-02.html#ejercicio",
    "title": "Apuntes de clase",
    "section": "EJERCICIO",
    "text": "EJERCICIO\nResolver la lista 1 publicada en Paideia, al menos hasta el item c (no included)."
  },
  {
    "objectID": "apuntes/clase-02.html#evaluación-de-la-precisión-del-modelo",
    "href": "apuntes/clase-02.html#evaluación-de-la-precisión-del-modelo",
    "title": "Apuntes de clase",
    "section": "Evaluación de la precisión del modelo",
    "text": "Evaluación de la precisión del modelo\n\nNingún método domina a todos los demás sobre todos conjuntos de datos posibles.\n\n\nFunción pérdida\n\nPara variable respuesta numérica, las métricas \\(L1\\) y \\(L2\\) suelen usarse.\nPara response categórica, se puede usar la asignación 0 (si \\(\\hat{y}_i = y_i\\) ); y, 1, caso contrario.\nEn problemas de regresión, suele emplearse la pérdida cuadrática (\\(L2\\)).\n\n\n\nMSE de entrenamiento\n\nNotación: \\(\\text{MSE}_{\\text{train}} = \\dfrac{1}{n} \\displaystyle{ \\sum_{i=1}^{n}\\left( y_i - \\hat{f}(x_i) \\right)^2}\\)\nCuando se evalúa \\(\\hat{f}\\) en una observación de entrenamiento, no es posible saber si la predicción fue precisa debido a que el modelo aprendió o porque para el modelo se empleó el valor observado para la response variable (caso modelo plagió).\n\n\n\nMSE de prueba\n\nEvaluamos el modelo con una muestra de observaciones que no fue usada para entrenar al modelo. Esta muestra se denomina datos de prueba (test).\nPara un conjunto de \\(n_0\\) observaciones de prueba \\(\\left( x_{0j}, y_{0j} \\right)\\), se define:\n\n\\(\\text{MSE}_{\\text{test}} = \\dfrac{1}{n_0} \\displaystyle{ \\sum_{j=1}^{n_0}\\left( y_{0j} - \\hat{f}(x_{0j}) \\right)^2}\\)"
  },
  {
    "objectID": "apuntes/clase-03.html#bias-variance-tradeoff",
    "href": "apuntes/clase-03.html#bias-variance-tradeoff",
    "title": "Apuntes de clase",
    "section": "Bias-variance tradeoff",
    "text": "Bias-variance tradeoff\nFijando un valor \\(x_0\\) para los predictores, y considerando una familia de training datasets, cada uno de esos training sets produce una estimación \\(\\hat{f}\\) de la función \\(f\\).\nAhora, considere el siguiente resultado:\n\nTheorem 1 \\[\nE \\left[\\left( Y - \\hat{f}\\left( x_o \\right) \\right)^2\\right]\n= \\text{ Var}\\left( \\epsilon \\right) +\n\\text{ Var}\\left( \\hat{f}\\left( x_0 \\right) \\right) +\n\\left[\\text{ Bias}\\left( \\hat{f}\\left( x_0 \\right) \\right)\\right]^2\n\\]\n\nEl sesgo consiste en cómo se aleja el valor real de la variable, en comparacion con el promedio de las estimaciones.\nModelos que suelen tenor menor sesgo, suelen tener mayor varianza, y viceversa.\n\nProposition 1 Para modelos más flexibles, la varianza se incrementa y el sesgo disminuye. (not aaalways true).\n\n\nProposition 2 Resulta que el punto donde el \\(\\text{ MSE }_{test}\\) alcanza un mínimo (en el eje Y, siendo el eje X el nivel de flexibilidad), a vecese coincide con el punto donde se intersectan el bias y varianza de la estimación \\(\\hat{f}\\)\n\n\n\n\n\n\nBias-variance tradeoff\n\n\n\n\n\nResumen:\n\nA medida que aumenta la complejidad (y, por tanto la flexibilidad) de un modelo, el modelo se vuelve más adaptable a estructuras subyacentes y cae el error de entrenaminto (sobreajuste)\nEl error de prueba es el error de predicción sobre una muestra de prueba.\nLos modelos inflexibles (con parámetros para ajustarse) son fáciles de calcular pero pueden producir subajuste (alto sesgo).\nLos modelos flexibles pueden producir sobreajuste."
  },
  {
    "objectID": "apuntes/clase-03.html#clasificación",
    "href": "apuntes/clase-03.html#clasificación",
    "title": "Apuntes de clase",
    "section": "Clasificación",
    "text": "Clasificación\n\n¿Qué es clasificación?\n\nModelo politómico: Cuando la variable respuesta cuenta con más de dos categorías.\nCuando el objetivo es predecir, no suele tomarse en cuenta la jerarquía (en caso exista) entre las categorías de la variable respuesta. Por ejemplo, en caso \\(Y\\) sea una variable ordinal.\nUsualmente construimos modelos que predigan las probabilidades de categorías, dadas ciertas covariables \\(X\\).\nNo siempre los modelos de clasificación te danla clase y probabilidad de pertenencia a la clase. Los modelos por lo general dan solo la probabilidad de pertenencia a las clases. Por ejemplo:\n\nRegresión logística solo te da la probabilidad de pertenencia a la clase. Sin mbargo, ni en el caso binario basta la regla “probabilidad mayor de 50%” para asignar una clase a una nueva observación.\nÁrboles de decisión te da la clase, pero no la probabilidad de pertenencia.\n\n\n\n\nConfiguración de la clasifiación general\n\nContexto:\n\nLa variable respuesta cuenta con una cantidad finita de valores posibles … categorías.\nLa variable respuesta \\(Y\\) es cualitativa.\n\nObjetivo:\n\nConstruir un clasificador que asigne una etiqueta de clasificación a una observación futura sin etiquetar, además de evaluar la incertidumbre en esta clasificación.\n\nMedidas de rendimiento\n\nLa más popular es la tasa de error de clasificación érronea (versión de entrenamiento y prueba).\n\nPérdida 0/1:\n\nLas clasificaciones erróneas reciben la pérdida 1; y las clasificaciones correctas, pérdida 0.\nNo se emplea pérdida cuadrática para la clasificación.\n\n\n\n\nMétodos Estadísticos Tradicionales para Clasificación\n\nTres métodos comúnmente usados para clasificación:\n\nRegresión Logística\nAnálisis Discrimante Lineal (LDA)\nAnálisis Discrimante Cuadrático (QDA)\n\n\n\n\n\n\n\nGeneralized Linear Models\n\n\n\n\n\n\n\n\n\nGeneralized Linear Models\n\n\n\n\n\nLa exponential family es una familia de distribuciones, tales como la Normal, Gamma, Binomial, etc.\nLa función \\(g\\) se denomina función de enlace y debe satisfacer tener inversa, \\(g^{-1}\\), la cual se denomina función de respuesta.\n\n\n\nPredicción con el modelo logit\n\nSe denomina modelo logit al modelo de regresión logística binaria.\nEs un modelo ideal para interpretar.\nNo es un modelo ideal para predecir, en especial si se tienen clases desbalanceadas.\nRecuerde que desbalanceado no implica difícil de predecir. Además, clases desbalanceadas no es un problema de los datos.\nPredicción\n\n\\(\\hat{p}_i = \\dfrac{1}{1 + e^{-\\eta_i}}\\) = \\(\\dfrac{1}{1 + e^{-\\left( \\hat{\\beta}_0 + \\hat{\\beta}_ 1 x_{1i} + \\dots + \\hat{\\beta}_p x_{pi}\\right)}}\\)\n\\[\n  \\begin{equation}\n    \\hat{Y}_i =\n    \\begin{cases*}\n1 & si $\\hat{p}_i \\geq c$ \\\\\n0 & si $\\hat{p}_i &lt; c$\n    \\end{cases*}\n  \\end{equation}\n  \\] donde \\(c\\) se denomina punto de corte (umbral).\nUsualmente se utiliza \\(c = 0.50\\) .\n\n\n\nTheorem 2 Se puede demostrar (es complicado) que con la elección de umbral \\(c = 0.50\\) se minimiza el error de clasificación; además de ser el único umbral que minimiza tal error.\n\n\n\n¿Regresión lineal para una clasificación binaria?\n\nLa regresión lineal puede producir probabilidades menores que cero o mayores que uno.\n\n\n\nRegresión logística binaria\n\nLa response presenta solo dos categorías posibles.\nSe modela entonces \\(Y_i \\sim Bernoulli(p_i)\\) .\nObjetivo: Estimar \\(p_i = P\\left( Y_i = 1 \\mid X_1, \\dots, X_p \\right)\\)"
  },
  {
    "objectID": "apuntes/clase-04.html#ejemplo-práctico",
    "href": "apuntes/clase-04.html#ejemplo-práctico",
    "title": "Apuntes de clase",
    "section": "Ejemplo práctico",
    "text": "Ejemplo práctico\n\nlibrary(ISLR2)\n\nWarning: package 'ISLR2' was built under R version 4.1.3\n\ndata(Default)\n\n# Default$default &lt;- as.numeric(Default$default) - 1\nglm_default &lt;- glm(\n  default ~ balance, \n  data = Default, family = 'binomial'\n)\n\nsummary(glm_default)$coef\n\n                 Estimate   Std. Error   z value      Pr(&gt;|z|)\n(Intercept) -10.651330614 0.3611573721 -29.49221 3.623124e-191\nbalance       0.005498917 0.0002203702  24.95309 1.976602e-137\n\n\n\\[\n  \\log \\left( \\dfrac{\\hat{p}_i}{1- \\hat{p}_i} \\right)  = \\hat{n}_i = -10.6513 + 0.0055*\\text{balance}_i\n\\]\n\nglm_default_2 &lt;- glm(\n  default ~ balance + income + student, \n  data = Default, family = 'binomial'\n)\n\neta &lt;- summary(glm_default_2)$coef[,1] %*% c(1, 2000, 40000, 1)\n1 / (1 + exp(-eta))\n\n          [,1]\n[1,] 0.5196218"
  },
  {
    "objectID": "apuntes/clase-04.html#clasificador-de-bayes",
    "href": "apuntes/clase-04.html#clasificador-de-bayes",
    "title": "Apuntes de clase",
    "section": "Clasificador de Bayes",
    "text": "Clasificador de Bayes\n\nEl clasificador de Bayes asigna una observación a la clase más probable, dado los valores de los predictores.\n\n\nPropiedades\n\nTiene la tasa de error de prueba más pequeña.\nPor lo general, no conocemos la verdadera distribución condicional \\(Pr(Y| X)\\) para datos reales.\nFunción pérdida: A las clasificaciones erróneas se les asigna la pérdida \\(1\\); y, a las clasificaciones correctas, \\(0\\) . Esta es conocida como pérdida-0/1.\n\n\n\nTasas de error\n\nTasa de error de entrenamiento:\n\n\\[\n\\dfrac{1}{n} \\sum_{i=1}^{n}I\\left( y_i \\ne \\hat{y}_i \\right)\n\\]\n\nTasa de error de prueba:\n\n\\[\nAverage \\left( I \\left( y_i \\ne \\hat{y}_i \\right) \\right)\n\\]\n\nSuponemos que un buen clasificador es aquel que tiene un error de prueba bajo."
  },
  {
    "objectID": "apuntes/clase-04.html#dos-paradigmas",
    "href": "apuntes/clase-04.html#dos-paradigmas",
    "title": "Apuntes de clase",
    "section": "Dos paradigmas",
    "text": "Dos paradigmas\n\nParadigma de diagnóstico:\n\nSe estima directamente la distribución a posteriori para las clases: \\(Pr(Y = k \\mid X = x)\\)\nEjemplos: Regresión logística, Clasificación KNN\n\nParadigma de muestreo:\n\nEnfoque indirecto:\n\nModele la distribución condicional de predictores, para cada clase: \\(f_k (x) = Pr(X = x \\mid Y = k)\\)\nConsidere las probabilidades a priori: \\(\\pi_k = Pr(Y = k)\\)\n\nClasificar en la clase con el producto máximo \\(pi_k f_k (x)\\)\n¿Cómo obtenemos \\(Pr(Y 0= k \\mid X = x_0)\\)?\nTeorema de Bayes: \\[\np_k (X) = Pr(Y = k \\mid X = x) =\n\\dfrac{Pr(X = x \\cap Y = k)}{f(x)} =\n\\dfrac{f_k (x) \\pi_k}{\\sum_{l=1}^{k}f_l (x) \\pi_l}\n  \\]\n\n\n\nProposition 1 No es recomendable usar los mismos datos de test para comparar modelos. Esto puesto que un modelo podría presentar menor test-error que otro, para un mismo test dataset, debido al azar."
  },
  {
    "objectID": "apuntes/clase-04.html#análisis-discriminante",
    "href": "apuntes/clase-04.html#análisis-discriminante",
    "title": "Apuntes de clase",
    "section": "Análisis Discriminante",
    "text": "Análisis Discriminante\n\nEl enfoque es modelar la distribución de \\(X\\) en cada una de las clases por separado, y, luego usar el teorema de Bayes para obtener \\(Pr(Y \\mid X)\\)."
  },
  {
    "objectID": "apuntes/clase-04.html#ejemplo-práctico-lda",
    "href": "apuntes/clase-04.html#ejemplo-práctico-lda",
    "title": "Apuntes de clase",
    "section": "Ejemplo práctico LDA",
    "text": "Ejemplo práctico LDA\n\ndiabetes = read.csv(\n  \"../datos/DiabetesTrain.csv\",\n  stringsAsFactors = TRUE\n)\n\nhead(diabetes)\n\n  glucose insulin sspg  class\n1      97     289  117 normal\n2     105     319  143 normal\n3      90     356  199 normal\n4      90     323  240 normal\n5      86     381  157 normal\n6     100     350  221 normal\n\n\n\nAnálisis descriptivo\n\nlibrary(psych)\n\ndescribeBy(diabetes[,-4], diabetes$class)\n\n\n Descriptive statistics by group \ngroup: chemical\n        vars  n   mean     sd median trimmed    mad min max range skew kurtosis\nglucose    1 26  99.46   8.81   98.5   99.32   9.64  85 114    29 0.24    -1.28\ninsulin    2 26 504.12  60.06  497.5  500.77  63.75 423 643   220 0.37    -0.89\nsspg       3 26 291.77 177.65  222.5  272.14 120.83 109 748   639 1.08    -0.01\n           se\nglucose  1.73\ninsulin 11.78\nsspg    34.84\n------------------------------------------------------------ \ngroup: normal\n        vars  n   mean    sd median trimmed   mad min max range  skew kurtosis\nglucose    1 66  91.98  8.16   91.5   92.00  8.15  70 112    42 -0.05     0.10\ninsulin    2 66 351.21 37.70  354.5  351.46 46.70 269 426   157 -0.09    -0.93\nsspg       3 66 169.02 65.49  156.5  163.39 51.89  73 490   417  1.86     6.76\n          se\nglucose 1.00\ninsulin 4.64\nsspg    8.06\n------------------------------------------------------------ \ngroup: overt\n        vars  n    mean     sd median trimmed    mad min  max range skew\nglucose    1 23  207.17  71.84    203  202.79  96.37 120  339   219 0.35\ninsulin    2 23 1002.96 315.85    972  998.21 382.51 538 1520   982 0.16\nsspg       3 23  112.61 106.57     87   94.37  65.23  10  460   450 1.72\n        kurtosis    se\nglucose    -1.28 14.98\ninsulin    -1.34 65.86\nsspg        2.72 22.22\n\npairs.panels(\n  diabetes[,1:3],\n  bg = c(\"red\",\"yellow\",\"blue\")[diabetes$class],\n  pch = 21\n)\n\n\n\npar(mfrow = c(2,2))\nboxplot(glucose ~ class, data = diabetes, main = \"glucose\")\nboxplot(insulin ~ class, data = diabetes, main = \"insulin\")\nboxplot(sspg ~ class, data = diabetes, main = \"sspg\")\npar(mfrow = c(1,1))\n\n\n\n\n\n\nAnálisis Discriminante Lineal\n\n# Estimación\nlibrary(MASS)\n\nWarning: package 'MASS' was built under R version 4.1.3\n\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:ISLR2':\n\n    Boston\n\nlda1 = lda(class ~., data = diabetes)\nlda1\n\nCall:\nlda(class ~ ., data = diabetes)\n\nPrior probabilities of groups:\nchemical   normal    overt \n0.226087 0.573913 0.200000 \n\nGroup means:\n           glucose   insulin     sspg\nchemical  99.46154  504.1154 291.7692\nnormal    91.98485  351.2121 169.0152\novert    207.17391 1002.9565 112.6087\n\nCoefficients of linear discriminants:\n                 LD1          LD2\nglucose -0.035130542 -0.056150421\ninsulin  0.013748266  0.009876402\nsspg     0.001344232  0.005489825\n\nProportion of trace:\n   LD1    LD2 \n0.8526 0.1474 \n\n# Predicción\nplda1 = predict(lda1, diabetes)$class\n\n# Matriz de confusion (entrenamiento)\ntable(plda1, diabetes$class)\n\n          \nplda1      chemical normal overt\n  chemical       19      1     5\n  normal          7     65     2\n  overt           0      0    16\n\n# Error (entrenamiento)\nerror1 = mean(plda1 != diabetes$class)\nerror1\n\n[1] 0.1304348\n\ncaret::confusionMatrix(\n  data = plda1, reference = diabetes$class\n)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction chemical normal overt\n  chemical       19      1     5\n  normal          7     65     2\n  overt           0      0    16\n\nOverall Statistics\n                                         \n               Accuracy : 0.8696         \n                 95% CI : (0.794, 0.9251)\n    No Information Rate : 0.5739         \n    P-Value [Acc &gt; NIR] : 6.334e-12      \n                                         \n                  Kappa : 0.7644         \n                                         \n Mcnemar's Test P-Value : 0.009308       \n\nStatistics by Class:\n\n                     Class: chemical Class: normal Class: overt\nSensitivity                   0.7308        0.9848       0.6957\nSpecificity                   0.9326        0.8163       1.0000\nPos Pred Value                0.7600        0.8784       1.0000\nNeg Pred Value                0.9222        0.9756       0.9293\nPrevalence                    0.2261        0.5739       0.2000\nDetection Rate                0.1652        0.5652       0.1391\nDetection Prevalence          0.2174        0.6435       0.1391\nBalanced Accuracy             0.8317        0.9006       0.8478\n\n\n\n\nSelección de Variables\n\nlibrary(klaR)\n\nWarning: package 'klaR' was built under R version 4.1.3\n\ngreedy.wilks(class ~., data = diabetes)\n\nFormula containing included variables: \n\nclass ~ insulin + glucose + sspg\n&lt;environment: 0x000000002e63c008&gt;\n\n\nValues calculated in each step of the selection procedure: \n\n     vars Wilks.lambda F.statistics.overall p.value.overall F.statistics.diff\n1 insulin    0.2469409            170.77488    9.665240e-35        170.774879\n2 glucose    0.1532280             86.28294    4.190502e-44         33.943348\n3    sspg    0.1311418             64.58470    7.641367e-46          9.262795\n  p.value.diff\n1 9.665240e-35\n2 2.994049e-12\n3 1.904104e-04\n\n# Validación Cruzada\nset.seed(666)\nstepclass(\n  diabetes[,-4], diabetes$class, \n  method = \"lda\", criterion = \"AC\",\n  # Consideramos una mejora significativa \n  # como de 10%, pero esta es una elección arbitraria\n  improvement = 0.10\n)\n\n `stepwise classification', using 10-fold cross-validated accuracy of method lda'.\n\n\n115 observations of 3 variables in 3 classes; direction: both\n\n\nstop criterion: improvement less than 10%.\n\n\naccuracy: 0.49254;  in: \"insulin\";  variables (1): insulin \naccuracy: 0.7221;  in: \"glucose\";  variables (2): insulin, glucose \n\n hr.elapsed min.elapsed sec.elapsed \n       0.00        0.00        0.38 \n\n\nmethod      : lda \nfinal model : diabetes$class ~ glucose + insulin\n&lt;environment: 0x000000002ede17a0&gt;\n\naccuracy = 0.7221 \n\n\n\n\nEstimación\n\nlda2 = lda(class ~ glucose + insulin, data = diabetes)\n\n# Predicción (entrenamiento)\nplda2 = predict(lda2, diabetes[-4])$class\n\n# Matriz de confusion (entrenamiento)\ncaret::confusionMatrix(plda2, diabetes$class)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction chemical normal overt\n  chemical       23      3     4\n  normal          3     63     3\n  overt           0      0    16\n\nOverall Statistics\n                                          \n               Accuracy : 0.887           \n                 95% CI : (0.8145, 0.9384)\n    No Information Rate : 0.5739          \n    P-Value [Acc &gt; NIR] : 2.26e-13        \n                                          \n                  Kappa : 0.8013          \n                                          \n Mcnemar's Test P-Value : 0.0719          \n\nStatistics by Class:\n\n                     Class: chemical Class: normal Class: overt\nSensitivity                   0.8846        0.9545       0.6957\nSpecificity                   0.9213        0.8776       1.0000\nPos Pred Value                0.7667        0.9130       1.0000\nNeg Pred Value                0.9647        0.9348       0.9293\nPrevalence                    0.2261        0.5739       0.2000\nDetection Rate                0.2000        0.5478       0.1391\nDetection Prevalence          0.2609        0.6000       0.1391\nBalanced Accuracy             0.9030        0.9160       0.8478\n\n# Error\nerror2 = mean(plda2 != diabetes$class)\nerror2\n\n[1] 0.1130435\n\npartimat(\n  class ~., data = diabetes, \n  method = \"lda\", nplots.vert = 2\n)\n\n\n\n\n\n\nDatos de Test\n\ndiabetes_test = read.csv(\n  \"../datos/DiabetesTest.csv\", stringsAsFactors = TRUE\n)\nhead(diabetes_test)\n\n  glucose insulin sspg    class\n1      89     472  162 chemical\n2      96     465  237 chemical\n3     112     503  408 chemical\n4     110     477  124 chemical\n5      90     413  344 chemical\n6     102     472  297 chemical\n\n\n\n# Evaluación en el conjunto de test\npredlda = predict(lda2, diabetes_test)$class\ncaret::confusionMatrix(predlda, diabetes_test$class)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction chemical normal overt\n  chemical        3      1     0\n  normal          7      9     0\n  overt           0      0    10\n\nOverall Statistics\n                                          \n               Accuracy : 0.7333          \n                 95% CI : (0.5411, 0.8772)\n    No Information Rate : 0.3333          \n    P-Value [Acc &gt; NIR] : 8.752e-06       \n                                          \n                  Kappa : 0.6             \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: chemical Class: normal Class: overt\nSensitivity                   0.3000        0.9000       1.0000\nSpecificity                   0.9500        0.6500       1.0000\nPos Pred Value                0.7500        0.5625       1.0000\nNeg Pred Value                0.7308        0.9286       1.0000\nPrevalence                    0.3333        0.3333       0.3333\nDetection Rate                0.1000        0.3000       0.3333\nDetection Prevalence          0.1333        0.5333       0.3333\nBalanced Accuracy             0.6250        0.7750       1.0000"
  },
  {
    "objectID": "apuntes/clase-04.html#ejemplo-práctico-qda",
    "href": "apuntes/clase-04.html#ejemplo-práctico-qda",
    "title": "Apuntes de clase",
    "section": "Ejemplo práctico QDA",
    "text": "Ejemplo práctico QDA\n\n# Estimación\nqda1 = qda(class ~., data = diabetes)\n\n# Predicción\npqda1 = predict(qda1, diabetes[-4])$class\n\n# Matriz de confusion (entrenamiento)\ncaret::confusionMatrix(pqda1, diabetes$class)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction chemical normal overt\n  chemical       22      1     2\n  normal          3     65     0\n  overt           1      0    21\n\nOverall Statistics\n                                          \n               Accuracy : 0.9391          \n                 95% CI : (0.8786, 0.9752)\n    No Information Rate : 0.5739          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.8938          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: chemical Class: normal Class: overt\nSensitivity                   0.8462        0.9848       0.9130\nSpecificity                   0.9663        0.9388       0.9891\nPos Pred Value                0.8800        0.9559       0.9545\nNeg Pred Value                0.9556        0.9787       0.9785\nPrevalence                    0.2261        0.5739       0.2000\nDetection Rate                0.1913        0.5652       0.1826\nDetection Prevalence          0.2174        0.5913       0.1913\nBalanced Accuracy             0.9062        0.9618       0.9511\n\n# Error (entrenamiento)\nerror1 = mean(pqda1 != diabetes$class)\nerror1\n\n[1] 0.06086957\n\n\n\nSelección de variables\n\nset.seed(666)\nstepclass(\n  diabetes[,-4], diabetes$class,\n  method = \"qda\", criterion = \"AC\",\n  improvement = 0.03\n)\n\n `stepwise classification', using 10-fold cross-validated accuracy of method qda'.\n\n\n115 observations of 3 variables in 3 classes; direction: both\n\n\nstop criterion: improvement less than 3%.\n\n\naccuracy: 0.82819;  in: \"insulin\";  variables (1): insulin \naccuracy: 0.86379;  in: \"glucose\";  variables (2): insulin, glucose \n\n hr.elapsed min.elapsed sec.elapsed \n       0.00        0.00        0.25 \n\n\nmethod      : qda \nfinal model : diabetes$class ~ glucose + insulin\n&lt;environment: 0x0000000027e19278&gt;\n\naccuracy = 0.8638 \n\n\n\n# Estimación\nqda2 = qda(class ~ glucose + insulin, data = diabetes)\n\n# Predicción (entrenamiento)\npqda2 = predict(qda2, diabetes[-4])$class\n\n# Matriz de confusion\ncaret::confusionMatrix(pqda2, diabetes$class)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction chemical normal overt\n  chemical       25      1     1\n  normal          1     65     0\n  overt           0      0    22\n\nOverall Statistics\n                                          \n               Accuracy : 0.9739          \n                 95% CI : (0.9257, 0.9946)\n    No Information Rate : 0.5739          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.955           \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: chemical Class: normal Class: overt\nSensitivity                   0.9615        0.9848       0.9565\nSpecificity                   0.9775        0.9796       1.0000\nPos Pred Value                0.9259        0.9848       1.0000\nNeg Pred Value                0.9886        0.9796       0.9892\nPrevalence                    0.2261        0.5739       0.2000\nDetection Rate                0.2174        0.5652       0.1913\nDetection Prevalence          0.2348        0.5739       0.1913\nBalanced Accuracy             0.9695        0.9822       0.9783\n\n# Error (entrenamiento)\nerror1 = mean(pqda2 != diabetes$class)\nerror1\n\n[1] 0.02608696\n\npartimat(\n  class ~.,data = diabetes, \n  method=\"qda\", nplots.vert=2\n)\n\n\n\n\n\n\nEvaluación\n\n# Evaluación en el conjunto de test\npredqda = predict(qda2, diabetes_test)$class\ncaret::confusionMatrix(predqda, diabetes_test$class)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction chemical normal overt\n  chemical        7      0     0\n  normal          2     10     0\n  overt           1      0    10\n\nOverall Statistics\n                                          \n               Accuracy : 0.9             \n                 95% CI : (0.7347, 0.9789)\n    No Information Rate : 0.3333          \n    P-Value [Acc &gt; NIR] : 1.665e-10       \n                                          \n                  Kappa : 0.85            \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: chemical Class: normal Class: overt\nSensitivity                   0.7000        1.0000       1.0000\nSpecificity                   1.0000        0.9000       0.9500\nPos Pred Value                1.0000        0.8333       0.9091\nNeg Pred Value                0.8696        1.0000       1.0000\nPrevalence                    0.3333        0.3333       0.3333\nDetection Rate                0.2333        0.3333       0.3333\nDetection Prevalence          0.2333        0.4000       0.3667\nBalanced Accuracy             0.8500        0.9500       0.9750"
  },
  {
    "objectID": "apuntes/clase-04.html#análisis-discriminante-regularizado",
    "href": "apuntes/clase-04.html#análisis-discriminante-regularizado",
    "title": "Apuntes de clase",
    "section": "Análisis Discriminante Regularizado",
    "text": "Análisis Discriminante Regularizado"
  },
  {
    "objectID": "apuntes/clase-04.html#ejemplo-práctico-1",
    "href": "apuntes/clase-04.html#ejemplo-práctico-1",
    "title": "Apuntes de clase",
    "section": "Ejemplo práctico",
    "text": "Ejemplo práctico\n\nrda(class ~., data = diabetes)\n\nCall: \nrda(formula = class ~ ., data = diabetes)\n\nRegularization parameters: \n       gamma       lambda \n6.350887e-04 6.026917e-10 \n\nPrior probabilities of groups: \nchemical   normal    overt \n0.226087 0.573913 0.200000 \n\nMisclassification rate: \n       apparent: 6.087 %\ncross-validated: 7.725 %\n\nrda1 = rda(class ~., data = diabetes)\nrda1 \n\nCall: \nrda(formula = class ~ ., data = diabetes)\n\nRegularization parameters: \n       gamma       lambda \n1.942273e-05 3.392599e-01 \n\nPrior probabilities of groups: \nchemical   normal    overt \n0.226087 0.573913 0.200000 \n\nMisclassification rate: \n       apparent: 9.565 %\ncross-validated: 12.828 %\n\n# Predicción (entrenamiento)\nprda1 = predict(rda1, diabetes[-4])$class\n\n# Matriz de confusion (entrenamiento)\ncaret::confusionMatrix(prda1, diabetes$class)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction chemical normal overt\n  chemical       21      1     3\n  normal          5     65     2\n  overt           0      0    18\n\nOverall Statistics\n                                          \n               Accuracy : 0.9043          \n                 95% CI : (0.8353, 0.9513)\n    No Information Rate : 0.5739          \n    P-Value [Acc &gt; NIR] : 5.776e-15       \n                                          \n                  Kappa : 0.8293          \n                                          \n Mcnemar's Test P-Value : 0.05343         \n\nStatistics by Class:\n\n                     Class: chemical Class: normal Class: overt\nSensitivity                   0.8077        0.9848       0.7826\nSpecificity                   0.9551        0.8571       1.0000\nPos Pred Value                0.8400        0.9028       1.0000\nNeg Pred Value                0.9444        0.9767       0.9485\nPrevalence                    0.2261        0.5739       0.2000\nDetection Rate                0.1826        0.5652       0.1565\nDetection Prevalence          0.2174        0.6261       0.1565\nBalanced Accuracy             0.8814        0.9210       0.8913\n\n# Error (entrenamiento)\nerror1 = mean(prda1 != diabetes$class)\nerror1\n\n[1] 0.09565217\n\n\n\nSelección de variables\n\n# Selección de Variables (validación cruzada)\nstepclass(\n  diabetes[,-4], diabetes$class,\n  method = \"rda\", criterion = \"AC\",\n  improvement = 0.03\n)\n\n `stepwise classification', using 10-fold cross-validated accuracy of method rda'.\n\n\n115 observations of 3 variables in 3 classes; direction: both\n\n\nstop criterion: improvement less than 3%.\n\n\naccuracy: 0.81469;  in: \"insulin\";  variables (1): insulin \naccuracy: 0.85865;  in: \"glucose\";  variables (2): insulin, glucose \n\n hr.elapsed min.elapsed sec.elapsed \n       0.00        0.00       17.97 \n\n\nmethod      : rda \nfinal model : diabetes$class ~ glucose + insulin\n&lt;environment: 0x00000000246baea8&gt;\n\naccuracy = 0.8587 \n\n\n\n\nModelo final\n\nrda2 = rda(class ~ glucose + insulin, data = diabetes)\nrda2\n\nCall: \nrda(formula = class ~ glucose + insulin, data = diabetes)\n\nRegularization parameters: \n     gamma     lambda \n0.96120286 0.06374861 \n\nPrior probabilities of groups: \nchemical   normal    overt \n0.226087 0.573913 0.200000 \n\nMisclassification rate: \n       apparent: 6.087 %\ncross-validated: 7.101 %\n\n# Predicción\nprda2 = predict(rda2, diabetes[-4])$class\n# Matriz de confusion\ncaret::confusionMatrix(prda2, diabetes$class)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction chemical normal overt\n  chemical       23      0     4\n  normal          3     66     0\n  overt           0      0    19\n\nOverall Statistics\n                                          \n               Accuracy : 0.9391          \n                 95% CI : (0.8786, 0.9752)\n    No Information Rate : 0.5739          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.8931          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: chemical Class: normal Class: overt\nSensitivity                   0.8846        1.0000       0.8261\nSpecificity                   0.9551        0.9388       1.0000\nPos Pred Value                0.8519        0.9565       1.0000\nNeg Pred Value                0.9659        1.0000       0.9583\nPrevalence                    0.2261        0.5739       0.2000\nDetection Rate                0.2000        0.5739       0.1652\nDetection Prevalence          0.2348        0.6000       0.1652\nBalanced Accuracy             0.9198        0.9694       0.9130\n\n# Error\nerror1 = mean(prda2 != diabetes$class)\nerror1\n\n[1] 0.06086957\n\n# Evaluación en el conjunto de test\npredrda = predict(rda2, diabetes_test)$class\ncaret::confusionMatrix(diabetes_test$class, predrda)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction chemical normal overt\n  chemical        7      2     1\n  normal          0     10     0\n  overt           0      0    10\n\nOverall Statistics\n                                          \n               Accuracy : 0.9             \n                 95% CI : (0.7347, 0.9789)\n    No Information Rate : 0.4             \n    P-Value [Acc &gt; NIR] : 1.698e-08       \n                                          \n                  Kappa : 0.85            \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: chemical Class: normal Class: overt\nSensitivity                   1.0000        0.8333       0.9091\nSpecificity                   0.8696        1.0000       1.0000\nPos Pred Value                0.7000        1.0000       1.0000\nNeg Pred Value                1.0000        0.9000       0.9500\nPrevalence                    0.2333        0.4000       0.3667\nDetection Rate                0.2333        0.3333       0.3333\nDetection Prevalence          0.3333        0.3333       0.3333\nBalanced Accuracy             0.9348        0.9167       0.9545"
  },
  {
    "objectID": "apuntes/clase-04.html#naive-bayes",
    "href": "apuntes/clase-04.html#naive-bayes",
    "title": "Apuntes de clase",
    "section": "Naive Bayes",
    "text": "Naive Bayes\n\nMétodo popular cuando se tiene una gran cantidad de predictores.\nSuponemos que en cada clase, los predictores son independientes, (supuesto de independencia condicional dentro de clases).\nEs un método escalable, es decir, no pierde eficiencia cuando se aumenta la cantidad de predictores (más columnas).\nRápidamente genera predicciones de clasificaciones, comparado a otros modelos.\nNo es tan útil para inferencia.\nCuando las distribuciones marginales son respecto a una un predictor numérico continuo, se supone que tal predictor sigue una distribución normal univariada."
  },
  {
    "objectID": "apuntes/clase-04.html#ejercicio",
    "href": "apuntes/clase-04.html#ejercicio",
    "title": "Apuntes de clase",
    "section": "Ejercicio",
    "text": "Ejercicio\nIr avanzando la lista 2."
  },
  {
    "objectID": "apuntes/clase-05.html#naive-bayes",
    "href": "apuntes/clase-05.html#naive-bayes",
    "title": "Apuntes de clase",
    "section": "Naive Bayes",
    "text": "Naive Bayes\n\nEs un modelo no paramétrico, donde, entre sus parámetros está la estimación, via frecuencia relativa, de los parámetros \\(Pr[Y=k]\\) de la población.\nNo es un modelo interpetable … no se usa para entender cómo afectan los predictores a la respuesta.\nNo nos permite conocer la verdadera distribución condicional de \\(Pr(Y\\mid X)\\) para datos reales.\nEl supuesto de normalidad multivariada casi nunca se cumple en la vida real, por lo que el modelo Naive Bayes propone, en parte, una menor restricción sobre la distribución de los predictores.\n\n\nCálculo de probabilidades condicionales\n\nEl clasificador Naive Bayes puede ser aplicado también cuando hay predictors continuos. Alternativas:\n\nDiscretización.\nDiscretizar variables numéricas no siempre es una pérdida de información. Más bien, la discretización puede ser una buena alternativa para remover ruido de los datos, lidiar con el hecho que las variables numéricas presentan distintas escalas (min max).\nEstimación no paramétrica de la densidad del kernel.\nSupone una distribución para cada predictora, por lo general Gaussiana, con media y varianza estimada de los datos.\n\n\nSi simplemente asumes normalidad, es parecido (no totalmente) al caso estudiado en Análisis Discriminante.\n\n\nEstimador Naive Bayes\n\nEs importante evitar que haya alguna probabilidad igual a cero, respecto a alguna de las clases.\n\nEn ese tipo de casos, se puede usar una correción de Laplace.\nNo existe un corrector Laplaciano mejor que todos, pero por lo general suele usarse el valor 1.\nAún así, puede tratarse el corrector Laplaciano como un hiperparámetro del modelo.\n\n\n\n\nPredicción\n\nPara predecir la clase a la cual pertenece \\(X\\), \\(Pr[Y = k]Pr[X \\mid Y = k]\\) es evaluado para cada clase \\(k\\) .\nEl clasificador predecirá que los valores de \\(X\\) pertenecen a la clase \\(i\\) si y solo si: \\[\n\\begin{gather}\nPr[Y = i]Pr[X \\mid Y = i] &gt; Pr[Y = j]Pr[X \\mid Y = j], \\\\\n\\; \\forall 1\\leq j\\leq m, \\; j \\ne i\n\\end{gather}\n\\]"
  },
  {
    "objectID": "apuntes/clase-05.html#ejemplo-práctico",
    "href": "apuntes/clase-05.html#ejemplo-práctico",
    "title": "Apuntes de clase",
    "section": "Ejemplo práctico",
    "text": "Ejemplo práctico\n\ndiabetes = read.csv(\n  \"../datos/DiabetesTrain.csv\", stringsAsFactors = TRUE\n)\nhead(diabetes)\n\n  glucose insulin sspg  class\n1      97     289  117 normal\n2     105     319  143 normal\n3      90     356  199 normal\n4      90     323  240 normal\n5      86     381  157 normal\n6     100     350  221 normal\n\n\n\n# ¿Hay normalidad?\ndiabetes$glucose |&gt; hist()\n\n\n\n\n\nSin discretizar y asumiendo normalidad\n\nlibrary(e1071)\n\nWarning: package 'e1071' was built under R version 4.1.3\n\na &lt;- naiveBayes(class ~ .,data = diabetes)\na\n\n\nNaive Bayes Classifier for Discrete Predictors\n\nCall:\nnaiveBayes.default(x = X, y = Y, laplace = laplace)\n\nA-priori probabilities:\nY\nchemical   normal    overt \n0.226087 0.573913 0.200000 \n\nConditional probabilities:\n          glucose\nY               [,1]      [,2]\n  chemical  99.46154  8.805593\n  normal    91.98485  8.164637\n  overt    207.17391 71.837982\n\n          insulin\nY               [,1]      [,2]\n  chemical  504.1154  60.05819\n  normal    351.2121  37.69861\n  overt    1002.9565 315.85288\n\n          sspg\nY              [,1]      [,2]\n  chemical 291.7692 177.65479\n  normal   169.0152  65.48952\n  overt    112.6087 106.57253\n\n\nEsta librería asume que las variables numéricas usadas siguen una distriubución normal marginal.\n\npred = predict(a, diabetes[,-4], type = \"raw\")\npred\n\n            chemical        normal        overt\n  [1,]  5.295056e-04  9.990214e-01 4.491287e-04\n  [2,]  2.339643e-03  9.971217e-01 5.386942e-04\n  [3,]  2.275912e-03  9.976460e-01 7.807859e-05\n  [4,]  1.180417e-03  9.987288e-01 9.074437e-05\n  [5,]  4.361057e-03  9.954717e-01 1.672017e-04\n  [6,]  6.386586e-03  9.934653e-01 1.481366e-04\n  [7,]  2.183812e-04  9.996164e-01 1.652600e-04\n  [8,]  1.109668e-02  9.886942e-01 2.091171e-04\n  [9,]  5.561492e-04  9.991428e-01 3.010012e-04\n [10,]  2.860365e-03  9.970620e-01 7.768089e-05\n [11,]  2.737109e-04  9.995964e-01 1.298984e-04\n [12,]  9.429950e-05  9.990956e-01 8.101451e-04\n [13,]  4.685303e-03  9.952171e-01 9.757051e-05\n [14,]  7.145331e-03  9.924505e-01 4.041340e-04\n [15,]  2.577827e-03  9.973178e-01 1.043346e-04\n [16,]  5.755875e-03  9.941035e-01 1.405755e-04\n [17,]  1.940953e-04  9.994795e-01 3.264158e-04\n [18,]  1.206339e-03  9.986771e-01 1.165314e-04\n [19,]  5.334159e-03  9.945073e-01 1.585289e-04\n [20,]  2.698566e-04  9.995533e-01 1.768468e-04\n [21,]  7.047837e-04  9.992159e-01 7.928524e-05\n [22,]  5.066611e-02  9.491241e-01 2.098169e-04\n [23,]  6.040562e-03  9.937900e-01 1.694724e-04\n [24,]  4.388028e-03  9.954543e-01 1.577203e-04\n [25,]  4.236783e-04  9.994793e-01 9.706961e-05\n [26,]  1.851161e-03  9.980173e-01 1.315720e-04\n [27,]  5.266643e-04  9.969041e-01 2.569231e-03\n [28,]  2.002639e-03  9.978337e-01 1.636298e-04\n [29,]  4.942068e-04  9.990224e-01 4.834124e-04\n [30,]  5.673222e-03  9.942287e-01 9.806927e-05\n [31,]  2.769180e-03  9.970957e-01 1.351083e-04\n [32,]  1.196466e-02  9.877739e-01 2.614334e-04\n [33,]  9.187795e-03  9.906305e-01 1.816555e-04\n [34,]  6.866383e-04  9.990540e-01 2.593738e-04\n [35,]  8.178601e-02  9.165555e-01 1.658472e-03\n [36,]  1.860864e-02  9.810572e-01 3.341775e-04\n [37,]  1.232439e-02  9.875490e-01 1.266344e-04\n [38,]  4.917400e-04  9.990449e-01 4.633993e-04\n [39,]  9.000279e-05  9.982301e-01 1.679900e-03\n [40,]  1.080090e-03  9.986026e-01 3.173065e-04\n [41,]  3.708905e-03  9.961129e-01 1.782156e-04\n [42,]  2.054200e-04  9.995771e-01 2.174545e-04\n [43,]  7.433120e-04  9.991504e-01 1.063132e-04\n [44,]  7.221347e-04  9.991566e-01 1.212746e-04\n [45,]  3.258039e-04  9.995439e-01 1.303026e-04\n [46,]  1.254320e-03  9.986309e-01 1.147530e-04\n [47,]  3.679488e-04  9.995214e-01 1.106108e-04\n [48,]  8.575833e-04  9.988566e-01 2.858096e-04\n [49,]  1.275039e-03  9.983903e-01 3.346247e-04\n [50,]  9.521719e-01  4.412351e-02 3.704616e-03\n [51,]  4.828539e-02  9.510608e-01 6.537688e-04\n [52,]  5.440347e-01  4.450488e-01 1.091648e-02\n [53,]  2.667509e-01  7.317704e-01 1.478681e-03\n [54,]  3.289868e-04  9.994252e-01 2.458509e-04\n [55,]  2.113070e-01  7.871383e-01 1.554676e-03\n [56,]  1.067948e-01  8.923053e-01 8.998199e-04\n [57,]  1.784725e-02  9.818881e-01 2.646238e-04\n [58,]  3.200394e-02  9.671740e-01 8.220511e-04\n [59,]  2.527538e-02  9.744571e-01 2.675100e-04\n [60,]  9.727327e-03  9.899078e-01 3.649193e-04\n [61,]  7.682488e-03  9.922034e-01 1.141017e-04\n [62,]  3.744839e-01  6.238293e-01 1.686796e-03\n [63,]  7.721793e-01  2.230828e-01 4.737862e-03\n [64,]  2.003020e-01  7.989477e-01 7.503410e-04\n [65,]  3.440964e-03  9.964530e-01 1.060147e-04\n [66,]  2.381242e-02  9.760112e-01 1.763595e-04\n [67,]  2.275912e-03  9.976460e-01 7.807859e-05\n [68,]  6.761531e-02  9.312615e-01 1.123204e-03\n [69,]  9.994407e-01  5.208362e-04 3.847674e-05\n [70,]  6.424901e-02  9.345319e-01 1.219088e-03\n [71,]  7.408047e-04  9.989515e-01 3.077439e-04\n [72,]  1.000000e+00  2.338103e-22 9.485134e-09\n [73,]  9.786212e-01  5.814184e-06 2.137299e-02\n [74,]  9.999997e-01  5.502353e-11 3.409691e-07\n [75,]  9.850467e-01  1.231466e-07 1.495318e-02\n [76,]  9.999758e-01  1.838167e-09 2.423367e-05\n [77,]  9.999997e-01  7.331410e-15 3.136628e-07\n [78,]  9.803726e-01  1.880635e-02 8.210894e-04\n [79,]  9.932796e-01  2.821303e-09 6.720425e-03\n [80,]  9.810128e-01  1.852270e-02 4.645416e-04\n [81,]  9.999985e-01  3.314769e-11 1.488563e-06\n [82,]  9.987065e-01  4.618651e-06 1.288886e-03\n [83,]  9.999115e-01  3.207163e-05 5.645985e-05\n [84,]  9.602919e-01  3.594148e-02 3.766582e-03\n [85,]  3.592417e-01  6.378324e-01 2.925914e-03\n [86,]  9.990254e-01  5.759318e-06 9.688743e-04\n [87,]  9.821413e-01  4.745440e-07 1.785827e-02\n [88,]  9.777282e-01  2.132689e-02 9.449512e-04\n [89,]  9.678721e-01  5.272532e-07 3.212741e-02\n [90,]  1.012043e-01  8.981187e-01 6.769971e-04\n [91,]  6.472607e-01  1.473760e-12 3.527393e-01\n [92,]  9.923048e-01  6.191171e-05 7.633263e-03\n [93,] 6.897663e-173  0.000000e+00 1.000000e+00\n [94,]  4.976878e-03  2.580640e-21 9.950231e-01\n [95,] 2.477275e-146 1.702646e-304 1.000000e+00\n [96,]  7.133953e-60 2.662460e-137 1.000000e+00\n [97,]  1.166381e-35  1.111443e-88 1.000000e+00\n [98,] 1.851074e-159 5.163499e-300 1.000000e+00\n [99,]  5.174439e-21  5.308143e-55 1.000000e+00\n[100,]  2.488687e-42  2.626983e-96 1.000000e+00\n[101,]  3.141734e-04  1.022311e-15 9.996858e-01\n[102,] 7.579262e-131 5.081788e-267 1.000000e+00\n[103,]  2.659830e-95 1.653866e-183 1.000000e+00\n[104,]  3.774942e-77 5.127724e-169 1.000000e+00\n[105,]  1.938547e-12  2.930057e-45 1.000000e+00\n[106,]  9.973924e-01  1.162370e-10 2.607642e-03\n[107,]  1.467368e-50 3.974579e-111 1.000000e+00\n[108,] 2.314463e-209  0.000000e+00 1.000000e+00\n[109,]  6.664180e-01  1.305333e-07 3.335819e-01\n[110,]  2.259447e-03  1.233813e-18 9.977406e-01\n[111,]  8.066183e-01  1.579498e-13 1.933817e-01\n[112,]  2.863917e-06  7.337970e-28 9.999971e-01\n[113,]  7.552413e-34  1.074422e-84 1.000000e+00\n[114,] 8.805182e-203  0.000000e+00 1.000000e+00\n[115,]  4.892801e-52 4.067389e-116 1.000000e+00\n\n\n\npred1 = factor(max.col(pred), labels = levels(diabetes$class))\npred1\n\n  [1] normal   normal   normal   normal   normal   normal   normal   normal  \n  [9] normal   normal   normal   normal   normal   normal   normal   normal  \n [17] normal   normal   normal   normal   normal   normal   normal   normal  \n [25] normal   normal   normal   normal   normal   normal   normal   normal  \n [33] normal   normal   normal   normal   normal   normal   normal   normal  \n [41] normal   normal   normal   normal   normal   normal   normal   normal  \n [49] normal   chemical normal   chemical normal   normal   normal   normal  \n [57] normal   normal   normal   normal   normal   normal   chemical normal  \n [65] normal   normal   normal   normal   chemical normal   normal   chemical\n [73] chemical chemical chemical chemical chemical chemical chemical chemical\n [81] chemical chemical chemical chemical normal   chemical chemical chemical\n [89] chemical normal   chemical chemical overt    overt    overt    overt   \n [97] overt    overt    overt    overt    overt    overt    overt    overt   \n[105] overt    chemical overt    overt    chemical overt    chemical overt   \n[113] overt    overt    overt   \nLevels: chemical normal overt\n\n\n\npred1 = predict(a, diabetes[,-4])\npred1\n\n  [1] normal   normal   normal   normal   normal   normal   normal   normal  \n  [9] normal   normal   normal   normal   normal   normal   normal   normal  \n [17] normal   normal   normal   normal   normal   normal   normal   normal  \n [25] normal   normal   normal   normal   normal   normal   normal   normal  \n [33] normal   normal   normal   normal   normal   normal   normal   normal  \n [41] normal   normal   normal   normal   normal   normal   normal   normal  \n [49] normal   chemical normal   chemical normal   normal   normal   normal  \n [57] normal   normal   normal   normal   normal   normal   chemical normal  \n [65] normal   normal   normal   normal   chemical normal   normal   chemical\n [73] chemical chemical chemical chemical chemical chemical chemical chemical\n [81] chemical chemical chemical chemical normal   chemical chemical chemical\n [89] chemical normal   chemical chemical overt    overt    overt    overt   \n [97] overt    overt    overt    overt    overt    overt    overt    overt   \n[105] overt    chemical overt    overt    chemical overt    chemical overt   \n[113] overt    overt    overt   \nLevels: chemical normal overt\n\n\n\ncaret::confusionMatrix(pred1, diabetes[,4])\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction chemical normal overt\n  chemical       20      3     3\n  normal          6     63     0\n  overt           0      0    20\n\nOverall Statistics\n                                          \n               Accuracy : 0.8957          \n                 95% CI : (0.8248, 0.9449)\n    No Information Rate : 0.5739          \n    P-Value [Acc &gt; NIR] : 3.778e-14       \n                                          \n                  Kappa : 0.8169          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: chemical Class: normal Class: overt\nSensitivity                   0.7692        0.9545       0.8696\nSpecificity                   0.9326        0.8776       1.0000\nPos Pred Value                0.7692        0.9130       1.0000\nNeg Pred Value                0.9326        0.9348       0.9684\nPrevalence                    0.2261        0.5739       0.2000\nDetection Rate                0.1739        0.5478       0.1739\nDetection Prevalence          0.2261        0.6000       0.1739\nBalanced Accuracy             0.8509        0.9160       0.9348\n\n\n\n\nSin discretizar ni asumir normalidad\n\nlibrary(naivebayes)\n\nWarning: package 'naivebayes' was built under R version 4.1.3\n\n\nnaivebayes 0.9.7 loaded\n\na &lt;- naive_bayes(class ~ ., data = diabetes)\na\n\n\n================================== Naive Bayes ================================== \n \n Call: \nnaive_bayes.formula(formula = class ~ ., data = diabetes)\n\n--------------------------------------------------------------------------------- \n \nLaplace smoothing: 0\n\n--------------------------------------------------------------------------------- \n \n A priori probabilities: \n\nchemical   normal    overt \n0.226087 0.573913 0.200000 \n\n--------------------------------------------------------------------------------- \n \n Tables: \n\n--------------------------------------------------------------------------------- \n ::: glucose (Gaussian) \n--------------------------------------------------------------------------------- \n       \nglucose   chemical     normal      overt\n   mean  99.461538  91.984848 207.173913\n   sd     8.805593   8.164637  71.837982\n\n--------------------------------------------------------------------------------- \n ::: insulin (Gaussian) \n--------------------------------------------------------------------------------- \n       \ninsulin   chemical     normal      overt\n   mean  504.11538  351.21212 1002.95652\n   sd     60.05819   37.69861  315.85288\n\n--------------------------------------------------------------------------------- \n ::: sspg (Gaussian) \n--------------------------------------------------------------------------------- \n      \nsspg    chemical    normal     overt\n  mean 291.76923 169.01515 112.60870\n  sd   177.65479  65.48952 106.57253\n\n---------------------------------------------------------------------------------\n\n\nPor default, se asumió normalidad marginal, debido al valor FALSE del parámetro usekernel de naive_bayes.\n\npred = predict(a, diabetes[,-4])\npred\n\n  [1] normal   normal   normal   normal   normal   normal   normal   normal  \n  [9] normal   normal   normal   normal   normal   normal   normal   normal  \n [17] normal   normal   normal   normal   normal   normal   normal   normal  \n [25] normal   normal   normal   normal   normal   normal   normal   normal  \n [33] normal   normal   normal   normal   normal   normal   normal   normal  \n [41] normal   normal   normal   normal   normal   normal   normal   normal  \n [49] normal   chemical normal   chemical normal   normal   normal   normal  \n [57] normal   normal   normal   normal   normal   normal   chemical normal  \n [65] normal   normal   normal   normal   chemical normal   normal   chemical\n [73] chemical chemical chemical chemical chemical chemical chemical chemical\n [81] chemical chemical chemical chemical normal   chemical chemical chemical\n [89] chemical normal   chemical chemical overt    overt    overt    overt   \n [97] overt    overt    overt    overt    overt    overt    overt    overt   \n[105] overt    chemical overt    overt    chemical overt    chemical overt   \n[113] overt    overt    overt   \nLevels: chemical normal overt\n\n\n\ncaret::confusionMatrix(pred,diabetes[,4])\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction chemical normal overt\n  chemical       20      3     3\n  normal          6     63     0\n  overt           0      0    20\n\nOverall Statistics\n                                          \n               Accuracy : 0.8957          \n                 95% CI : (0.8248, 0.9449)\n    No Information Rate : 0.5739          \n    P-Value [Acc &gt; NIR] : 3.778e-14       \n                                          \n                  Kappa : 0.8169          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: chemical Class: normal Class: overt\nSensitivity                   0.7692        0.9545       0.8696\nSpecificity                   0.9326        0.8776       1.0000\nPos Pred Value                0.7692        0.9130       1.0000\nNeg Pred Value                0.9326        0.9348       0.9684\nPrevalence                    0.2261        0.5739       0.2000\nDetection Rate                0.1739        0.5478       0.1739\nDetection Prevalence          0.2261        0.6000       0.1739\nBalanced Accuracy             0.8509        0.9160       0.9348\n\npredict(a, diabetes[,-4], type = \"prob\")\n\n            chemical        normal        overt\n  [1,]  5.295056e-04  9.990214e-01 4.491287e-04\n  [2,]  2.339643e-03  9.971217e-01 5.386942e-04\n  [3,]  2.275912e-03  9.976460e-01 7.807859e-05\n  [4,]  1.180417e-03  9.987288e-01 9.074437e-05\n  [5,]  4.361057e-03  9.954717e-01 1.672017e-04\n  [6,]  6.386586e-03  9.934653e-01 1.481366e-04\n  [7,]  2.183812e-04  9.996164e-01 1.652600e-04\n  [8,]  1.109668e-02  9.886942e-01 2.091171e-04\n  [9,]  5.561492e-04  9.991428e-01 3.010012e-04\n [10,]  2.860365e-03  9.970620e-01 7.768089e-05\n [11,]  2.737109e-04  9.995964e-01 1.298984e-04\n [12,]  9.429950e-05  9.990956e-01 8.101451e-04\n [13,]  4.685303e-03  9.952171e-01 9.757051e-05\n [14,]  7.145331e-03  9.924505e-01 4.041340e-04\n [15,]  2.577827e-03  9.973178e-01 1.043346e-04\n [16,]  5.755875e-03  9.941035e-01 1.405755e-04\n [17,]  1.940953e-04  9.994795e-01 3.264158e-04\n [18,]  1.206339e-03  9.986771e-01 1.165314e-04\n [19,]  5.334159e-03  9.945073e-01 1.585289e-04\n [20,]  2.698566e-04  9.995533e-01 1.768468e-04\n [21,]  7.047837e-04  9.992159e-01 7.928524e-05\n [22,]  5.066611e-02  9.491241e-01 2.098169e-04\n [23,]  6.040562e-03  9.937900e-01 1.694724e-04\n [24,]  4.388028e-03  9.954543e-01 1.577203e-04\n [25,]  4.236783e-04  9.994793e-01 9.706961e-05\n [26,]  1.851161e-03  9.980173e-01 1.315720e-04\n [27,]  5.266643e-04  9.969041e-01 2.569231e-03\n [28,]  2.002639e-03  9.978337e-01 1.636298e-04\n [29,]  4.942068e-04  9.990224e-01 4.834124e-04\n [30,]  5.673222e-03  9.942287e-01 9.806927e-05\n [31,]  2.769180e-03  9.970957e-01 1.351083e-04\n [32,]  1.196466e-02  9.877739e-01 2.614334e-04\n [33,]  9.187795e-03  9.906305e-01 1.816555e-04\n [34,]  6.866383e-04  9.990540e-01 2.593738e-04\n [35,]  8.178601e-02  9.165555e-01 1.658472e-03\n [36,]  1.860864e-02  9.810572e-01 3.341775e-04\n [37,]  1.232439e-02  9.875490e-01 1.266344e-04\n [38,]  4.917400e-04  9.990449e-01 4.633993e-04\n [39,]  9.000279e-05  9.982301e-01 1.679900e-03\n [40,]  1.080090e-03  9.986026e-01 3.173065e-04\n [41,]  3.708905e-03  9.961129e-01 1.782156e-04\n [42,]  2.054200e-04  9.995771e-01 2.174545e-04\n [43,]  7.433120e-04  9.991504e-01 1.063132e-04\n [44,]  7.221347e-04  9.991566e-01 1.212746e-04\n [45,]  3.258039e-04  9.995439e-01 1.303026e-04\n [46,]  1.254320e-03  9.986309e-01 1.147530e-04\n [47,]  3.679488e-04  9.995214e-01 1.106108e-04\n [48,]  8.575833e-04  9.988566e-01 2.858096e-04\n [49,]  1.275039e-03  9.983903e-01 3.346247e-04\n [50,]  9.521719e-01  4.412351e-02 3.704616e-03\n [51,]  4.828539e-02  9.510608e-01 6.537688e-04\n [52,]  5.440347e-01  4.450488e-01 1.091648e-02\n [53,]  2.667509e-01  7.317704e-01 1.478681e-03\n [54,]  3.289868e-04  9.994252e-01 2.458509e-04\n [55,]  2.113070e-01  7.871383e-01 1.554676e-03\n [56,]  1.067948e-01  8.923053e-01 8.998199e-04\n [57,]  1.784725e-02  9.818881e-01 2.646238e-04\n [58,]  3.200394e-02  9.671740e-01 8.220511e-04\n [59,]  2.527538e-02  9.744571e-01 2.675100e-04\n [60,]  9.727327e-03  9.899078e-01 3.649193e-04\n [61,]  7.682488e-03  9.922034e-01 1.141017e-04\n [62,]  3.744839e-01  6.238293e-01 1.686796e-03\n [63,]  7.721793e-01  2.230828e-01 4.737862e-03\n [64,]  2.003020e-01  7.989477e-01 7.503410e-04\n [65,]  3.440964e-03  9.964530e-01 1.060147e-04\n [66,]  2.381242e-02  9.760112e-01 1.763595e-04\n [67,]  2.275912e-03  9.976460e-01 7.807859e-05\n [68,]  6.761531e-02  9.312615e-01 1.123204e-03\n [69,]  9.994407e-01  5.208362e-04 3.847674e-05\n [70,]  6.424901e-02  9.345319e-01 1.219088e-03\n [71,]  7.408047e-04  9.989515e-01 3.077439e-04\n [72,]  1.000000e+00  2.338103e-22 9.485134e-09\n [73,]  9.786212e-01  5.814184e-06 2.137299e-02\n [74,]  9.999997e-01  5.502353e-11 3.409691e-07\n [75,]  9.850467e-01  1.231466e-07 1.495318e-02\n [76,]  9.999758e-01  1.838167e-09 2.423367e-05\n [77,]  9.999997e-01  7.331410e-15 3.136628e-07\n [78,]  9.803726e-01  1.880635e-02 8.210894e-04\n [79,]  9.932796e-01  2.821303e-09 6.720425e-03\n [80,]  9.810128e-01  1.852270e-02 4.645416e-04\n [81,]  9.999985e-01  3.314769e-11 1.488563e-06\n [82,]  9.987065e-01  4.618651e-06 1.288886e-03\n [83,]  9.999115e-01  3.207163e-05 5.645985e-05\n [84,]  9.602919e-01  3.594148e-02 3.766582e-03\n [85,]  3.592417e-01  6.378324e-01 2.925914e-03\n [86,]  9.990254e-01  5.759318e-06 9.688743e-04\n [87,]  9.821413e-01  4.745440e-07 1.785827e-02\n [88,]  9.777282e-01  2.132689e-02 9.449512e-04\n [89,]  9.678721e-01  5.272532e-07 3.212741e-02\n [90,]  1.012043e-01  8.981187e-01 6.769971e-04\n [91,]  6.472607e-01  1.473760e-12 3.527393e-01\n [92,]  9.923048e-01  6.191171e-05 7.633263e-03\n [93,] 6.897663e-173  0.000000e+00 1.000000e+00\n [94,]  4.976878e-03  2.580640e-21 9.950231e-01\n [95,] 2.477275e-146 1.702646e-304 1.000000e+00\n [96,]  7.133953e-60 2.662460e-137 1.000000e+00\n [97,]  1.166381e-35  1.111443e-88 1.000000e+00\n [98,] 1.851074e-159 5.163499e-300 1.000000e+00\n [99,]  5.174439e-21  5.308143e-55 1.000000e+00\n[100,]  2.488687e-42  2.626983e-96 1.000000e+00\n[101,]  3.141734e-04  1.022311e-15 9.996858e-01\n[102,] 7.579262e-131 5.081788e-267 1.000000e+00\n[103,]  2.659830e-95 1.653866e-183 1.000000e+00\n[104,]  3.774942e-77 5.127724e-169 1.000000e+00\n[105,]  1.938547e-12  2.930057e-45 1.000000e+00\n[106,]  9.973924e-01  1.162370e-10 2.607642e-03\n[107,]  1.467368e-50 3.974579e-111 1.000000e+00\n[108,] 2.314463e-209  0.000000e+00 1.000000e+00\n[109,]  6.664180e-01  1.305333e-07 3.335819e-01\n[110,]  2.259447e-03  1.233813e-18 9.977406e-01\n[111,]  8.066183e-01  1.579498e-13 1.933817e-01\n[112,]  2.863917e-06  7.337970e-28 9.999971e-01\n[113,]  7.552413e-34  1.074422e-84 1.000000e+00\n[114,] 8.805182e-203  0.000000e+00 1.000000e+00\n[115,]  4.892801e-52 4.067389e-116 1.000000e+00\n\n\nAhora no asumiremos normalidad marginal:\n\na &lt;- naive_bayes(class ~ ., data = diabetes, usekernel = TRUE)\na\n\n\n================================== Naive Bayes ================================== \n \n Call: \nnaive_bayes.formula(formula = class ~ ., data = diabetes, usekernel = TRUE)\n\n--------------------------------------------------------------------------------- \n \nLaplace smoothing: 0\n\n--------------------------------------------------------------------------------- \n \n A priori probabilities: \n\nchemical   normal    overt \n0.226087 0.573913 0.200000 \n\n--------------------------------------------------------------------------------- \n \n Tables: \n\n--------------------------------------------------------------------------------- \n ::: glucose::chemical (KDE)\n--------------------------------------------------------------------------------- \n\nCall:\n    density.default(x = x, na.rm = TRUE)\n\nData: x (26 obs.);  Bandwidth 'bw' = 4.131\n\n       x                y            \n Min.   : 72.61   Min.   :4.949e-05  \n 1st Qu.: 86.05   1st Qu.:3.464e-03  \n Median : 99.50   Median :2.127e-02  \n Mean   : 99.50   Mean   :1.857e-02  \n 3rd Qu.:112.95   3rd Qu.:3.093e-02  \n Max.   :126.39   Max.   :3.998e-02  \n\n--------------------------------------------------------------------------------- \n ::: glucose::normal (KDE)\n--------------------------------------------------------------------------------- \n\nCall:\n    density.default(x = x, na.rm = TRUE)\n\nData: x (66 obs.);  Bandwidth 'bw' = 3.179\n\n       x                y            \n Min.   : 60.46   Min.   :2.145e-05  \n 1st Qu.: 75.73   1st Qu.:2.132e-03  \n Median : 91.00   Median :8.905e-03  \n Mean   : 91.00   Mean   :1.636e-02  \n 3rd Qu.:106.27   3rd Qu.:3.114e-02  \n Max.   :121.54   Max.   :4.846e-02  \n\n--------------------------------------------------------------------------------- \n ::: glucose::overt (KDE)\n--------------------------------------------------------------------------------- \n\nCall:\n    density.default(x = x, na.rm = TRUE)\n\nData: x (23 obs.);  Bandwidth 'bw' = 34.53\n\n       x               y            \n Min.   : 16.4   Min.   :8.447e-06  \n 1st Qu.:122.9   1st Qu.:5.362e-04  \n Median :229.5   Median :2.638e-03  \n Mean   :229.5   Mean   :2.343e-03  \n 3rd Qu.:336.1   3rd Qu.:3.926e-03  \n Max.   :442.6   Max.   :4.754e-03  \n\n--------------------------------------------------------------------------------- \n ::: insulin::chemical (KDE)\n--------------------------------------------------------------------------------- \n\nCall:\n    density.default(x = x, na.rm = TRUE)\n\nData: x (26 obs.);  Bandwidth 'bw' = 28.17\n\n       x               y            \n Min.   :338.5   Min.   :6.131e-06  \n 1st Qu.:435.7   1st Qu.:4.410e-04  \n Median :533.0   Median :2.180e-03  \n Mean   :533.0   Mean   :2.567e-03  \n 3rd Qu.:630.3   3rd Qu.:4.770e-03  \n Max.   :727.5   Max.   :5.687e-03  \n\n--------------------------------------------------------------------------------- \n ::: insulin::normal (KDE)\n--------------------------------------------------------------------------------- \n\nCall:\n    density.default(x = x, na.rm = TRUE)\n\nData: x (66 obs.);  Bandwidth 'bw' = 14.68\n\n       x               y            \n Min.   :225.0   Min.   :4.725e-06  \n 1st Qu.:286.2   1st Qu.:5.085e-04  \n Median :347.5   Median :3.890e-03  \n Mean   :347.5   Mean   :4.076e-03  \n 3rd Qu.:408.8   3rd Qu.:7.314e-03  \n Max.   :470.0   Max.   :9.098e-03  \n\n--------------------------------------------------------------------------------- \n ::: insulin::overt (KDE)\n--------------------------------------------------------------------------------- \n\nCall:\n    density.default(x = x, na.rm = TRUE)\n\nData: x (23 obs.);  Bandwidth 'bw' = 151.8\n\n       x                 y            \n Min.   :  82.48   Min.   :2.495e-06  \n 1st Qu.: 555.74   1st Qu.:1.141e-04  \n Median :1029.00   Median :6.280e-04  \n Mean   :1029.00   Mean   :5.276e-04  \n 3rd Qu.:1502.26   3rd Qu.:8.772e-04  \n Max.   :1975.52   Max.   :1.019e-03  \n\n--------------------------------------------------------------------------------- \n ::: sspg::chemical (KDE)\n--------------------------------------------------------------------------------- \n\nCall:\n    density.default(x = x, na.rm = TRUE)\n\nData: x (26 obs.);  Bandwidth 'bw' = 60.82\n\n       x                y            \n Min.   :-73.47   Min.   :2.835e-06  \n 1st Qu.:177.52   1st Qu.:2.706e-04  \n Median :428.50   Median :6.522e-04  \n Mean   :428.50   Mean   :9.949e-04  \n 3rd Qu.:679.48   3rd Qu.:1.536e-03  \n Max.   :930.47   Max.   :3.155e-03  \n\n--------------------------------------------------------------------------------- \n ::: sspg::normal (KDE)\n--------------------------------------------------------------------------------- \n\nCall:\n    density.default(x = x, na.rm = TRUE)\n\nData: x (66 obs.);  Bandwidth 'bw' = 19.76\n\n       x                y            \n Min.   : 13.73   Min.   :1.000e-09  \n 1st Qu.:147.61   1st Qu.:2.739e-05  \n Median :281.50   Median :3.686e-04  \n Mean   :281.50   Mean   :1.865e-03  \n 3rd Qu.:415.39   3rd Qu.:2.967e-03  \n Max.   :549.27   Max.   :7.958e-03  \n\n--------------------------------------------------------------------------------- \n ::: sspg::overt (KDE)\n--------------------------------------------------------------------------------- \n\nCall:\n    density.default(x = x, na.rm = TRUE)\n\nData: x (23 obs.);  Bandwidth 'bw' = 31.39\n\n       x                y            \n Min.   :-84.17   Min.   :6.215e-06  \n 1st Qu.: 75.41   1st Qu.:2.632e-04  \n Median :235.00   Median :5.806e-04  \n Mean   :235.00   Mean   :1.565e-03  \n 3rd Qu.:394.59   3rd Qu.:2.299e-03  \n Max.   :554.17   Max.   :5.651e-03  \n\n---------------------------------------------------------------------------------\n\nplot(a)\n\n\n\n\n\n\n\n\n\n\n\npred = predict(a, diabetes[,-4])\npred\n\n  [1] normal   normal   normal   normal   normal   normal   normal   normal  \n  [9] normal   normal   normal   normal   normal   normal   normal   normal  \n [17] normal   normal   normal   normal   normal   normal   normal   normal  \n [25] normal   normal   normal   normal   normal   normal   normal   normal  \n [33] normal   normal   normal   normal   normal   normal   normal   normal  \n [41] normal   normal   normal   normal   normal   normal   normal   normal  \n [49] normal   chemical normal   chemical chemical normal   normal   normal  \n [57] normal   normal   normal   normal   normal   normal   chemical normal  \n [65] normal   normal   normal   normal   normal   normal   normal   chemical\n [73] chemical chemical chemical chemical chemical chemical chemical chemical\n [81] chemical chemical chemical chemical chemical chemical chemical chemical\n [89] chemical normal   chemical chemical overt    overt    overt    overt   \n [97] overt    overt    overt    overt    overt    overt    overt    overt   \n[105] overt    chemical overt    overt    chemical overt    chemical overt   \n[113] overt    overt    overt   \nLevels: chemical normal overt\n\ncaret::confusionMatrix(pred, diabetes[,4])\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction chemical normal overt\n  chemical       22      2     3\n  normal          4     64     0\n  overt           0      0    20\n\nOverall Statistics\n                                          \n               Accuracy : 0.9217          \n                 95% CI : (0.8566, 0.9636)\n    No Information Rate : 0.5739          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.8634          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: chemical Class: normal Class: overt\nSensitivity                   0.8462        0.9697       0.8696\nSpecificity                   0.9438        0.9184       1.0000\nPos Pred Value                0.8148        0.9412       1.0000\nNeg Pred Value                0.9545        0.9574       0.9684\nPrevalence                    0.2261        0.5739       0.2000\nDetection Rate                0.1913        0.5565       0.1739\nDetection Prevalence          0.2348        0.5913       0.1739\nBalanced Accuracy             0.8950        0.9440       0.9348\n\n\n\n\nDiscretizar usando Chi-Merge\n\nUna discretización no supervisada no toma en cuenta los valores de la response, al momento de discretizar uno o más predictores.\nPor otro lado, la discretización supervisada sí considera la respuesta al momento de discretizar uno o más predictores, con el fin de crear intervalos de discretización que maximicen la probabilidad de predecir adecuadamente una training observation, no de tipo testing.\nChi-Merge\n\n\n# Discretizando por el método Chi-Merge\nlibrary(discretization)\n\nWarning: package 'discretization' was built under R version 4.1.3\n\n# Discretizamos todas las columnas, salvo la última.\nd_diab = chiM(diabetes, 0.01)$Disc.data\n\n# Discretizamos solo la primera y tercera columnas\n# d_diab = chiM(diabetes[, c(1, 3)], 0.01)$Disc.data\n\n\n# Convertimos a categórica para evitar que la función\n# naive_bayes asuma con variables numéricas (gaussianas o no)\nfor (i in 1:3) {\n  d_diab[,i] &lt;- as.factor(d_diab[,i])\n}\n\nd_diab\n\n    glucose insulin sspg    class\n1         1       1    2   normal\n2         2       1    3   normal\n3         1       1    3   normal\n4         1       1    3   normal\n5         1       1    3   normal\n6         1       1    3   normal\n7         1       1    3   normal\n8         1       1    3   normal\n9         1       1    2   normal\n10        1       1    3   normal\n11        1       1    3   normal\n12        1       1    2   normal\n13        1       1    3   normal\n14        1       1    3   normal\n15        1       1    3   normal\n16        1       1    3   normal\n17        1       1    2   normal\n18        1       1    2   normal\n19        1       1    2   normal\n20        1       1    2   normal\n21        1       1    3   normal\n22        1       1    3   normal\n23        1       1    3   normal\n24        1       1    3   normal\n25        1       1    3   normal\n26        1       1    3   normal\n27        1       1    2   normal\n28        1       1    3   normal\n29        1       1    3   normal\n30        1       1    3   normal\n31        1       1    3   normal\n32        1       1    2   normal\n33        1       1    3   normal\n34        1       1    3   normal\n35        2       1    2   normal\n36        2       1    3   normal\n37        1       1    3   normal\n38        1       1    2   normal\n39        1       1    2   normal\n40        1       1    2   normal\n41        1       1    2   normal\n42        1       1    3   normal\n43        1       1    3   normal\n44        1       1    3   normal\n45        1       1    3   normal\n46        1       1    3   normal\n47        1       1    3   normal\n48        1       1    2   normal\n49        1       1    2   normal\n50        1       2    3 chemical\n51        1       1    2   normal\n52        2       2    2   normal\n53        1       2    3 chemical\n54        1       1    2   normal\n55        1       2    3 chemical\n56        1       1    2   normal\n57        1       1    2   normal\n58        1       1    3   normal\n59        1       1    3   normal\n60        1       1    2   normal\n61        1       1    3   normal\n62        2       1    3   normal\n63        2       1    3   normal\n64        1       2    3 chemical\n65        1       1    3   normal\n66        1       1    3   normal\n67        1       1    3   normal\n68        1       1    2   normal\n69        1       1    4   normal\n70        1       2    3 chemical\n71        1       1    2   normal\n72        2       2    4 chemical\n73        2       2    3 chemical\n74        2       2    4 chemical\n75        2       2    3 chemical\n76        2       2    4 chemical\n77        2       2    4 chemical\n78        1       2    4 chemical\n79        2       2    3 chemical\n80        1       2    4 chemical\n81        1       2    4 chemical\n82        2       2    4 chemical\n83        1       2    4 chemical\n84        2       2    3 chemical\n85        1       2    2 chemical\n86        1       2    4 chemical\n87        1       2    2 chemical\n88        1       2    4 chemical\n89        2       2    2 chemical\n90        1       2    3 chemical\n91        2       2    3 chemical\n92        2       2    2 chemical\n93        3       3    1    overt\n94        3       3    3    overt\n95        3       3    1    overt\n96        3       3    2    overt\n97        3       3    2    overt\n98        3       3    1    overt\n99        3       3    2    overt\n100       3       3    2    overt\n101       3       2    2    overt\n102       3       3    1    overt\n103       3       3    2    overt\n104       3       3    2    overt\n105       3       3    2    overt\n106       3       2    4    overt\n107       3       3    1    overt\n108       3       3    1    overt\n109       3       2    2    overt\n110       3       3    1    overt\n111       3       2    4    overt\n112       3       3    3    overt\n113       3       3    2    overt\n114       3       3    1    overt\n115       3       3    1    overt\n\n\nEsta librería asume que la última columna del data frame es la de la clase que deseamos predecir.\n\n# Sin corrección de Laplace (observar prob. cond.)\nb0 &lt;- naive_bayes(class ~ ., data = d_diab)\n\nWarning: naive_bayes(): Feature glucose - zero probabilities are present.\nConsider Laplace smoothing.\n\n\nWarning: naive_bayes(): Feature insulin - zero probabilities are present.\nConsider Laplace smoothing.\n\n\nWarning: naive_bayes(): Feature sspg - zero probabilities are present. Consider\nLaplace smoothing.\n\nb0\n\n\n================================== Naive Bayes ================================== \n \n Call: \nnaive_bayes.formula(formula = class ~ ., data = d_diab)\n\n--------------------------------------------------------------------------------- \n \nLaplace smoothing: 0\n\n--------------------------------------------------------------------------------- \n \n A priori probabilities: \n\nchemical   normal    overt \n0.226087 0.573913 0.200000 \n\n--------------------------------------------------------------------------------- \n \n Tables: \n\n--------------------------------------------------------------------------------- \n ::: glucose (Categorical) \n--------------------------------------------------------------------------------- \n       \nglucose   chemical     normal      overt\n      1 0.53846154 0.90909091 0.00000000\n      2 0.46153846 0.09090909 0.00000000\n      3 0.00000000 0.00000000 1.00000000\n\n--------------------------------------------------------------------------------- \n ::: insulin (Categorical) \n--------------------------------------------------------------------------------- \n       \ninsulin   chemical     normal      overt\n      1 0.00000000 0.98484848 0.00000000\n      2 1.00000000 0.01515152 0.17391304\n      3 0.00000000 0.00000000 0.82608696\n\n--------------------------------------------------------------------------------- \n ::: sspg (Categorical) \n--------------------------------------------------------------------------------- \n    \nsspg   chemical     normal      overt\n   1 0.00000000 0.00000000 0.39130435\n   2 0.15384615 0.36363636 0.43478261\n   3 0.42307692 0.62121212 0.08695652\n   4 0.42307692 0.01515152 0.08695652\n\n---------------------------------------------------------------------------------\n\n# Aplicando laplaciano = 1\nb1 &lt;- naive_bayes(class ~ ., data = d_diab, laplace = 1)\nb1\n\n\n================================== Naive Bayes ================================== \n \n Call: \nnaive_bayes.formula(formula = class ~ ., data = d_diab, laplace = 1)\n\n--------------------------------------------------------------------------------- \n \nLaplace smoothing: 1\n\n--------------------------------------------------------------------------------- \n \n A priori probabilities: \n\nchemical   normal    overt \n0.226087 0.573913 0.200000 \n\n--------------------------------------------------------------------------------- \n \n Tables: \n\n--------------------------------------------------------------------------------- \n ::: glucose (Categorical) \n--------------------------------------------------------------------------------- \n       \nglucose   chemical     normal      overt\n      1 0.51724138 0.88405797 0.03846154\n      2 0.44827586 0.10144928 0.03846154\n      3 0.03448276 0.01449275 0.92307692\n\n--------------------------------------------------------------------------------- \n ::: insulin (Categorical) \n--------------------------------------------------------------------------------- \n       \ninsulin   chemical     normal      overt\n      1 0.03448276 0.95652174 0.03846154\n      2 0.93103448 0.02898551 0.19230769\n      3 0.03448276 0.01449275 0.76923077\n\n--------------------------------------------------------------------------------- \n ::: sspg (Categorical) \n--------------------------------------------------------------------------------- \n    \nsspg   chemical     normal      overt\n   1 0.03333333 0.01428571 0.37037037\n   2 0.16666667 0.35714286 0.40740741\n   3 0.40000000 0.60000000 0.11111111\n   4 0.40000000 0.02857143 0.11111111\n\n---------------------------------------------------------------------------------\n\n\n\npred0 = predict(b0, d_diab[,-4])\ncaret::confusionMatrix(pred0, d_diab[,4])\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction chemical normal overt\n  chemical       26      1     0\n  normal          0     65     0\n  overt           0      0    23\n\nOverall Statistics\n                                          \n               Accuracy : 0.9913          \n                 95% CI : (0.9525, 0.9998)\n    No Information Rate : 0.5739          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.9851          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: chemical Class: normal Class: overt\nSensitivity                   1.0000        0.9848          1.0\nSpecificity                   0.9888        1.0000          1.0\nPos Pred Value                0.9630        1.0000          1.0\nNeg Pred Value                1.0000        0.9800          1.0\nPrevalence                    0.2261        0.5739          0.2\nDetection Rate                0.2261        0.5652          0.2\nDetection Prevalence          0.2348        0.5652          0.2\nBalanced Accuracy             0.9944        0.9924          1.0\n\n\n\npred1 = predict(b1, d_diab[,-4])\ncaret::confusionMatrix(pred1, d_diab[,4])\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction chemical normal overt\n  chemical       26      1     0\n  normal          0     65     0\n  overt           0      0    23\n\nOverall Statistics\n                                          \n               Accuracy : 0.9913          \n                 95% CI : (0.9525, 0.9998)\n    No Information Rate : 0.5739          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.9851          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: chemical Class: normal Class: overt\nSensitivity                   1.0000        0.9848          1.0\nSpecificity                   0.9888        1.0000          1.0\nPos Pred Value                0.9630        1.0000          1.0\nNeg Pred Value                1.0000        0.9800          1.0\nPrevalence                    0.2261        0.5739          0.2\nDetection Rate                0.2261        0.5652          0.2\nDetection Prevalence          0.2348        0.5652          0.2\nBalanced Accuracy             0.9944        0.9924          1.0\n\n\n\n\nTesting data\n\n# Datos de Prueba\ndiabetes_test = read.csv(\"../datos/DiabetesTest.csv\")\ndiabetes_test$class &lt;- as.factor(diabetes_test$class)\n\nhead(diabetes_test)\n\n  glucose insulin sspg    class\n1      89     472  162 chemical\n2      96     465  237 chemical\n3     112     503  408 chemical\n4     110     477  124 chemical\n5      90     413  344 chemical\n6     102     472  297 chemical\n\ncaret::confusionMatrix(\n  predict(a, diabetes_test[,-4]), diabetes_test[,4]\n)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction chemical normal overt\n  chemical        9      1     0\n  normal          1      9     0\n  overt           0      0    10\n\nOverall Statistics\n                                          \n               Accuracy : 0.9333          \n                 95% CI : (0.7793, 0.9918)\n    No Information Rate : 0.3333          \n    P-Value [Acc &gt; NIR] : 8.747e-12       \n                                          \n                  Kappa : 0.9             \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: chemical Class: normal Class: overt\nSensitivity                   0.9000        0.9000       1.0000\nSpecificity                   0.9500        0.9500       1.0000\nPos Pred Value                0.9000        0.9000       1.0000\nNeg Pred Value                0.9500        0.9500       1.0000\nPrevalence                    0.3333        0.3333       0.3333\nDetection Rate                0.3000        0.3000       0.3333\nDetection Prevalence          0.3333        0.3333       0.3333\nBalanced Accuracy             0.9250        0.9250       1.0000\n\n\n\nTheorem 1 Si se discretizó el training set, usar esos mismos intervalos de discretazicación para el testing set.\n\n\n# Intervalos de discretización\nalpha &lt;- 0.01\nd &lt;- chiM(diabetes, alpha)\nd$cutp\n\n[[1]]\n[1] 100.5 117.0\n\n[[2]]\n[1] 420.5 656.5\n\n[[3]]\n[1]  63.5 140.5 283.5\n\n\nEl nivel de discretización (alpha) también es un hiperparámtro.\n\nProposition 1 Ejercicio\nDiscretizar el testing dataset usando los intervalos de discretización del training dataset, para evaluar la matriz de confusión asociada a los modelos b0 y b1.\n\n\nProposition 2 Ejercicio\nImplementar un for loop para el hiperparámetro alpha, y obtener la matriz de confusión asociada al modelo con discretización Chi-Merge."
  },
  {
    "objectID": "apuntes/clase-05.html#k-vecinos-más-cercanos-knn",
    "href": "apuntes/clase-05.html#k-vecinos-más-cercanos-knn",
    "title": "Apuntes de clase",
    "section": "K-vecinos más cercanos (KNN)",
    "text": "K-vecinos más cercanos (KNN)\n\nNo es eficiente para inferencia, es decir, entender el efecto de predictores sobre la response.\nÚtil para regresión y clasificación.\nDifícil de intepretar.\nNo le afecta si son varias clases para respuesta.\nLlega a demorar computacionalmente para altos volúmenes de datos, por lo que no es un algoritmo escalable.\nSe le considera un lazy algorithm, pues no estima nada, no es que genere una función de predicción, como hemos vimos en otros algoritmos.   Por ello, si se aumenta la cantidad de datos de entrenamiento, habría que ejecutar todo el algoritmo de nuevo.\nSe emplea para predictores numéricos.\nSe deben estandarizar los datos (mapeándolos al intervalo \\(\\left( 0,1 \\right)\\) por ejemplo), puesto que las unidades de los predictores pueden alterar la noción de cercanía.\nSe emplea para predictores numéricos.\nSe deben estandarizar los datos (mapeándolos al intervalo \\(\\left( 0,1 \\right)\\) por ejemplo), puesto que las unidades de los predictores pueden alterar la noción de cercanía.\nEn el caso se desee usar predictores categóricos en el modelo, se tienen algunas alternativas:\n\nEstimar variables latentes asociadas a predictores categóricos (lo veremos en Stats. Learning 2)\nBinarizar las variables categóricas, pero sin descartar las columnas consideradas extra, cuando se hace el tratamiento de dummy variables.\n\n\n\nPasos\n\nElegimos dos parámetros:\n\nMétrica para calcular distancias.\nValor de K (número de vecinos a considerar).\n\nDada una observación \\(x_0\\), buscar los \\(K\\) puntos de datos de entrenamiento más cercanos a \\(x_0\\).\nEstos \\(K\\) puntos forman la vecindad \\(\\mathcal{N}_0\\) de \\(x_0\\).\nLa clasificación se realiza vía algún tipo de promedio. Por ejemplo, media (ponderada o no) para el caso de regresión; y, moda, para el caso de clasificación.\n\n\n\n¿Cómo elegir K?\n\n\\(K = 1\\) genera un modelo muy flexible.\n\\(K\\) muy grande puede generar una separación via un hiperplano, por lo que se espera un resultado similar a usar Análisis Discriminante."
  },
  {
    "objectID": "apuntes/clase-05.html#ejemplo-práctico-1",
    "href": "apuntes/clase-05.html#ejemplo-práctico-1",
    "title": "Apuntes de clase",
    "section": "Ejemplo práctico",
    "text": "Ejemplo práctico\n\nlibrary(class)\n\nWarning: package 'class' was built under R version 4.1.3\n\nb &lt;- knn(\n  # Repetimos los datos, por fines ilustrativos\n  train = diabetes[,1:3], test = diabetes[,1:3],\n  cl = diabetes[,4]\n  # Por default, K vale 1\n)\nb\n\n  [1] normal   normal   normal   normal   normal   normal   normal   normal  \n  [9] normal   normal   normal   normal   normal   normal   normal   normal  \n [17] normal   normal   normal   normal   normal   normal   normal   normal  \n [25] normal   normal   normal   normal   normal   normal   normal   normal  \n [33] normal   normal   normal   normal   normal   normal   normal   normal  \n [41] normal   normal   normal   normal   normal   normal   normal   normal  \n [49] normal   chemical normal   normal   chemical normal   chemical normal  \n [57] normal   normal   normal   normal   normal   normal   normal   chemical\n [65] normal   normal   normal   normal   normal   chemical normal   chemical\n [73] chemical chemical chemical chemical chemical chemical chemical chemical\n [81] chemical chemical chemical chemical chemical chemical chemical chemical\n [89] chemical chemical chemical chemical overt    overt    overt    overt   \n [97] overt    overt    overt    overt    overt    overt    overt    overt   \n[105] overt    overt    overt    overt    overt    overt    overt    overt   \n[113] overt    overt    overt   \nLevels: chemical normal overt\n\n# Estimacion del error por resubstitución\ncaret::confusionMatrix(b, diabetes[,4])\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction chemical normal overt\n  chemical       26      0     0\n  normal          0     66     0\n  overt           0      0    23\n\nOverall Statistics\n                                     \n               Accuracy : 1          \n                 95% CI : (0.9684, 1)\n    No Information Rate : 0.5739     \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16  \n                                     \n                  Kappa : 1          \n                                     \n Mcnemar's Test P-Value : NA         \n\nStatistics by Class:\n\n                     Class: chemical Class: normal Class: overt\nSensitivity                   1.0000        1.0000          1.0\nSpecificity                   1.0000        1.0000          1.0\nPos Pred Value                1.0000        1.0000          1.0\nNeg Pred Value                1.0000        1.0000          1.0\nPrevalence                    0.2261        0.5739          0.2\nDetection Rate                0.2261        0.5739          0.2\nDetection Prevalence          0.2261        0.5739          0.2\nBalanced Accuracy             1.0000        1.0000          1.0\n\n\n\n# con K=3\nk_3 &lt;- knn(\n  train = diabetes[,1:3], test = diabetes[,1:3],\n  cl = diabetes[,4], k = 3\n)\ncaret::confusionMatrix(k_3, diabetes[,4])\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction chemical normal overt\n  chemical       25      1     4\n  normal          1     65     0\n  overt           0      0    19\n\nOverall Statistics\n                                          \n               Accuracy : 0.9478          \n                 95% CI : (0.8899, 0.9806)\n    No Information Rate : 0.5739          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.9098          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: chemical Class: normal Class: overt\nSensitivity                   0.9615        0.9848       0.8261\nSpecificity                   0.9438        0.9796       1.0000\nPos Pred Value                0.8333        0.9848       1.0000\nNeg Pred Value                0.9882        0.9796       0.9583\nPrevalence                    0.2261        0.5739       0.2000\nDetection Rate                0.2174        0.5652       0.1652\nDetection Prevalence          0.2609        0.5739       0.1652\nBalanced Accuracy             0.9527        0.9822       0.9130\n\nmean(k_3 == diabetes[,4])\n\n[1] 0.9478261\n\n# con K=7\nk_7 &lt;- knn(\n  train = diabetes[,1:3], test = diabetes[,1:3],\n  cl = diabetes[,4], k = 7\n)\ncaret::confusionMatrix(k_7, diabetes[,4])\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction chemical normal overt\n  chemical       22      1     5\n  normal          4     65     0\n  overt           0      0    18\n\nOverall Statistics\n                                          \n               Accuracy : 0.913           \n                 95% CI : (0.8459, 0.9575)\n    No Information Rate : 0.5739          \n    P-Value [Acc &gt; NIR] : 8.022e-16       \n                                          \n                  Kappa : 0.8473          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: chemical Class: normal Class: overt\nSensitivity                   0.8462        0.9848       0.7826\nSpecificity                   0.9326        0.9184       1.0000\nPos Pred Value                0.7857        0.9420       1.0000\nNeg Pred Value                0.9540        0.9783       0.9485\nPrevalence                    0.2261        0.5739       0.2000\nDetection Rate                0.1913        0.5652       0.1565\nDetection Prevalence          0.2435        0.6000       0.1565\nBalanced Accuracy             0.8894        0.9516       0.8913\n\nmean(k_7 == diabetes[,4])\n\n[1] 0.9130435\n\n# con K=15\nk_15 &lt;- knn(\n  train = diabetes[,1:3], test = diabetes[,1:3],\n  cl = diabetes[,4], k = 15\n)\ncaret::confusionMatrix(k_15, diabetes[,4])\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction chemical normal overt\n  chemical       20      1     5\n  normal          6     65     0\n  overt           0      0    18\n\nOverall Statistics\n                                          \n               Accuracy : 0.8957          \n                 95% CI : (0.8248, 0.9449)\n    No Information Rate : 0.5739          \n    P-Value [Acc &gt; NIR] : 3.778e-14       \n                                          \n                  Kappa : 0.8147          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: chemical Class: normal Class: overt\nSensitivity                   0.7692        0.9848       0.7826\nSpecificity                   0.9326        0.8776       1.0000\nPos Pred Value                0.7692        0.9155       1.0000\nNeg Pred Value                0.9326        0.9773       0.9485\nPrevalence                    0.2261        0.5739       0.2000\nDetection Rate                0.1739        0.5652       0.1565\nDetection Prevalence          0.2261        0.6174       0.1565\nBalanced Accuracy             0.8509        0.9312       0.8913\n\nmean(k_15 == diabetes[,4])\n\n[1] 0.8956522\n\n\n\nTesting data\n\n# Aplicando K=3 en los datos de test\ndiabetes_test = read.csv(\"../datos/DiabetesTest.csv\")\ndiabetes_test$class &lt;- as.factor(diabetes_test$class)\n\nK_3 &lt;- knn(\n  train = diabetes[,1:3], test = diabetes_test[,-4],\n  cl = diabetes[,4], k = 3, prob = TRUE\n)\n\n# Obtener la proporción de votos para la clase ganadora\nK_3_prob &lt;- attr(K_3, \"prob\")\n\n# Valores predichos\nhead(K_3)\n\n[1] chemical chemical chemical chemical normal   chemical\nLevels: chemical normal overt\n\n# Proporciones de \"votos\"\nhead(K_3_prob)\n\n[1] 1.0000000 1.0000000 0.6666667 1.0000000 0.6666667 1.0000000\n\n\n\ncaret::confusionMatrix(K_3, diabetes_test[,4])\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction chemical normal overt\n  chemical        8      0     0\n  normal          2     10     0\n  overt           0      0    10\n\nOverall Statistics\n                                          \n               Accuracy : 0.9333          \n                 95% CI : (0.7793, 0.9918)\n    No Information Rate : 0.3333          \n    P-Value [Acc &gt; NIR] : 8.747e-12       \n                                          \n                  Kappa : 0.9             \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: chemical Class: normal Class: overt\nSensitivity                   0.8000        1.0000       1.0000\nSpecificity                   1.0000        0.9000       1.0000\nPos Pred Value                1.0000        0.8333       1.0000\nNeg Pred Value                0.9091        1.0000       1.0000\nPrevalence                    0.3333        0.3333       0.3333\nDetection Rate                0.2667        0.3333       0.3333\nDetection Prevalence          0.2667        0.4000       0.3333\nBalanced Accuracy             0.9000        0.9500       1.0000"
  }
]