{
  "hash": "535199ab5caf72fe02e1cb0a70c42d14",
  "result": {
    "markdown": "# Apuntes de clase {-}\n\n## Ejemplo práctico: KNN\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(class)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'class' was built under R version 4.1.3\n```\n:::\n\n```{.r .cell-code}\nadmision <- read.csv(\"../datos/binary.csv\")\nadmision$rank <- factor(admision$rank)\nadmision$admit <- factor(admision$admit)\nhead(admision)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  admit gre  gpa rank\n1     0 380 3.61    3\n2     1 660 3.67    3\n3     1 800 4.00    1\n4     1 640 3.19    4\n5     0 520 2.93    4\n6     1 760 3.00    2\n```\n:::\n:::\n\n\n\n### Sin usar predictores categóricos\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nk_11_a <- knn(\n  train = admision[,2:3], test = admision[,2:3],\n  cl = admision[,1], k = 11\n)\ncaret::confusionMatrix(k_11_a, admision[,1])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 258  96\n         1  15  31\n                                          \n               Accuracy : 0.7225          \n                 95% CI : (0.6758, 0.7658)\n    No Information Rate : 0.6825          \n    P-Value [Acc > NIR] : 0.04672         \n                                          \n                  Kappa : 0.228           \n                                          \n Mcnemar's Test P-Value : 3.119e-14       \n                                          \n            Sensitivity : 0.9451          \n            Specificity : 0.2441          \n         Pos Pred Value : 0.7288          \n         Neg Pred Value : 0.6739          \n             Prevalence : 0.6825          \n         Detection Rate : 0.6450          \n   Detection Prevalence : 0.8850          \n      Balanced Accuracy : 0.5946          \n                                          \n       'Positive' Class : 0               \n                                          \n```\n:::\n\n```{.r .cell-code}\nk_11_b <- knn(\n  train = admision[,2:4], test = admision[,2:4],\n  cl = admision[,1], k = 11\n)\ncaret::confusionMatrix(k_11_b, admision[,1])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 256  87\n         1  17  40\n                                          \n               Accuracy : 0.74            \n                 95% CI : (0.6941, 0.7823)\n    No Information Rate : 0.6825          \n    P-Value [Acc > NIR] : 0.007092        \n                                          \n                  Kappa : 0.2964          \n                                          \n Mcnemar's Test P-Value : 1.324e-11       \n                                          \n            Sensitivity : 0.9377          \n            Specificity : 0.3150          \n         Pos Pred Value : 0.7464          \n         Neg Pred Value : 0.7018          \n             Prevalence : 0.6825          \n         Detection Rate : 0.6400          \n   Detection Prevalence : 0.8575          \n      Balanced Accuracy : 0.6263          \n                                          \n       'Positive' Class : 0               \n                                          \n```\n:::\n:::\n\n\n\n### Usando predictores categóricos\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Binarizar variables, sin remover\n# ninguna de las columnas binarizadas.\nrank_d <- fastDummies::dummy_cols(\n  admision, select_columns = \"rank\"\n)\n#rank_e <-model.matrix( ~ rank-1, data=admision)\nadmision_d <- cbind(admision[,-4], rank_d)\n\nk_11_c <- knn(\n  train = admision_d[,-1], test = admision_d[,-1],\n  cl = admision[,1], k = 11\n)\ncaret::confusionMatrix(k_11_c,admision[,1])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 266  66\n         1   7  61\n                                          \n               Accuracy : 0.8175          \n                 95% CI : (0.7761, 0.8541)\n    No Information Rate : 0.6825          \n    P-Value [Acc > NIR] : 8.178e-10       \n                                          \n                  Kappa : 0.5192          \n                                          \n Mcnemar's Test P-Value : 1.134e-11       \n                                          \n            Sensitivity : 0.9744          \n            Specificity : 0.4803          \n         Pos Pred Value : 0.8012          \n         Neg Pred Value : 0.8971          \n             Prevalence : 0.6825          \n         Detection Rate : 0.6650          \n   Detection Prevalence : 0.8300          \n      Balanced Accuracy : 0.7273          \n                                          \n       'Positive' Class : 0               \n                                          \n```\n:::\n:::\n\n\n\n### Normalización Min-Max\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nadmision_mm <- admision[,2:3]\na <- 0\nb <- 1\nadmision_mm$gre <- \n  (admision_mm$gre - min(admision_mm$gre)) / \n  (max(admision_mm$gre) - min(admision_mm$gre)) *\n  (b - a) + a\n\nadmision_mm$gpa <- \n  (admision_mm$gpa - min(admision_mm$gpa)) /\n  (max(admision_mm$gpa) - min(admision_mm$gpa)) *\n  (b - a) + a\n\nadmision_mm <- cbind(admision_mm, rank_d)\nsummary(admision_mm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      gre              gpa         admit        gre             gpa       \n Min.   :0.0000   Min.   :0.0000   0:273   Min.   :220.0   Min.   :2.260  \n 1st Qu.:0.5172   1st Qu.:0.5000   1:127   1st Qu.:520.0   1st Qu.:3.130  \n Median :0.6207   Median :0.6523           Median :580.0   Median :3.395  \n Mean   :0.6340   Mean   :0.6494           Mean   :587.7   Mean   :3.390  \n 3rd Qu.:0.7586   3rd Qu.:0.8103           3rd Qu.:660.0   3rd Qu.:3.670  \n Max.   :1.0000   Max.   :1.0000           Max.   :800.0   Max.   :4.000  \n rank        rank_1           rank_2           rank_3           rank_4      \n 1: 61   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 2:151   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n 3:121   Median :0.0000   Median :0.0000   Median :0.0000   Median :0.0000  \n 4: 67   Mean   :0.1525   Mean   :0.3775   Mean   :0.3025   Mean   :0.1675  \n         3rd Qu.:0.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n         Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n```\n:::\n\n```{.r .cell-code}\nk_11_mm <- knn(\n  train = admision_mm, test = admision_mm,\n  cl = admision[,1], k = 11\n)\ncaret::confusionMatrix(k_11_mm, admision[,1])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 266  67\n         1   7  60\n                                          \n               Accuracy : 0.815           \n                 95% CI : (0.7734, 0.8519)\n    No Information Rate : 0.6825          \n    P-Value [Acc > NIR] : 1.706e-09       \n                                          \n                  Kappa : 0.5114          \n                                          \n Mcnemar's Test P-Value : 6.953e-12       \n                                          \n            Sensitivity : 0.9744          \n            Specificity : 0.4724          \n         Pos Pred Value : 0.7988          \n         Neg Pred Value : 0.8955          \n             Prevalence : 0.6825          \n         Detection Rate : 0.6650          \n   Detection Prevalence : 0.8325          \n      Balanced Accuracy : 0.7234          \n                                          \n       'Positive' Class : 0               \n                                          \n```\n:::\n:::\n\n\n\n### Normalización SoftMax\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nadmision_sm <- admision[,2:3]\nadmision_sm <- DMwR2::SoftMax(admision[,2:3], lambda = 2*pi)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n```\n:::\n\n```{.r .cell-code}\nadmision_sm <- cbind(admision_sm, rank_d)\nsummary(admision_sm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      gre               gpa          admit        gre             gpa       \n Min.   :0.03981   Min.   :0.04885   0:273   Min.   :220.0   Min.   :2.260  \n 1st Qu.:0.35754   1st Qu.:0.33561   1:127   1st Qu.:520.0   1st Qu.:3.130  \n Median :0.48334   Median :0.50335           Median :580.0   Median :3.395  \n Mean   :0.50137   Mean   :0.50218           Mean   :587.7   Mean   :3.390  \n 3rd Qu.:0.65156   3rd Qu.:0.67612           3rd Qu.:660.0   3rd Qu.:3.670  \n Max.   :0.86269   Max.   :0.83246           Max.   :800.0   Max.   :4.000  \n rank        rank_1           rank_2           rank_3           rank_4      \n 1: 61   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 2:151   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n 3:121   Median :0.0000   Median :0.0000   Median :0.0000   Median :0.0000  \n 4: 67   Mean   :0.1525   Mean   :0.3775   Mean   :0.3025   Mean   :0.1675  \n         3rd Qu.:0.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n         Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n```\n:::\n\n```{.r .cell-code}\nk_11_sm <- knn(\n  train = admision_sm, test = admision_sm,\n  cl = admision[,1], k = 11\n)\ncaret::confusionMatrix(k_11_sm,admision[,1])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 266  68\n         1   7  59\n                                          \n               Accuracy : 0.8125          \n                 95% CI : (0.7707, 0.8496)\n    No Information Rate : 0.6825          \n    P-Value [Acc > NIR] : 3.503e-09       \n                                          \n                  Kappa : 0.5036          \n                                          \n Mcnemar's Test P-Value : 4.262e-12       \n                                          \n            Sensitivity : 0.9744          \n            Specificity : 0.4646          \n         Pos Pred Value : 0.7964          \n         Neg Pred Value : 0.8939          \n             Prevalence : 0.6825          \n         Detection Rate : 0.6650          \n   Detection Prevalence : 0.8350          \n      Balanced Accuracy : 0.7195          \n                                          \n       'Positive' Class : 0               \n                                          \n```\n:::\n:::\n\n\n\n## Clasificación: Medidas de Evaluación\n\n### Predicción\n\n- Los modelos de clasificación generan dos tipos de predicciones:\n    - Continuas: **Probabilidad** de pertenencia a una clase.\n    - Categóricas: **Clase** predicha.\n\n- KNN provee ambos tipos de predicciones; mientras\nque Regresión Logística solo provee la probabilidad de pertenencia a una clase.\n\n- Las **probabilidades de pertenencia** a una clase pueden\nservir incluso como input para modelos.\n\n### Evaluación de las clases predichas\n\n- Un método común para tal evaluación es la **matriz de confusión**.\n\nPredichos | Eventos Observados | No Eventos Observados\n| :---: | :---: | :---: |\nEventos | **True Positive** | **False Positive**\nNo Eventos | **False Negative** | **True Negative**\n\n- Métrica más simple:\n  - **Ratio de la exactitud total** (accuracy)\n    $\\quad acc(d) = \\dfrac{TP + TN}{P + N}$ \n  - **Ratio de error**: $\\quad 1 - acc(d)$ \n\n- Desventajas de este par de métricas:\n    - **No realiza distinciones entre el tipo de error cometido**. \\\n    Por ejemplo, en filtros de spam, el costo de borrar un e-mail\n    importante es mucho mayor que los costos de permitir un \n    e-mail spam pase el filtro.\n    - Es importante considerar la frecuencia total de cada clase. \\\n    **Cuando se tienen clases muy desbalanceadas, el accuracy no debería**\n    **ser la métrica principal para evaluar el modelo de clasificación**.\n\n- Para regresión logística binaria, el *accuracy*\nse maximiza cuando el punto de corte $c$ \n(para asignar clase en base a probabilidad estimada) vale 0.5.\n\n- **Ratio no informativo**: Ratio de exactitud que se podría alcanzar sin\nusar un modelo. Por ejemplo, $50\\%$ en caso de tirar una moneda justa.\n\n- Este último ratio se puede definir de varias formas:\n    - Para un conjunto de datos con $C$ clases, se puede considerar $1/C$.\n    - Puede usarse el porcentaje de la clase de mayor frecuencia,\n    en el conjunto de entrenamiento.\n    - Sobre el efecto severo de clases no balanceadas y posibles medidas\n    remediales, ver Kuhn y Johnson (2013).\n\n- No necesariamente clases desbalanceadas implica dificultad de predicción.\nPor ejemplo, si la clase representa alguna condición extremadamente \npoco común (*que una persona sea intersex*, o algo así). \\ \nEn ese contexto, (clases poco comunes pero fáciles de predecir) \n**no se consideran** los datos como desbalanceados.\n\n#### Coeficiente Kappa de Cohen\n\n- El coeficiente Kappa fue diseñado para \n**evaluar la concordancia entre dos evaluadores/jueces**.\n\n- Kappa toma en cuenta la precisión que sería generada\npor causas aleatorias: $Kappa = \\dfrac{O - E}{1 - E}$,\ndonde $O$ es la **precisión observada**, y, $E$,\nla **precisión esperada**.\n\n- $O$ es $acc(.)$ del modelo usado.\n\n- Kappa está entre $-1$ y $1$.\n    - $Kappa \\leq 0$ implica baja concordancia, \n    pudiendo ser generada por haber definido erróneamente\n    algunas categorías.\n    - $Kappa \\approx 0$ implica **no hay concordancia**\n    entre las clases observadas y las pronisticadas..\n    - $Kappa \\approx 1$ implica **perfecta concordancia**\n    entre las clases observadas y las pronisticadas.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Interpretation of Cohen's Kappa](../media/cohen-kappa.jpg){width=100%}\n:::\n:::\n\n\n\n- Ejemplo para una matriz de confusión que vimos en un modelo de hoy:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmc <- matrix(\n  data = c(256, 84, 17, 43), nrow = 2, byrow = TRUE\n) \nmc\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2]\n[1,]  256   84\n[2,]   17   43\n```\n:::\n\n```{.r .cell-code}\nP <- sum(mc[1, ])\nN <- sum(mc[2, ])\n\nO <- sum(diag(mc)) / (P + N)\nO\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.7475\n```\n:::\n\n```{.r .cell-code}\nprop_P <- P / (P + N)\nprop_N <- N / (P + N)\n\nE <- (sum(mc[,1])*prop_P + sum(mc[,2])*prop_N) / (P + N)\nE\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.62775\n```\n:::\n\n```{.r .cell-code}\nkappa <- (O - E) / (1 - E)\nkappa\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.3216924\n```\n:::\n:::\n\n\n\n#### Clasificación Binaria\n\n- La **sensitivity/sensibilidad/recall** de un modelo \nes el ratio en que el evento de interés es **predicho  correctamente**, para todas las muestras que contienen el evento:\n    - Es el **ratio de verdaderos positivos**.\n    - $sensibilidad(d) = \\dfrac{TP}{TP + FN}$\n\n- **Especificidad**:\n    - Es el **ratio de verdaderos negativos**.\n    - $Especificidad(d) = \\dfrac{TN}{FP + TN}$ \n\n- En el caso de predecir si un correo es **SPAM**,\nconsiderando como *éxito* $Y = 1$, de tratarse de SPAM,\nes mejor una mayor *especificidad*, que *sensibilidad*.\n\n- **Falsa alarma**:\n    - Es el ratio de los **falsos positivos**.\n    - $Falarm(d) = 1 - Especificidad(d) = \\dfrac{FP}{FP + TN}$ \n\n- **Precisión**:\n    - Comparación entre los verdaderos positivos, \n    con las instancias predichas como positivas.\n    - $Precision(d) = \\dfrac{TP}{TP + FP}$  \n\n- **$F_{\\beta}-score$**\n    - Indicador muy útil cuando hay **datos desbalanceados**,\n    aunque aún se puede usar si no hay desbalance.\n    - $\\beta$ se puede considerar un *hiperparámetro*.\n    - **F1-score**: \\\n    \\begin{gather}\n      F_1 (d) = 2 \\dfrac{precision * sensibilidad}{precision + sensibilidad} \\\\\n\n      F_{\\beta}(d) = \\dfrac{\\left( 1 + \\beta^2  \\right)*precision * sensibiidad}{\\left( \\beta^2 * precision + sensibilidad \\right)}\n    \\end{gather}\n    \n  - Cuando el desbalance va a favor de la clase positiva, \n  es útil considerar $\\beta = 1$.\n\n  - **Mayor $F_1$-score** significa que balanceas bien\n  la *sensibilidad* y *precisión* del modelo ... \n  es un promedio de dos indicadores.\n\n  - **No hay un punto de corte exacto** para el $F_1$-score.\n  Pero tal indicador es útil para **comparar** modelos.\n\n## Equilibrio entre sensibilidad y especificdad\n\nfalta completar apuntes\n\n### Curva ROC\n\nfalta completar apuntes\n\n## Selección de Medidas\n\n- No existe un único indicador que baste usar para evaluar un modelo.\n- En general, ningún clasificador es óptimo para todas las métricas de evaluación.\n\n## Tarea\n\nLa siguiente semana se compartirá la lista de ejercicios,\npor lo que se debe ir avanzando con las listas ya publicadas.\n",
    "supporting": [
      "clase-06_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}